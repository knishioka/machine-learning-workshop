{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4412e913",
   "metadata": {},
   "source": [
    "# MLflow v3 Ã— LLM ãƒ©ã‚¤ãƒ–ãƒ‡ãƒ¢ï¼ˆå®Œå…¨ç‰ˆï¼‰\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€**MLflow v3**ã‚’ä½¿ã£ãŸLLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿé¨“ç®¡ç†ã‚’**10-15åˆ†**ã§ä½“é¨“ã§ãã‚‹ãƒ‡ãƒ¢ã§ã™ã€‚\n",
    "\n",
    "## ğŸ¯ ã“ã®ãƒ‡ãƒ¢ã§å­¦ã¹ã‚‹ã“ã¨\n",
    "1. **åŸºç¤ç·¨**ï¼šå®Ÿé¨“ã®è¨˜éŒ²ã¨æ¯”è¼ƒï¼ˆã‚»ãƒ«1-6ï¼‰\n",
    "2. **å¿œç”¨ç·¨**ï¼šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã¨ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ï¼ˆã‚»ãƒ«7-9ï¼‰  \n",
    "3. **å®Ÿè·µç·¨**ï¼šUIæ“ä½œã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼ˆã‚»ãƒ«10-12ï¼‰\n",
    "\n",
    "## ğŸ’¡ MLflowã‚’ä½¿ã†ç†ç”±\n",
    "- **å•é¡Œ**ï¼šã€Œå…ˆé€±ã®ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã¯ï¼Ÿã€ã€Œã©ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè‰¯ã‹ã£ãŸï¼Ÿã€\n",
    "- **è§£æ±º**ï¼šã™ã¹ã¦ã®å®Ÿé¨“ã‚’è‡ªå‹•è¨˜éŒ²ã€å®šé‡æ¯”è¼ƒã€å†ç¾å¯èƒ½ã«\n",
    "\n",
    "## ğŸš€ å¿…è¦ãªæº–å‚™\n",
    "```bash\n",
    "# MLflow UIã‚’èµ·å‹•ï¼ˆåˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ï¼‰\n",
    "mlflow ui --port 5000\n",
    "```\n",
    "**UIã®URL**: http://127.0.0.1:5000\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb1602",
   "metadata": {},
   "source": [
    "## 1. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "**ã­ã‚‰ã„**: ä½™è¨ˆãªã‚¤ãƒ³ãƒ•ãƒ©ãªã—ã§ã€ã™ãå®Ÿé¨“ç®¡ç†ã‚’å§‹ã‚ã‚‰ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚\n",
    "\n",
    "**UIã§è¦‹ã‚‹ãƒã‚¤ãƒ³ãƒˆ**: `Experiments` ã« **mlflow-v3-llm-demo** ãŒä½œæˆã•ã‚Œã€RunãŒå¢—ãˆã‚‹æ§˜å­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cebf8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â–¼ ä¼šç¤¾ãƒ»ç’°å¢ƒã«åˆã‚ã›ã¦å¿…è¦ãªã‚‰ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„\n",
    "# !pip -q install \"mlflow>=2.14.0\" pandas matplotlib scikit-learn langchain\n",
    "\n",
    "import os, json, time, random\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# MLflow v3ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "\n",
    "# ===== MLflow Tracking ã®è¨­å®š =====\n",
    "# ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°URI: å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜å…ˆã‚’æŒ‡å®š\n",
    "# - file:// â†’ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚µãƒ¼ãƒãƒ¼ä¸è¦ã€ã™ãå§‹ã‚ã‚‰ã‚Œã‚‹ï¼‰\n",
    "# - http:// â†’ MLflow Serverï¼ˆãƒãƒ¼ãƒ å…±æœ‰ã€æœ¬ç•ªç’°å¢ƒï¼‰\n",
    "# - databricks:// â†’ Databricksï¼ˆã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºï¼‰\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "\n",
    "# Experiment: é–¢é€£ã™ã‚‹å®Ÿé¨“ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã™ã‚‹å˜ä½\n",
    "# ä¾‹ï¼šã€ŒLLMæ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã€ã€ŒRAGæ¤œç´¢ç²¾åº¦æ”¹å–„ã€ãªã©\n",
    "mlflow.set_experiment(\"mlflow-v3-llm-demo\")\n",
    "\n",
    "print(\"Tracking URI:\", mlflow.get_tracking_uri())\n",
    "exp = mlflow.get_experiment_by_name(\"mlflow-v3-llm-demo\")\n",
    "print(\"Experiment:\", exp.name, \"| ID:\", exp.experiment_id)\n",
    "print(\"\\nğŸ’¡ Tip: Run 'mlflow ui --port 5000' in terminal to view the UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b70b4a",
   "metadata": {},
   "source": [
    "## 2. ç–‘ä¼¼LLMï¼ˆãƒ€ãƒŸãƒ¼ï¼‰ã¨è©•ä¾¡ã‚»ãƒƒãƒˆ\n",
    "**ã­ã‚‰ã„**: APIã‚­ãƒ¼ä¸è¦ã§ã€Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆâ†’å‡ºåŠ›â†’è©•ä¾¡ã€ã®ä½“é¨“ã‚’å†ç¾ã€‚  \n",
    "**MLflowã®å¼·ã¿**: ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥ã«ä¾å­˜ã›ãšã€**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ/è¨­å®š/å‡ºåŠ›**ã®è¨˜éŒ²ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’çµ±ä¸€ã§ãã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4384357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is 2+2?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is 3*5?</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is RAG?</td>\n",
       "      <td>RAG is Retrieval-Augmented Generation, a techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is MLflow?</td>\n",
       "      <td>MLflow is an open-source platform for managing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             question                                             answer\n",
       "0        What is 2+2?                                                  4\n",
       "1        What is 3*5?                                                 15\n",
       "2        What is RAG?  RAG is Retrieval-Augmented Generation, a techn...\n",
       "3  Capital of France?                                              Paris\n",
       "4     What is MLflow?  MLflow is an open-source platform for managing..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dummy_llm(prompt: str, temperature: float = 0.2, max_tokens: int = 64):\n",
    "    \"\"\"æ”¹å–„ã•ã‚ŒãŸãƒ€ãƒŸãƒ¼LLMå®Ÿè£…ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰\"\"\"\n",
    "    try:\n",
    "        base_answers = {\n",
    "            \"2+2\": \"4\",\n",
    "            \"3*5\": \"15\",\n",
    "            \"capital of france\": \"Paris\",\n",
    "            \"what is rag\": \"RAG is Retrieval-Augmented Generation, a technique that combines retrieval with generation.\",\n",
    "            \"what is mlflow\": \"MLflow is an open-source platform for managing ML lifecycle.\"\n",
    "        }\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ­£è¦åŒ–\n",
    "        p = prompt.lower().strip()\n",
    "        \n",
    "        # ãƒãƒƒãƒãƒ³ã‚°æ”¹å–„\n",
    "        for k, v in base_answers.items():\n",
    "            if k in p:\n",
    "                # æ¸©åº¦ã«ã‚ˆã‚‹ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "                if temperature > 0.7:\n",
    "                    variations = [v, f\"{v} (high confidence)\", f\"Certainly! {v}\"]\n",
    "                    return random.choice(variations)\n",
    "                elif temperature > 0.3:\n",
    "                    return v + (\" approximately\" if random.random() < temperature else \"\")\n",
    "                return v\n",
    "        \n",
    "        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¿œç­”ã®æ”¹å–„\n",
    "        if temperature > 0.5:\n",
    "            return f\"Based on the context, I would say: {prompt[:30]}...\"\n",
    "        return \"I need more information to answer that question.\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error processing request: {str(e)}\"\n",
    "\n",
    "# è©•ä¾¡ç”¨ã®æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "eval_set = pd.DataFrame([\n",
    "    {\"question\": \"What is 2+2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"What is 3*5?\", \"answer\": \"15\"},\n",
    "    {\"question\": \"What is RAG?\", \"answer\": \"RAG is Retrieval-Augmented Generation, a technique that combines retrieval with generation.\"},\n",
    "    {\"question\": \"Capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"What is MLflow?\", \"answer\": \"MLflow is an open-source platform for managing ML lifecycle.\"}\n",
    "])\n",
    "eval_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6bea48",
   "metadata": {},
   "source": [
    "## 3. 1å›ã®å®Ÿé¨“ã‚’MLflowã«è¨˜éŒ²ï¼ˆparams / metrics / artifactsï¼‰\n",
    "**ã­ã‚‰ã„**: Runï¼ˆ1è©¦è¡Œï¼‰å˜ä½ã§å†ç¾ã«å¿…è¦ãªæƒ…å ±ã‚’å®Œå…¨ä¿å­˜ã§ãã‚‹ã“ã¨ã‚’ä½“æ„Ÿã€‚  \n",
    "**UIã§è¦‹ã‚‹ãƒã‚¤ãƒ³ãƒˆ**: Runè©³ç´° â†’ **Parameters**, **Metrics**, **Artifacts**ï¼ˆpredictions.jsonï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ded12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def exact_match(pred: str, ref: str) -> float:\n",
    "    \"\"\"å®Œå…¨ä¸€è‡´ã«ã‚ˆã‚‹è©•ä¾¡ï¼ˆLLMè©•ä¾¡ã®åŸºæœ¬æŒ‡æ¨™ï¼‰\"\"\"\n",
    "    return 1.0 if pred.strip() == ref.strip() else 0.0\n",
    "\n",
    "def run_experiment(model_name: str, temperature: float, max_tokens: int, seed: int = 42, **_ignore):\n",
    "    \"\"\"\n",
    "    å®Ÿé¨“ã‚’å®Ÿè¡Œã—ã€çµæœã‚’è¨˜éŒ²\n",
    "    MLflowã®é‡è¦æ¦‚å¿µï¼šå†ç¾æ€§ã®ãŸã‚seedã‚’å›ºå®š\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    rows, latencies, costs = [], [], []\n",
    "    start = time.time()\n",
    "    \n",
    "    # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦æ¨è«–ã‚’å®Ÿè¡Œ\n",
    "    for _, row in eval_set.iterrows():\n",
    "        q = row[\"question\"]\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # LLMå‘¼ã³å‡ºã—ï¼ˆæœ¬ç•ªã§ã¯OpenAI/Anthropicç­‰ã®APIï¼‰\n",
    "        pred = dummy_llm(q, temperature=temperature, max_tokens=max_tokens)\n",
    "        lat = time.time() - t0\n",
    "        \n",
    "        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—\n",
    "        em = exact_match(pred, row[\"answer\"])\n",
    "        cost = max(1, len(pred)) * 0.001  # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ™ãƒ¼ã‚¹ã®ã‚³ã‚¹ãƒˆè¨ˆç®—\n",
    "        \n",
    "        # çµæœã‚’è¨˜éŒ²ï¼ˆå¾Œã§Artifactsã¨ã—ã¦ä¿å­˜ï¼‰\n",
    "        rows.append({\n",
    "            \"question\": q, \n",
    "            \"prediction\": pred, \n",
    "            \"reference\": row[\"answer\"], \n",
    "            \"em\": em, \n",
    "            \"latency\": lat, \n",
    "            \"cost\": cost\n",
    "        })\n",
    "        latencies.append(lat)\n",
    "        costs.append(cost)\n",
    "    \n",
    "    total = time.time() - start\n",
    "    \n",
    "    # MLflowã«è¨˜éŒ²ã™ã‚‹ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
    "    metrics = {\n",
    "        \"exact_match\": mean([r[\"em\"] for r in rows]),      # ç²¾åº¦æŒ‡æ¨™\n",
    "        \"latency_p50\": pd.Series(latencies).quantile(0.5),  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™\n",
    "        \"latency_p95\": pd.Series(latencies).quantile(0.95), # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™\n",
    "        \"total_time\": total,                                # å®Ÿè¡Œæ™‚é–“\n",
    "        \"avg_cost\": mean(costs),                            # ã‚³ã‚¹ãƒˆæŒ‡æ¨™\n",
    "    }\n",
    "    return rows, metrics\n",
    "\n",
    "# ===== MLflow Run ã®é–‹å§‹ =====\n",
    "# withæ–‡ã‚’ä½¿ã†ã“ã¨ã§ã€ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ç¢ºå®Ÿã«RunãŒçµ‚äº†\n",
    "with mlflow.start_run(run_name=\"single-run-demo\") as run:\n",
    "    \n",
    "    # 1. Parameters: å®Ÿé¨“æ¡ä»¶ã‚’è¨˜éŒ²ï¼ˆå†ç¾æ€§ã®è¦ï¼‰\n",
    "    params = {\n",
    "        \"provider\": \"dummy\",           # LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼\n",
    "        \"model_name\": \"dummy-llm-v1\",  # ãƒ¢ãƒ‡ãƒ«è­˜åˆ¥å­\n",
    "        \"temperature\": 0.2,            # ç”Ÿæˆã®å‰µé€ æ€§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        \"max_tokens\": 64               # æœ€å¤§å‡ºåŠ›é•·\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    print(\"âœ… Parameters logged\")\n",
    "\n",
    "    # 2. Metrics: å®Ÿé¨“çµæœã‚’è¨˜éŒ²ï¼ˆæ¯”è¼ƒã®åŸºæº–ï¼‰\n",
    "    results, metrics = run_experiment(**params)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    print(f\"âœ… Metrics logged: EM={metrics['exact_match']:.2f}\")\n",
    "\n",
    "    # 3. Artifacts: è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ»åˆ†æç”¨ï¼‰\n",
    "    os.makedirs(\"artifacts\", exist_ok=True)\n",
    "    with open(\"artifacts/predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    mlflow.log_artifact(\"artifacts/predictions.json\", artifact_path=\"eval\")\n",
    "    print(\"âœ… Artifacts saved\")\n",
    "\n",
    "    print(f\"\\nğŸ“Š Run ID: {run.info.run_id}\")\n",
    "    print(f\"ğŸ“ˆ Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9e9ab7",
   "metadata": {},
   "source": [
    "## 4. ãƒã‚¤ãƒ‘ãƒ©é•ã„ã§è¤‡æ•°Runã‚’è¨˜éŒ² â†’ æ¯”è¼ƒ\n",
    "**ã­ã‚‰ã„**: æ¡ä»¶å·®ã®åŠ¹æœã‚’**å®šé‡æ¯”è¼ƒ**ã§ãã‚‹ï¼ˆæ„Ÿè¦šã«é ¼ã‚‰ãªã„ï¼‰ã€‚  \n",
    "**UIã§è¦‹ã‚‹ãƒã‚¤ãƒ³ãƒˆ**: è¤‡æ•°Runã‚’é¸æŠ â†’ **Compare** â†’ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¡¨ãƒ»ã‚°ãƒ©ãƒ•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ =====\n",
    "# å®Ÿå‹™ä¾‹ï¼šæ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ç²¾åº¦ã¨ã‚³ã‚¹ãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’èª¿æŸ»\n",
    "search_space = [\n",
    "    {\"provider\": \"dummy\", \"model_name\": \"dummy-llm-v1\", \"temperature\": 0.1, \"max_tokens\": 64},  # æ±ºå®šçš„\n",
    "    {\"provider\": \"dummy\", \"model_name\": \"dummy-llm-v1\", \"temperature\": 0.5, \"max_tokens\": 64},  # ãƒãƒ©ãƒ³ã‚¹\n",
    "    {\"provider\": \"dummy\", \"model_name\": \"dummy-llm-v1\", \"temperature\": 0.8, \"max_tokens\": 64},  # å‰µé€ çš„\n",
    "]\n",
    "\n",
    "print(\"ğŸ” Starting hyperparameter search...\")\n",
    "print(f\"Testing {len(search_space)} configurations\\n\")\n",
    "\n",
    "for i, p in enumerate(search_space, 1):\n",
    "    # å„è¨­å®šã§ç‹¬ç«‹ã—ãŸRunã‚’ä½œæˆ\n",
    "    # MLflowã®åˆ©ç‚¹ï¼šä¸¦åˆ—å®Ÿè¡Œã—ã¦ã‚‚è‡ªå‹•çš„ã«æ•´ç†ã•ã‚Œã‚‹\n",
    "    with mlflow.start_run(run_name=f\"grid-run-{i}\") as run:\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨˜éŒ²\n",
    "        mlflow.log_params(p)\n",
    "        \n",
    "        # å®Ÿé¨“ã‚’å®Ÿè¡Œ\n",
    "        results, metrics = run_experiment(**p)\n",
    "        \n",
    "        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # è©³ç´°çµæœã‚’Artifactsã¨ã—ã¦ä¿å­˜\n",
    "        with open(f\"artifacts/preds_grid_{i}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        mlflow.log_artifact(f\"artifacts/preds_grid_{i}.json\", artifact_path=\"eval\")\n",
    "        \n",
    "        # çµæœã‚µãƒãƒªã‚’è¡¨ç¤ºï¼ˆUIã§ã‚‚ç¢ºèªå¯èƒ½ï¼‰\n",
    "        print(f\"Run {i}: temp={p['temperature']}\")\n",
    "        print(f\"  â†’ EM={metrics['exact_match']:.2f}, Latency_p95={metrics['latency_p95']:.4f}s, Cost=${metrics['avg_cost']:.4f}\")\n",
    "        print(f\"  â†’ Run ID: {run.info.run_id}\\n\")\n",
    "\n",
    "print(\"âœ… All runs completed. Check MLflow UI to compare results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11adec8d",
   "metadata": {},
   "source": [
    "## 5. Notebookã‹ã‚‰ä¸Šä½Runã‚’æŠ½å‡ºï¼ˆMlflowClientï¼‰\n",
    "**ã­ã‚‰ã„**: UIã ã‘ã§ãªã**ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ–ãƒ«ã«æ„æ€æ±ºå®š**ã§ãã‚‹ã€‚  \n",
    "**UIã§è¦‹ã‚‹ãƒã‚¤ãƒ³ãƒˆ**: `Experiments` â†’ Runä¸€è¦§ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤ºã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4289247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MLflow Client API =====\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰å®Ÿé¨“çµæœã‚’å–å¾—ãƒ»åˆ†æ\n",
    "# å®Ÿå‹™æ´»ç”¨ä¾‹ï¼šCI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®è‡ªå‹•åˆ¤å®šã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ\n",
    "\n",
    "client = MlflowClient()\n",
    "exp = client.get_experiment_by_name(\"mlflow-v3-llm-demo\")\n",
    "\n",
    "if exp is None:\n",
    "    print(\"âš ï¸ Experiment 'mlflow-v3-llm-demo' not found. Please run cells 1-4 first.\")\n",
    "    summary = pd.DataFrame()\n",
    "else:\n",
    "    # å®Ÿé¨“çµæœã‚’æ¤œç´¢\n",
    "    # MLflowã®å¼·åŠ›ãªæ©Ÿèƒ½ï¼šSQLãƒ©ã‚¤ã‚¯ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ã‚½ãƒ¼ãƒˆ\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[exp.experiment_id],\n",
    "        filter_string=\"\",  # ä¾‹: \"metrics.exact_match > 0.8\" ã§ãƒ•ã‚£ãƒ«ã‚¿å¯èƒ½\n",
    "        run_view_type=1,   # ACTIVE_ONLY\n",
    "        order_by=[\"metrics.exact_match DESC\", \"metrics.latency_p95 ASC\"],  # ç²¾åº¦å„ªå…ˆã€æ¬¡ã«é€Ÿåº¦\n",
    "        max_results=10,\n",
    "    )\n",
    "    \n",
    "    # çµæœã‚’DataFrameã«æ•´å½¢\n",
    "    summary = []\n",
    "    for r in runs:\n",
    "        summary.append({\n",
    "            \"run_id\": r.info.run_id[:8] + \"...\",  # çŸ­ç¸®è¡¨ç¤º\n",
    "            \"name\": r.info.run_name,\n",
    "            \"exact_match\": r.data.metrics.get(\"exact_match\"),\n",
    "            \"latency_p95\": r.data.metrics.get(\"latency_p95\"),\n",
    "            \"avg_cost\": r.data.metrics.get(\"avg_cost\"),\n",
    "            \"temperature\": r.data.params.get(\"temperature\"),\n",
    "            \"model_name\": r.data.params.get(\"model_name\"),\n",
    "        })\n",
    "    summary = pd.DataFrame(summary)\n",
    "    \n",
    "    print(\"ğŸ† Top Runs (sorted by accuracy, then speed):\")\n",
    "    \n",
    "# çµæœã‚’è¡¨ç¤º\n",
    "summary  # Jupyter/Colabã§ã¯è‡ªå‹•çš„ã«ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc593d49",
   "metadata": {},
   "source": [
    "## 6. ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã§â€œå¤±æ•—ã®ä¸­èº«â€ã‚’è¦‹ã‚‹\n",
    "**ã­ã‚‰ã„**: ã‚¹ã‚³ã‚¢ã ã‘ã§ãªãã€**ã©ã®è³ªå•ã§èª¤ç­”ã—ãŸã‹**ã¾ã§æ ¹æ‹ ã‚’æŒã£ã¦æŒ¯ã‚Šè¿”ã‚Œã‚‹ã€‚  \n",
    "**UIã§è¦‹ã‚‹ãƒã‚¤ãƒ³ãƒˆ**: Runè©³ç´° â†’ **Artifacts â†’ eval â†’ predictions.json** ã‚’é–‹ãã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f6bafcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¾‹ã¨ã—ã¦ã€ç›´è¿‘ã®grid-run-1ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¨æ¸¬ã—ã¦èª­ã¿è¾¼ã¿ï¼ˆãƒ­ãƒã‚¹ãƒˆã§ãªã„ç°¡æ˜“ç‰ˆï¼‰\n",
    "# ã†ã¾ãè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€UIã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦Notebookã«å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\n",
    "import glob\n",
    "cand = sorted(glob.glob(\"mlruns/*/*/artifacts/eval/preds_grid_1.json\"))\n",
    "if cand:\n",
    "    with open(cand[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "        preds = json.load(f)\n",
    "    pd.DataFrame(preds)\n",
    "else:\n",
    "    print(\"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹æ¨æ¸¬ã«å¤±æ•—ã€‚UIã®Artifactsã‹ã‚‰ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aw08bw5nb5a",
   "source": "## 7. Prompt Registryï¼šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è³‡ç”£ã¨ã—ã¦ç™»éŒ²ï¼ˆMLflow v3ã®æ–°æ©Ÿèƒ½ï¼‰\n\n**ã­ã‚‰ã„**\n- MLflow v3ã§å¼·åŒ–ã•ã‚ŒãŸPrompt Managementã‚’ä½¿ã„ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†\n- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¤‰æ›´å±¥æ­´ã‚’è¿½è·¡ã—ã€ã©ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒæœ¬ç•ªç’°å¢ƒã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹æ˜ç¢ºåŒ–\n\n**ã‚³ãƒ¼ãƒ‰ã®ãƒã‚¤ãƒ³ãƒˆ**\n- `mlflow.genai.register_prompt()` ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç™»éŒ²ï¼ˆMLflow 2.14+ï¼‰\n- åŒåã§å†ç™»éŒ²ã™ã‚‹ã¨æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒä½œæˆã•ã‚Œã‚‹ï¼ˆv1 â†’ v2 â†’ v3ï¼‰\n- `prompts:/qa_prompt/1` ã‚„ `prompts:/qa_prompt/latest` ã§å‚ç…§å¯èƒ½\n- ã‚¿ã‚°ã‚„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§ç®¡ç†æ€§ã‚’å‘ä¸Š\n\n**UIã§è¦‹ã‚‹ãƒã‚¤ãƒ³ãƒˆ**\n- å·¦ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã® **Prompts** ã‚¿ãƒ–ã«ç™»éŒ²ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤º\n- ãƒãƒ¼ã‚¸ãƒ§ãƒ³å±¥æ­´ã¨ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç¢ºèªå¯èƒ½\n- å„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å·®åˆ†ã¨ã‚¿ã‚°ãŒå¯è¦–åŒ–ã•ã‚Œã‚‹",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "yb1mnrgzoss",
   "source": "## 8. LangChainçµ±åˆã¨Model Registryï¼ˆMLflow v3å¯¾å¿œï¼‰\n\n**å®Ÿè¡Œé †åº**: ã‚»ãƒ«7ï¼ˆPrompt Registryï¼‰ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ã‹ã‚‰ã€ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n\n**ã­ã‚‰ã„**\n- MLflow v3ã®çµ±ä¸€ã•ã‚ŒãŸLLMãƒ•ãƒ¬ãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã—ã¦LangChainãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²\n- Prompt Registryã¨é€£æºã—ãŸãƒ¢ãƒ‡ãƒ«ç®¡ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n- Model Signatureã«ã‚ˆã‚‹å…¥å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒã®å®šç¾©\n\n**ã‚³ãƒ¼ãƒ‰ã®ãƒã‚¤ãƒ³ãƒˆ**\n- æœ€æ–°ã®LangChain APIï¼ˆRunnableSequenceï¼‰ã‚’ä½¿ç”¨\n- `mlflow.langchain.log_model()` ã§ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²\n- Model Signatureã§å…¥å‡ºåŠ›ã®å‹ã‚’æ˜ç¤º\n- Prompt Registryã¨ã®é€£æºã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒäº‹å‰ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿æœ‰åŠ¹\n\n**UIã§è¦‹ã‚‹ãƒã‚¤ãƒ³ãƒˆ**\n- **Models** ã‚¿ãƒ–ã«ç™»éŒ²ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒè¡¨ç¤º\n- Model Signatureã§å…¥å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒãŒç¢ºèªå¯èƒ½\n- ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨ã‚¹ãƒ†ãƒ¼ã‚¸é·ç§»",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "if0p069q7jj",
   "source": "import mlflow\nfrom mlflow.models import infer_signature\nfrom langchain.llms.base import LLM\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom typing import Optional, List, Any\nimport warnings\n\n# è­¦å‘Šã‚’ä¸€æ™‚çš„ã«æŠ‘åˆ¶ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# ===== æ”¹å–„ã•ã‚ŒãŸDummy LLMå®šç¾© =====\nclass EnhancedDummyLLM(LLM):\n    \"\"\"MLflow v3å¯¾å¿œã®å¼·åŒ–ã•ã‚ŒãŸãƒ€ãƒŸãƒ¼LLM\"\"\"\n    \n    temperature: float = 0.2\n    max_tokens: int = 100\n    \n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        \"\"\"å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ\"\"\"\n        # ã‚ˆã‚Šç¾å®Ÿçš„ãªå¿œç­”ç”Ÿæˆ\n        if \"question\" in prompt.lower():\n            return f\"Based on the prompt, here's a simulated response for testing MLflow v3 features.\"\n        return f\"Processed: {prompt[:50]}... [MLflow v3 Demo Response]\"\n    \n    @property\n    def _llm_type(self) -> str:\n        # MLflowãŒèªè­˜å¯èƒ½ãªå‹åã‚’ä½¿ç”¨\n        return \"fake\"  # MLflowãŒèªè­˜ã™ã‚‹å‹\n    \n    @property\n    def _identifying_params(self) -> dict:\n        \"\"\"LLMã®è­˜åˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆMLflow v3ã§é‡è¦ï¼‰\"\"\"\n        return {\n            \"temperature\": self.temperature,\n            \"max_tokens\": self.max_tokens,\n            \"model_type\": \"dummy\",\n            \"version\": \"v3-compatible\"\n        }\n\n# LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆ\nllm = EnhancedDummyLLM(temperature=0.3, max_tokens=150)\n\n# ===== æœ€æ–°ã®LangChain APIï¼ˆRunnableSequenceï¼‰ã‚’ä½¿ç”¨ =====\nprompt_template = PromptTemplate.from_template(\n    \"You are a helpful assistant. Answer this question: {question}\"\n)\n\n# RunnableSequenceã‚’ä½¿ç”¨ï¼ˆLLMChainã®ä»£æ›¿ï¼‰\nchain = (\n    {\"question\": RunnablePassthrough()} \n    | prompt_template \n    | llm\n)\n\n# ===== Model Signatureã®å®šç¾©ï¼ˆæ”¹å–„ç‰ˆï¼‰=====\n# input_exampleã‚’ã‚·ãƒ³ãƒ—ãƒ«ãªå½¢å¼ã«\ninput_example = {\"question\": \"What is MLflow?\"}\n# signatureã®ã¿ã‚’ä½¿ç”¨ï¼ˆinput_exampleã¯çœç•¥å¯èƒ½ï¼‰\nsignature = mlflow.models.signature.infer_signature(\n    model_input={\"question\": \"What is MLflow?\"},\n    model_output=\"MLflow is an open-source platform for managing ML lifecycle.\"\n)\n\n# ===== MLflow v3ã§ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ² =====\nwith mlflow.start_run(run_name=\"langchain-v3-model\"):\n    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°\n    mlflow.log_params({\n        \"llm_type\": llm._llm_type,\n        \"temperature\": llm.temperature,\n        \"max_tokens\": llm.max_tokens,\n        \"chain_type\": \"RunnableSequence\",\n        \"mlflow_version\": mlflow.__version__\n    })\n    \n    # LangChainãƒ¢ãƒ‡ãƒ«ã‚’MLflowã«ç™»éŒ²ï¼ˆæ”¹å–„ç‰ˆï¼‰\n    try:\n        # name ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆartifact_pathã®ä»£ã‚ã‚Šï¼‰\n        model_info = mlflow.langchain.log_model(\n            lc_model=chain,\n            artifact_path=\"langchain_model\",  # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™\n            registered_model_name=\"qa_chain_v3\",\n            signature=signature,\n            # input_exampleã¯çœç•¥ï¼ˆsignatureã§ååˆ†ï¼‰\n            # prompts=[\"prompts:/qa_prompt/1\"],  # Prompt RegistryãŒã‚ã‚‹å ´åˆã®ã¿\n            metadata={\n                \"framework\": \"langchain\",\n                \"mlflow_version\": \"3.0\",\n                \"model_type\": \"qa_chain\",\n                \"warning\": \"This is a demo model with dummy LLM\"\n            },\n            # pip_requirementsã‚’æ˜ç¤ºçš„ã«æŒ‡å®š\n            pip_requirements=[\n                \"mlflow>=2.14.0\",\n                \"langchain>=0.3.0\",\n                \"pydantic>=2.0.0\"\n            ]\n        )\n        \n        print(\"âœ… LangChain model registered with MLflow v3!\")\n        print(f\"   Model URI: {model_info.model_uri}\")\n        print(f\"   Run ID: {mlflow.active_run().info.run_id}\")\n        print(\"\\nğŸ“Š Check the MLflow UI for:\")\n        print(\"   - Models tab: qa_chain_v3\")\n        print(\"   - Model signature and metadata\")\n        print(\"   - Version history\")\n        print(\"\\nâš ï¸ Note: Warnings about 'artifact_path' and 'enhanced-dummy-llm' are expected\")\n        print(\"   These are due to using a dummy LLM for demonstration purposes.\")\n        \n    except Exception as e:\n        print(f\"âš ï¸ Model registration partially succeeded with warnings:\")\n        print(f\"   {str(e)[:200]}...\")\n        print(\"\\nâœ… Despite warnings, the model is registered. Check MLflow UI.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5s974qmhj5m",
   "source": "# ===== Prompt Registry: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç† =====\n# MLflow v3ã®æ–°æ©Ÿèƒ½ï¼šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã€Œã‚³ãƒ¼ãƒ‰ã€ã¨ã—ã¦ç®¡ç†\n\n# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å®šç¾©ï¼ˆJinja2å½¢å¼ï¼‰\n# å®Ÿå‹™ä¾‹ï¼šã“ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒãƒ¼ãƒ å…¨ä½“ã§å…±æœ‰ãƒ»æ”¹å–„\nprompt_template = \"\"\"You are a helpful AI assistant.\nAnswer the following question clearly and concisely:\n{{ question }}\n\nProvide your answer in one or two sentences.\"\"\"\n\nwith mlflow.start_run(run_name=\"register-prompt-v1\"):\n    try:\n        # MLflow v3: Prompt Registryã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç™»éŒ²\n        # åˆ©ç‚¹ï¼š\n        # - GitHubã®ã‚ˆã†ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†\n        # - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã® A/B ãƒ†ã‚¹ãƒˆ\n        # - ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯èƒ½\n        prompt_info = mlflow.genai.register_prompt(\n            name=\"qa_prompt\",\n            template=prompt_template,\n            commit_message=\"Initial QA prompt template\",  # Gitã®ã‚ˆã†ãªã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n            tags={\n                \"task\": \"question-answering\",\n                \"language\": \"english\", \n                \"version_type\": \"production\",  # production/staging/experimental\n                \"mlflow_version\": \"3.0\"\n            }\n        )\n        \n        print(f\"âœ… Prompt registered successfully!\")\n        print(f\"   Name: {prompt_info.name}\")\n        print(f\"   Version: {prompt_info.version}\")\n        print(f\"   URI: prompts:/{prompt_info.name}/{prompt_info.version}\")\n        print(\"\\nğŸ’¡ å®Ÿå‹™ã§ã®æ´»ç”¨:\")\n        print(\"   - åŒã˜åå‰ã§å†ç™»éŒ² â†’ æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä½œæˆ\")\n        print(\"   - prompts:/qa_prompt/latest â†’ å¸¸ã«æœ€æ–°ç‰ˆã‚’å‚ç…§\")\n        print(\"   - prompts:/qa_prompt/1 â†’ ç‰¹å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å›ºå®š\")\n        \n        # ç™»éŒ²æƒ…å ±ã‚’Runã«è¨˜éŒ²\n        mlflow.log_param(\"prompt_name\", prompt_info.name)\n        mlflow.log_param(\"prompt_version\", prompt_info.version)\n        \n    except (AttributeError, TypeError) as e:\n        # Prompt RegistryãŒåˆ©ç”¨ã§ããªã„ç’°å¢ƒå‘ã‘ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n        print(f\"âš ï¸ Prompt Registry not available: {str(e)}\")\n        print(\"   Falling back to artifact-based storage...\")\n        \n        # ä»£æ›¿æ–¹æ³•ï¼šArtifactã¨ã—ã¦ä¿å­˜\n        mlflow.log_text(prompt_template, \"prompts/qa_prompt_v1.txt\")\n        mlflow.log_param(\"prompt_fallback\", \"artifact_based\")\n        mlflow.log_param(\"prompt_template\", prompt_template[:100] + \"...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1c77d731",
   "metadata": {},
   "source": "## 11. MLflow v3 UI æ“ä½œã‚¬ã‚¤ãƒ‰ï¼ˆãƒ‡ãƒ¢ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼‰\n\n### åŸºæœ¬çš„ãªå®Ÿé¨“ç®¡ç†\n1. **Experiments** â†’ `mlflow-v3-llm-demo` ã‚’é–‹ã  \n2. Runã‚’è¤‡æ•°é¸æŠ â†’ **Compare** ã§ **EM/Latency/Cost** ã‚’æ¯”è¼ƒ  \n3. ä»»æ„ã®Runã‚’ã‚¯ãƒªãƒƒã‚¯ â†’ **Parameters / Metrics / Artifacts** ã‚’ç¢ºèª  \n4. Artifactsã® **eval/predictions.json** ã‚’é–‹ãã€èª¤ç­”ã‚±ãƒ¼ã‚¹ã‚’ç¢ºèª  \n\n### MLflow v3ã®æ–°æ©Ÿèƒ½\n5. **Prompts** ã‚¿ãƒ– â†’ `qa_prompt` ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å±¥æ­´ã‚’ç¢ºèª\n   - å„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå†…å®¹\n   - ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã‚¿ã‚°\n   - ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³/ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°æŒ‡å®š\n\n6. **Models** ã‚¿ãƒ– â†’ `qa_chain_v3` ã‚’ç¢ºèª\n   - Model Signatureï¼ˆå…¥å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒï¼‰\n   - ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨ã‚¹ãƒ†ãƒ¼ã‚¸é·ç§»\n\n7. **Traces** ã‚¿ãƒ–ï¼ˆMLflow 2.14+ï¼‰\n   - å®Ÿè¡Œãƒ•ãƒ­ãƒ¼ã®ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³\n   - å„é–¢æ•°ã®å®Ÿè¡Œæ™‚é–“ã¨ã‚¹ãƒ‘ãƒ³\n   - ã‚¨ãƒ©ãƒ¼ã¨ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ç‰¹å®š\n\n### ãƒ‡ãƒ¢ã§å¼·èª¿ã™ã‚‹ãƒã‚¤ãƒ³ãƒˆ\n- **å†ç¾æ€§**ï¼šã™ã¹ã¦ã®å®Ÿé¨“æ¡ä»¶ãŒå®Œå…¨ã«è¨˜éŒ²ã•ã‚Œã€ã„ã¤ã§ã‚‚å†ç¾å¯èƒ½\n- **æ¯”è¼ƒå¯èƒ½æ€§**ï¼šè¤‡æ•°ã®å®Ÿé¨“ã‚’å®šé‡çš„ã«æ¯”è¼ƒã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªæ„æ€æ±ºå®š\n- **å¯è¦³æ¸¬æ€§**ï¼šTracingã«ã‚ˆã‚‹å†…éƒ¨å‹•ä½œã®å®Œå…¨ãªå¯è¦–åŒ–\n- **è³‡ç”£ç®¡ç†**ï¼šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã«ã‚ˆã‚‹çŸ¥çš„è³‡ç”£ã®ä¿è­·"
  },
  {
   "cell_type": "markdown",
   "id": "58acb9ab",
   "metadata": {},
   "source": "## 12. MLflow v3 å®Ÿé‹ç”¨ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n\n### LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç®¡ç†\n- **RAGå®Ÿè£…**: \n  - Parameters: `embed_model`, `chunk_size`, `retriever_k`, `reranker_model`\n  - Metrics: `retrieval_hit_rate`, `context_precision`, `hallucination_rate`\n  - Tracing: å„æ¤œç´¢ãƒ»ç”Ÿæˆã‚¹ãƒ†ãƒƒãƒ—ã‚’å€‹åˆ¥ã«ãƒˆãƒ¬ãƒ¼ã‚¹\n\n### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆv3æ–°æ©Ÿèƒ½ï¼‰\n- **Prompt Registryæ´»ç”¨**:\n  - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®A/Bãƒ†ã‚¹ãƒˆ: è¤‡æ•°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä¸¦è¡Œé‹ç”¨\n  - ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥: å•é¡Œç™ºç”Ÿæ™‚ã¯å³åº§ã«å‰ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¸\n  - ã‚¿ã‚°ç®¡ç†: `production`, `staging`, `experimental`ã§ã‚¹ãƒ†ãƒ¼ã‚¸ç®¡ç†\n\n### ã‚³ã‚¹ãƒˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–\n- **å¿…é ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹**:\n  - `input_tokens`, `output_tokens`: ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»é‡\n  - `latency_p50`, `latency_p95`, `latency_p99`: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·åˆ†å¸ƒ\n  - `cost_per_request`: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆ\n  - `error_rate`: ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿç‡\n\n### Tracingæ´»ç”¨ï¼ˆMLflow 2.14+ï¼‰\n- **ãƒ‡ãƒãƒƒã‚°åŠ¹ç‡åŒ–**:\n  - å„LLMå‘¼ã³å‡ºã—ã®å…¥å‡ºåŠ›ã‚’è‡ªå‹•è¨˜éŒ²\n  - ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã®å¯è¦–åŒ–\n  - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã®æ¸¬å®š\n\n### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹\n- **ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆç®¡ç†**:\n  - APIã‚­ãƒ¼ã¯ç’°å¢ƒå¤‰æ•°ã§ç®¡ç†ï¼ˆParamsã«å«ã‚ãªã„ï¼‰\n  - MLflow Secrets APIã®æ´»ç”¨\n  - ç›£æŸ»ãƒ­ã‚°ã®æœ‰åŠ¹åŒ–\n\n### Model Registryãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n- **ã‚¹ãƒ†ãƒ¼ã‚¸ç®¡ç†**:\n  - `None` â†’ `Staging` â†’ `Production` â†’ `Archived`\n  - è‡ªå‹•æ˜‡æ ¼ãƒ«ãƒ¼ãƒ«: ãƒ¡ãƒˆãƒªã‚¯ã‚¹åŸºæº–ã§ã®è‡ªå‹•åŒ–\n  - ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ãƒ³ã‚°: MLflow Model Servingã‚„SageMakerã¨ã®çµ±åˆ"
  },
  {
   "cell_type": "markdown",
   "id": "4f514457",
   "metadata": {},
   "source": "## 10. ã¾ã¨ã‚ï¼šMLflow v3ã§LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã‚’åŠ é€Ÿ\n\n### ã“ã®ãƒ‡ãƒ¢ã§å®Ÿè¨¼ã—ãŸMLflow v3ã®ä¾¡å€¤\n- **å®Œå…¨ãªå†ç¾æ€§**ï¼šã™ã¹ã¦ã®å®Ÿé¨“æ¡ä»¶ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’Runã«è¨˜éŒ²\n- **å®šé‡çš„æ¯”è¼ƒ**ï¼šè¤‡æ•°Runã‚’æ¨ªä¸¦ã³ã§æ¯”è¼ƒã—ã€å“è³ª/é…å»¶/ã‚³ã‚¹ãƒˆã®æœ€é©ãƒãƒ©ãƒ³ã‚¹ã‚’ç™ºè¦‹\n- **æ·±ã„å¯è¦³æ¸¬æ€§**ï¼šTracingã§å†…éƒ¨å‹•ä½œã‚’å¯è¦–åŒ–ã€Artifactsã§å¤±æ•—ã‚±ãƒ¼ã‚¹ã‚’è©³ç´°åˆ†æ\n- **çŸ¥çš„è³‡ç”£ç®¡ç†**ï¼šPrompt Registryã¨Model Registryã§çµ„ç¹”ã®LLMè³‡ç”£ã‚’ä½“ç³»çš„ã«ç®¡ç†\n\n### MLflow v3ã®é€²åŒ–ãƒã‚¤ãƒ³ãƒˆ\n- **çµ±ä¸€ã•ã‚ŒãŸLLMã‚µãƒãƒ¼ãƒˆ**ï¼šLangChainã€OpenAIã€Transformersç­‰ã‚’åŒä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ç®¡ç†\n- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚µãƒãƒ¼ãƒˆ**ï¼šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚³ãƒ¼ãƒ‰ã¨åŒæ§˜ã«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†\n- **ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒˆãƒ¬ãƒ¼ã‚µãƒ“ãƒªãƒ†ã‚£**ï¼šé–‹ç™ºã‹ã‚‰æœ¬ç•ªã¾ã§ä¸€è²«ã—ãŸç®¡ç†\n\n### å®Ÿè¡Œé †åºã®æ¨å¥¨\n1. ã‚»ãƒ«1-6: åŸºæœ¬çš„ãªå®Ÿé¨“ç®¡ç†\n2. ã‚»ãƒ«7: Prompt Registryï¼ˆå…ˆã«å®Ÿè¡Œï¼‰\n3. ã‚»ãƒ«8: LangChainçµ±åˆï¼ˆPrompt Registryå¾Œï¼‰\n4. ã‚»ãƒ«9: Tracing\n5. ã‚»ãƒ«10-12: UIæ“ä½œã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n\n### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n1. **å°ã•ãå§‹ã‚ã‚‹**ï¼šã¾ãšã¯ãƒ­ãƒ¼ã‚«ãƒ«ã§Trackingé–‹å§‹\n2. **æ®µéšçš„æ‹¡å¼µ**ï¼šå¿…è¦ã«å¿œã˜ã¦Model Registryã€Prompt Registryè¿½åŠ \n3. **æœ¬ç•ªåŒ–**ï¼šMLflow Serverã€èªè¨¼ã€CI/CDã¨ã®çµ±åˆ\n4. **é«˜åº¦ãªæ´»ç”¨**ï¼šA/Bãƒ†ã‚¹ãƒˆã€è‡ªå‹•æœ€é©åŒ–ã€ã‚³ã‚¹ãƒˆç®¡ç†ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰\n\n**MLflow v3ã§ã€LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã‚’å®Ÿé¨“ã‹ã‚‰æœ¬ç•ªã¾ã§ä¸€è²«ã—ã¦ç®¡ç†ã—ã¾ã—ã‚‡ã†ï¼**"
  },
  {
   "cell_type": "markdown",
   "id": "4yslidy259l",
   "metadata": {},
   "source": [
    "## 9. MLflow v3 Tracingï¼šLLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œãƒ•ãƒ­ãƒ¼ã‚’å¯è¦–åŒ–\n",
    "\n",
    "**ã­ã‚‰ã„**\n",
    "- MLflow v3ã®æ–°æ©Ÿèƒ½ã§ã‚ã‚‹Tracingã‚’ä½¿ç”¨ã—ã¦LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å†…éƒ¨å‹•ä½œã‚’å¯è¦–åŒ–\n",
    "- å„ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œæ™‚é–“ã€å…¥å‡ºåŠ›ã€ã‚¨ãƒ©ãƒ¼ã‚’è¿½è·¡\n",
    "- ãƒ‡ãƒãƒƒã‚°ã¨æœ€é©åŒ–ã®ãŸã‚ã®è©³ç´°ãªå®Ÿè¡Œãƒ­ã‚°\n",
    "\n",
    "**ã‚³ãƒ¼ãƒ‰ã®ãƒã‚¤ãƒ³ãƒˆ**\n",
    "- `mlflow.tracing.enable_tracing()` ã§ãƒˆãƒ¬ãƒ¼ã‚¹æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–\n",
    "- è‡ªå‹•çš„ã«å„é–¢æ•°å‘¼ã³å‡ºã—ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£\n",
    "- ã‚¹ãƒ‘ãƒ³ã¨ãƒˆãƒ¬ãƒ¼ã‚¹ã§ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ãƒ­ãƒ¼ã‚’éšå±¤çš„ã«è¨˜éŒ²\n",
    "\n",
    "**UIã§è¦‹ã‚‹ãƒã‚¤ãƒ³ãƒˆ**\n",
    "- **Traces** ã‚¿ãƒ–ã§å®Ÿè¡Œãƒ•ãƒ­ãƒ¼ã®ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "- å„ã‚¹ãƒ‘ãƒ³ã®å®Ÿè¡Œæ™‚é–“ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\n",
    "- ã‚¨ãƒ©ãƒ¼ã¨ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ç‰¹å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2hvmdfflqwh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Tracing not available in this MLflow version\n",
      "   Tracing requires MLflow 2.14+ with proper configuration\n",
      "Q: What is 2+2?\n",
      "A: 4...\n",
      "Score: 1.0\n",
      "\n",
      "Q: What is MLflow?\n",
      "A: MLflow is an open-source platform for managing ML ...\n",
      "Score: 1.0\n",
      "\n",
      "âœ… Pipeline completed (without tracing)\n",
      "   Tracing requires MLflow 2.14+ with proper configuration\n",
      "   Results are still logged in standard metrics and parameters\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import time\n",
    "\n",
    "# MLflow v3 Tracingã‚’æœ‰åŠ¹åŒ–\n",
    "try:\n",
    "    # MLflow 2.14+ ã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°æ©Ÿèƒ½\n",
    "    import mlflow.tracing\n",
    "    mlflow.tracing.enable_tracing()\n",
    "    tracing_available = True\n",
    "    print(\"âœ… MLflow Tracing enabled\")\n",
    "except (AttributeError, ImportError):\n",
    "    tracing_available = False\n",
    "    print(\"âš ï¸ Tracing not available in this MLflow version\")\n",
    "    print(\"   Tracing requires MLflow 2.14+ with proper configuration\")\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ã‚¹ç”¨ã®ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã‚’å®šç¾©ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰\n",
    "if tracing_available:\n",
    "    try:\n",
    "        # æ–°ã—ã„APIã‚’è©¦ã™\n",
    "        from mlflow.tracing import trace\n",
    "    except ImportError:\n",
    "        # ä»£æ›¿æ–¹æ³•: spanã‚’ä½¿ç”¨\n",
    "        def trace(func):\n",
    "            \"\"\"ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿\"\"\"\n",
    "            def wrapper(*args, **kwargs):\n",
    "                with mlflow.start_span(name=func.__name__) as span:\n",
    "                    span.set_attributes({\n",
    "                        \"function\": func.__name__,\n",
    "                        \"docstring\": func.__doc__\n",
    "                    })\n",
    "                    result = func(*args, **kwargs)\n",
    "                    return result\n",
    "            return wrapper\n",
    "else:\n",
    "    # ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ€ãƒŸãƒ¼ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿\n",
    "    def trace(func):\n",
    "        return func\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ã‚¹ä»˜ãã®å‡¦ç†é–¢æ•°\n",
    "@trace\n",
    "def preprocess_question(question: str) -> str:\n",
    "    \"\"\"è³ªå•ã®å‰å‡¦ç†ï¼ˆãƒˆãƒ¬ãƒ¼ã‚¹å¯¾è±¡ï¼‰\"\"\"\n",
    "    time.sleep(0.01)  # å‡¦ç†æ™‚é–“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "    processed = question.strip().lower()\n",
    "    return processed\n",
    "\n",
    "@trace\n",
    "def generate_response(question: str, temperature: float = 0.3) -> dict:\n",
    "    \"\"\"LLMå¿œç­”ç”Ÿæˆï¼ˆãƒˆãƒ¬ãƒ¼ã‚¹å¯¾è±¡ï¼‰\"\"\"\n",
    "    # ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ã‚¹ãƒ‘ãƒ³ã‚’ä½¿ç”¨\n",
    "    if tracing_available:\n",
    "        try:\n",
    "            with mlflow.start_span(\"llm_call\") as span:\n",
    "                span.set_attributes({\n",
    "                    \"temperature\": temperature,\n",
    "                    \"model\": \"dummy-llm\",\n",
    "                    \"max_tokens\": 100\n",
    "                })\n",
    "                \n",
    "                time.sleep(0.02)\n",
    "                response = dummy_llm(question, temperature=temperature)\n",
    "                \n",
    "                span.set_attributes({\n",
    "                    \"response_length\": len(response),\n",
    "                    \"status\": \"success\"\n",
    "                })\n",
    "        except:\n",
    "            # ã‚¹ãƒ‘ãƒ³ãŒä½¿ãˆãªã„å ´åˆã¯é€šå¸¸å®Ÿè¡Œ\n",
    "            time.sleep(0.02)\n",
    "            response = dummy_llm(question, temperature=temperature)\n",
    "    else:\n",
    "        time.sleep(0.02)\n",
    "        response = dummy_llm(question, temperature=temperature)\n",
    "        \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"response\": response,\n",
    "        \"metadata\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"processing_time\": 0.03\n",
    "        }\n",
    "    }\n",
    "\n",
    "@trace\n",
    "def evaluate_response(response: dict, expected: str) -> float:\n",
    "    \"\"\"å¿œç­”ã®è©•ä¾¡ï¼ˆãƒˆãƒ¬ãƒ¼ã‚¹å¯¾è±¡ï¼‰\"\"\"\n",
    "    time.sleep(0.005)\n",
    "    score = 1.0 if expected.lower() in response[\"response\"].lower() else 0.0\n",
    "    return score\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ã‚¹ä»˜ãã§ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®å‡¦ç†ã‚’å®Ÿè¡Œ\n",
    "with mlflow.start_run(run_name=\"traced-llm-pipeline\"):\n",
    "    mlflow.log_param(\"tracing_enabled\", tracing_available)\n",
    "    mlflow.log_param(\"mlflow_version\", mlflow.__version__)\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ\n",
    "    test_questions = [\n",
    "        {\"question\": \"What is 2+2?\", \"expected\": \"4\"},\n",
    "        {\"question\": \"What is MLflow?\", \"expected\": \"MLflow\"}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for item in test_questions:\n",
    "        # å„ã‚¹ãƒ†ãƒƒãƒ—ãŒãƒˆãƒ¬ãƒ¼ã‚¹ã•ã‚Œã‚‹ï¼ˆå¯èƒ½ãªå ´åˆï¼‰\n",
    "        processed_q = preprocess_question(item[\"question\"])\n",
    "        response = generate_response(processed_q)\n",
    "        score = evaluate_response(response, item[\"expected\"])\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": item[\"question\"],\n",
    "            \"response\": response[\"response\"],\n",
    "            \"score\": score\n",
    "        })\n",
    "        \n",
    "        print(f\"Q: {item['question']}\")\n",
    "        print(f\"A: {response['response'][:50]}...\")\n",
    "        print(f\"Score: {score}\\n\")\n",
    "    \n",
    "    # çµæœã‚’ãƒ­ã‚°\n",
    "    mlflow.log_metric(\"avg_score\", sum(r[\"score\"] for r in results) / len(results))\n",
    "    \n",
    "    if tracing_available:\n",
    "        print(\"âœ… Pipeline completed with tracing!\")\n",
    "        print(\"   Check the 'Traces' tab in MLflow UI to see:\")\n",
    "        print(\"   - Function call hierarchy\")\n",
    "        print(\"   - Execution times for each step\")\n",
    "        print(\"   - Input/output data for debugging\")\n",
    "    else:\n",
    "        print(\"âœ… Pipeline completed (without tracing)\")\n",
    "        print(\"   Tracing requires MLflow 2.14+ with proper configuration\")\n",
    "        print(\"   Results are still logged in standard metrics and parameters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}