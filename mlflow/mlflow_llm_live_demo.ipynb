{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e379cac6",
   "metadata": {},
   "source": [
    "# MLflow × LLM ライブデモ\n",
    "\n",
    "**目的**: MLflowでLLM実験（プロンプト／設定／出力／メトリクス）を記録・比較する流れを、最小構成でライブで見せます。\n",
    "\n",
    "**構成**\n",
    "1. セットアップ\n",
    "2. 疑似LLM（ダミー）でプロンプト実行\n",
    "3. MLflowで params / metrics / artifacts を記録\n",
    "4. 複数Runを並べて比較（UI & Python）\n",
    "5. （任意）実API連携の型/Tracingのヒント\n",
    "\n",
    "> 事前に別ターミナルで `mlflow ui` を実行しておくと、記録がすぐに見られます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30d19d9",
   "metadata": {},
   "source": [
    "## 1. セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9a705a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/28 12:09:38 INFO mlflow.tracking.fluent: Experiment with name 'mlflow-llm-live-demo' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI: file:./mlruns\n",
      "Experiment set to: mlflow-llm-live-demo\n"
     ]
    }
   ],
   "source": [
    "import os, json, time, random\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# ローカルにトラッキング（サーバ不要）\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"mlflow-llm-live-demo\")\n",
    "\n",
    "print(\"Tracking URI:\", mlflow.get_tracking_uri())\n",
    "print(\"Experiment set to:\", mlflow.get_experiment_by_name(\"mlflow-llm-live-demo\").name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf33f51",
   "metadata": {},
   "source": [
    "## 2. 疑似LLM（ダミー関数）\n",
    "実APIに依存しないでデモできるよう、簡単なルールベースのダミー生成器を用意します。\n",
    "温度（`temperature`）などの設定で、少し出力が揺れるようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "275e41a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2+2?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3*5?</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is RAG?</td>\n",
       "      <td>RAG is Retrieval-Augmented Generation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             question                                  answer\n",
       "0                2+2?                                       4\n",
       "1                3*5?                                      15\n",
       "2        What is RAG?  RAG is Retrieval-Augmented Generation.\n",
       "3  Capital of France?                                   Paris"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dummy_llm(prompt: str, temperature: float = 0.2, max_tokens: int = 64):\n",
    "    \"\"\"非常に単純なダミー生成。温度でノイズを加える。\"\"\"\n",
    "    base_answers = {\n",
    "        \"2+2\": \"4\",\n",
    "        \"3*5\": \"15\",\n",
    "        \"capital of france\": \"Paris\",\n",
    "        \"what is rag\": \"RAG is Retrieval-Augmented Generation.\"\n",
    "    }\n",
    "    p = prompt.lower()\n",
    "    for k, v in base_answers.items():\n",
    "        if k in p:\n",
    "            # 温度が高いとたまにバリエーション\n",
    "            if temperature > 0.5 and random.random() < min(temperature, 0.9):\n",
    "                return v + \" (approx)\"\n",
    "            return v\n",
    "\n",
    "    # 未知質問は適当回答（温度高いほど冗長）\n",
    "    extra = \" Definitely.\" if temperature > 0.5 else \"\"\n",
    "    return \"I'm not sure.\" + extra\n",
    "\n",
    "# 簡単なデータセット\n",
    "eval_set = pd.DataFrame([\n",
    "    {\"question\": \"2+2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"3*5?\", \"answer\": \"15\"},\n",
    "    {\"question\": \"What is RAG?\", \"answer\": \"RAG is Retrieval-Augmented Generation.\"},\n",
    "    {\"question\": \"Capital of France?\", \"answer\": \"Paris\"}\n",
    "])\n",
    "eval_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c9e843",
   "metadata": {},
   "source": [
    "## 3. 1回の実験をMLflowに記録\n",
    "- `params`: モデル名・温度・max_tokens など\n",
    "- `metrics`: 正答率、平均レイテンシ、（擬似）トークンコスト\n",
    "- `artifacts`: 予測詳細 `predictions.json`（プロンプト、出力、正解など）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b4384a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 40f14cc0c34b497899f0e5e1b70b7737\n",
      "Metrics: {'exact_match': 1.0, 'latency_p50': np.float64(2.0265579223632812e-06), 'latency_p95': np.float64(3.5643577575683588e-06), 'total_time': 0.0004811286926269531, 'avg_cost': 0.0115}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from statistics import mean\n",
    "\n",
    "def exact_match(pred: str, ref: str) -> float:\n",
    "    return 1.0 if pred.strip() == ref.strip() else 0.0\n",
    "\n",
    "def run_experiment(model_name: str, temperature: float, max_tokens: int, seed: int = 42, **_):\n",
    "    import time, random, pandas as pd\n",
    "    from statistics import mean\n",
    "\n",
    "    random.seed(seed)\n",
    "    results, latencies, token_costs = [], [], []\n",
    "\n",
    "    start = time.time()\n",
    "    for _, row in eval_set.iterrows():\n",
    "        q = row[\"question\"]\n",
    "        t0 = time.time()\n",
    "        pred = dummy_llm(q, temperature=temperature, max_tokens=max_tokens)\n",
    "        lat = time.time() - t0\n",
    "\n",
    "        acc = 1.0 if pred.strip() == row[\"answer\"].strip() else 0.0\n",
    "        cost = max(1, len(pred)) * 0.001  # 仮のコスト\n",
    "        results.append({\"question\": q, \"prediction\": pred, \"reference\": row[\"answer\"], \"em\": acc, \"latency\": lat, \"cost\": cost})\n",
    "        latencies.append(lat)\n",
    "        token_costs.append(cost)\n",
    "    total = time.time() - start\n",
    "\n",
    "    metrics = {\n",
    "        \"exact_match\": mean([r[\"em\"] for r in results]),\n",
    "        \"latency_p50\": pd.Series(latencies).quantile(0.5),\n",
    "        \"latency_p95\": pd.Series(latencies).quantile(0.95),\n",
    "        \"total_time\": total,\n",
    "        \"avg_cost\": mean(token_costs)\n",
    "    }\n",
    "    return results, metrics\n",
    "    \n",
    "with mlflow.start_run(run_name=\"demo-single-run\") as run:\n",
    "    params = {\"provider\": \"dummy\", \"model_name\": \"dummy-llm-v1\", \"temperature\": 0.2, \"max_tokens\": 64}\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    results, metrics = run_experiment(**params)\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # 予測詳細をartifactとして保存\n",
    "    os.makedirs(\"artifacts\", exist_ok=True)\n",
    "    with open(\"artifacts/predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    mlflow.log_artifact(\"artifacts/predictions.json\", artifact_path=\"eval\")\n",
    "\n",
    "    print(\"Run ID:\", run.info.run_id)\n",
    "    print(\"Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef33a6c8",
   "metadata": {},
   "source": [
    "## 4. ハイパラを変えて複数Runを記録 → 比較\n",
    "温度をいくつか変えて、正答率・レイテンシ・コストがどう変わるか比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70aeec39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged: b08fa621833e4505a5e63ae587b35ff6  -> EM=1.00, p95=0.0000, cost=0.0115\n",
      "Logged: 876c755b9f0742898b10757462f9332c  -> EM=1.00, p95=0.0000, cost=0.0115\n",
      "Logged: ae3f70e97bef48439cde6488fd0da2af  -> EM=0.00, p95=0.0000, cost=0.0205\n"
     ]
    }
   ],
   "source": [
    "\n",
    "search_space = [\n",
    "    {\"provider\": \"dummy\", \"model_name\": \"dummy-llm-v1\", \"temperature\": 0.1, \"max_tokens\": 64},\n",
    "    {\"provider\": \"dummy\", \"model_name\": \"dummy-llm-v1\", \"temperature\": 0.5, \"max_tokens\": 64},\n",
    "    {\"provider\": \"dummy\", \"model_name\": \"dummy-llm-v1\", \"temperature\": 0.8, \"max_tokens\": 64},\n",
    "]\n",
    "\n",
    "for i, p in enumerate(search_space, 1):\n",
    "    with mlflow.start_run(run_name=f\"grid-run-{i}\") as run:\n",
    "        mlflow.log_params(p)\n",
    "        results, metrics = run_experiment(**p)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        with open(f\"artifacts/preds_grid_{i}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        mlflow.log_artifact(f\"artifacts/preds_grid_{i}.json\", artifact_path=\"eval\")\n",
    "        print(f\"Logged: {run.info.run_id}  -> EM={metrics['exact_match']:.2f}, p95={metrics['latency_p95']:.4f}, cost={metrics['avg_cost']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b63419",
   "metadata": {},
   "source": [
    "## 5. Notebook上で上位Runを抽出（UIと併用）\n",
    "MLflow UIでの比較に加え、Pythonからも一覧を取得して“PM視点”の意思決定に使えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3dee4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>name</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>latency_p95</th>\n",
       "      <th>avg_cost</th>\n",
       "      <th>temperature</th>\n",
       "      <th>model_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97a06eccd4a5481c8d6a40177ffa97ee</td>\n",
       "      <td>grid-run-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.1</td>\n",
       "      <td>dummy-llm-v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c41bd7331ff3438d93e0a78f7c3f484f</td>\n",
       "      <td>grid-run-2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.5</td>\n",
       "      <td>dummy-llm-v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7fa384aca6d0428b81f76f066472c60d</td>\n",
       "      <td>demo-single-run</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.2</td>\n",
       "      <td>dummy-llm-v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54a12eab529f45dbace8a76099f5cbf3</td>\n",
       "      <td>grid-run-3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.8</td>\n",
       "      <td>dummy-llm-v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4ae5924a8dcf4dfba92fedc3edf11154</td>\n",
       "      <td>demo-single-run</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>dummy-llm-v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             run_id             name  exact_match  \\\n",
       "0  97a06eccd4a5481c8d6a40177ffa97ee       grid-run-1          1.0   \n",
       "1  c41bd7331ff3438d93e0a78f7c3f484f       grid-run-2          1.0   \n",
       "2  7fa384aca6d0428b81f76f066472c60d  demo-single-run          1.0   \n",
       "3  54a12eab529f45dbace8a76099f5cbf3       grid-run-3          0.0   \n",
       "4  4ae5924a8dcf4dfba92fedc3edf11154  demo-single-run          NaN   \n",
       "\n",
       "   latency_p95  avg_cost temperature    model_name  \n",
       "0     0.000003    0.0115         0.1  dummy-llm-v1  \n",
       "1     0.000004    0.0115         0.5  dummy-llm-v1  \n",
       "2     0.000011    0.0115         0.2  dummy-llm-v1  \n",
       "3     0.000004    0.0205         0.8  dummy-llm-v1  \n",
       "4          NaN       NaN         0.2  dummy-llm-v1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client = MlflowClient()\n",
    "exp = client.get_experiment_by_name(\"mlflow-llm-live-demo\")\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[exp.experiment_id],\n",
    "    filter_string=\"\",\n",
    "    run_view_type=1,\n",
    "    order_by=[\"metrics.exact_match DESC\", \"metrics.latency_p95 ASC\"],\n",
    "    max_results=5,\n",
    ")\n",
    "\n",
    "summary = []\n",
    "for r in runs:\n",
    "    summary.append({\n",
    "        \"run_id\": r.info.run_id,\n",
    "        \"name\": r.info.run_name,\n",
    "        \"exact_match\": r.data.metrics.get(\"exact_match\"),\n",
    "        \"latency_p95\": r.data.metrics.get(\"latency_p95\"),\n",
    "        \"avg_cost\": r.data.metrics.get(\"avg_cost\"),\n",
    "        \"temperature\": r.data.params.get(\"temperature\"),\n",
    "        \"model_name\": r.data.params.get(\"model_name\"),\n",
    "    })\n",
    "\n",
    "pd.DataFrame(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4340e9",
   "metadata": {},
   "source": [
    "## 6. アーティファクト（予測詳細）を読み出して可視化\n",
    "どの質問で失敗しているかを具体的に確認できます（ライブでは1件だけ表示）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc8f21b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 直近のRunのpredictions.jsonをロード（簡易）\n",
    "last_run = runs[0]\n",
    "art_uri = last_run.info.artifact_uri\n",
    "pred_path = art_uri.replace(\"file://\", \"\") + \"/eval/preds_grid_1.json\"  # 例: 最初のグリッドRun\n",
    "\n",
    "try:\n",
    "    with open(pred_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        preds = json.load(f)\n",
    "    pd.DataFrame(preds)\n",
    "except FileNotFoundError:\n",
    "    print(\"アーティファクトパスの自動特定に失敗した場合は、UIからダウンロードして確認してください。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17076f0",
   "metadata": {},
   "source": [
    "## 7. （任意）実API連携・Tracingの型\n",
    "以下は“どこを差し替えれば実APIになるか”のガイドです。セキュリティ上、ライブでは鍵を使わない構成でOK。\n",
    "\n",
    "- OpenAIやAnthropic等のSDK呼び出し部分を `dummy_llm()` の中で置換\n",
    "- `mlflow.log_params()` に `provider`, `model_version`, `api_base` なども追加\n",
    "- **Tracing** を使う場合は、各プロバイダの auto-log / コールバックを有効化\n",
    "    - 例: `mlflow.openai.autolog()`（使用可否は環境・バージョン依存）\n",
    "- RAGなら、`embed_model`, `chunk_size`, `retriever_k`, `reranker` などを **params** に、\n",
    "  `retrieval_hit_rate`, `context_precision`, `hallucination_rate` などを **metrics** に記録"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c1932",
   "metadata": {},
   "source": [
    "## 8. 補助：MLflow UI の開き方\n",
    "ローカル端末の別ターミナルで：\n",
    "\n",
    "```bash\n",
    "mlflow ui --port 5000\n",
    "```\n",
    "ブラウザで `http://127.0.0.1:5000` を開く。\n",
    "\n",
    "→ このNotebookで記録したRunが一覧表示され、**Compare** からメトリクス比較・アーティファクト確認ができます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
