{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4412e913",
   "metadata": {},
   "source": [
    "# MLflow × LLM ライブデモ（完全版）\n",
    "\n",
    "このノートは、**Tracking → 比較 → アーティファクト → RetrieverをModel登録 → Prompt Registry** まで、\n",
    "LLMアプリの実験管理に必要な最小構成をライブで見せるためのものです。\n",
    "\n",
    "**所要時間目安**: 10–15分  \n",
    "**前提**: 別ターミナルで `mlflow ui --port 5000` を起動しておくと、UIでRunがすぐ見られます。  \n",
    "**UIのURL**: http://127.0.0.1:5000\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb1602",
   "metadata": {},
   "source": [
    "## 1. セットアップ\n",
    "**ねらい**: 余計なインフラなしで、すぐ実験管理を始められることを示す。\n",
    "\n",
    "**UIで見るポイント**: `Experiments` に **mlflow-llm-live-demo** が作成され、Runが増える様子。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cebf8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ▼ 会社・環境に合わせて必要ならコメントアウトを外してください\n",
    "# !pip -q install mlflow pandas matplotlib scikit-learn\n",
    "\n",
    "import os, json, time, random\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# ローカルファイルにトラッキング（サーバ不要）\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"mlflow-llm-live-demo\")\n",
    "\n",
    "print(\"Tracking URI:\", mlflow.get_tracking_uri())\n",
    "exp = mlflow.get_experiment_by_name(\"mlflow-llm-live-demo\")\n",
    "print(\"Experiment:\", exp.name, \"| ID:\", exp.experiment_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b70b4a",
   "metadata": {},
   "source": [
    "## 2. 疑似LLM（ダミー）と評価セット\n",
    "**ねらい**: APIキー不要で「プロンプト→出力→評価」の体験を再現。  \n",
    "**MLflowの強み**: モデル種別に依存せず、**プロンプト/設定/出力**の記録フォーマットを統一できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dummy_llm(prompt: str, temperature: float = 0.2, max_tokens: int = 64):\n",
    "    \"\"\"非常に単純なダミー生成。温度で少し出力が揺れる。\"\"\"\n",
    "    base_answers = {\n",
    "        \"2+2\": \"4\",\n",
    "        \"3*5\": \"15\",\n",
    "        \"capital of france\": \"Paris\",\n",
    "        \"what is rag\": \"RAG is Retrieval-Augmented Generation.\"\n",
    "    }\n",
    "    p = prompt.lower()\n",
    "    for k, v in base_answers.items():\n",
    "        if k in p:\n",
    "            if temperature > 0.5 and random.random() < min(temperature, 0.9):\n",
    "                return v + \" (approx)\"\n",
    "            return v\n",
    "    return \"I'm not sure.\" + (\" Definitely.\" if temperature > 0.5 else \"\")\n",
    "\n",
    "# 評価用の簡易データ\n",
    "eval_set = pd.DataFrame([\n",
    "    {\"question\": \"2+2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"3*5?\", \"answer\": \"15\"},\n",
    "    {\"question\": \"What is RAG?\", \"answer\": \"RAG is Retrieval-Augmented Generation.\"},\n",
    "    {\"question\": \"Capital of France?\", \"answer\": \"Paris\"}\n",
    "])\n",
    "eval_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6bea48",
   "metadata": {},
   "source": [
    "## 3. 1回の実験をMLflowに記録（params / metrics / artifacts）\n",
    "**ねらい**: Run（1試行）単位で再現に必要な情報を完全保存できることを体感。  \n",
    "**UIで見るポイント**: Run詳細 → **Parameters**, **Metrics**, **Artifacts**（predictions.json）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ded12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statistics import mean\n",
    "\n",
    "def exact_match(pred: str, ref: str) -> float:\n",
    "    return 1.0 if pred.strip() == ref.strip() else 0.0\n",
    "\n",
    "def run_experiment(model_name: str, temperature: float, max_tokens: int, seed: int = 42, **_ignore):\n",
    "    random.seed(seed)\n",
    "    rows, latencies, costs = [], [], []\n",
    "    start = time.time()\n",
    "    for _, row in eval_set.iterrows():\n",
    "        q = row[\"question\"]\n",
    "        t0 = time.time()\n",
    "        pred = dummy_llm(q, temperature=temperature, max_tokens=max_tokens)\n",
    "        lat = time.time() - t0\n",
    "        em = exact_match(pred, row[\"answer\"])\n",
    "        cost = max(1, len(pred)) * 0.001  # 仮の「トークンコスト」\n",
    "        rows.append({\"question\": q, \"prediction\": pred, \"reference\": row[\"answer\"], \"em\": em, \"latency\": lat, \"cost\": cost})\n",
    "        latencies.append(lat); costs.append(cost)\n",
    "    total = time.time() - start\n",
    "    metrics = {\n",
    "        \"exact_match\": mean([r[\"em\"] for r in rows]),\n",
    "        \"latency_p50\": pd.Series(latencies).quantile(0.5),\n",
    "        \"latency_p95\": pd.Series(latencies).quantile(0.95),\n",
    "        \"total_time\": total,\n",
    "        \"avg_cost\": mean(costs),\n",
    "    }\n",
    "    return rows, metrics\n",
    "\n",
    "with mlflow.start_run(run_name=\"single-run-demo\") as run:\n",
    "    params = {\"provider\": \"dummy\", \"model_name\": \"dummy-llm-v1\", \"temperature\": 0.2, \"max_tokens\": 64}\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    results, metrics = run_experiment(**params)\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # 予測詳細をArtifactsに保存\n",
    "    os.makedirs(\"artifacts\", exist_ok=True)\n",
    "    with open(\"artifacts/predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    mlflow.log_artifact(\"artifacts/predictions.json\", artifact_path=\"eval\")\n",
    "\n",
    "    print(\"Run ID:\", run.info.run_id)\n",
    "    print(\"Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9e9ab7",
   "metadata": {},
   "source": [
    "## 4. ハイパラ違いで複数Runを記録 → 比較\n",
    "**ねらい**: 条件差の効果を**定量比較**できる（感覚に頼らない）。  \n",
    "**UIで見るポイント**: 複数Runを選択 → **Compare** → メトリクスの表・グラフ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_space = [\n",
    "    {\"provider\": \"dummy\", \"model_name\": \"dummy-llm-v1\", \"temperature\": 0.1, \"max_tokens\": 64},\n",
    "    {\"provider\": \"dummy\", \"model_name\": \"dummy-llm-v1\", \"temperature\": 0.5, \"max_tokens\": 64},\n",
    "    {\"provider\": \"dummy\", \"model_name\": \"dummy-llm-v1\", \"temperature\": 0.8, \"max_tokens\": 64},\n",
    "]\n",
    "\n",
    "for i, p in enumerate(search_space, 1):\n",
    "    with mlflow.start_run(run_name=f\"grid-run-{i}\") as run:\n",
    "        mlflow.log_params(p)\n",
    "        results, metrics = run_experiment(**p)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        with open(f\"artifacts/preds_grid_{i}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        mlflow.log_artifact(f\"artifacts/preds_grid_{i}.json\", artifact_path=\"eval\")\n",
    "        print(f\"Logged: {run.info.run_id}  -> EM={metrics['exact_match']:.2f}, p95={metrics['latency_p95']:.4f}, cost={metrics['avg_cost']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11adec8d",
   "metadata": {},
   "source": [
    "## 5. Notebookから上位Runを抽出（MlflowClient）\n",
    "**ねらい**: UIだけでなく**プログラマブルに意思決定**できる。  \n",
    "**UIで見るポイント**: `Experiments` → Run一覧のメトリクス表示と一致しているか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4289247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = MlflowClient()\n",
    "exp = client.get_experiment_by_name(\"mlflow-llm-live-demo\")\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[exp.experiment_id],\n",
    "    filter_string=\"\",\n",
    "    run_view_type=1,\n",
    "    order_by=[\"metrics.exact_match DESC\", \"metrics.latency_p95 ASC\"],\n",
    "    max_results=10,\n",
    ")\n",
    "summary = []\n",
    "for r in runs:\n",
    "    summary.append({\n",
    "        \"run_id\": r.info.run_id,\n",
    "        \"name\": r.info.run_name,\n",
    "        \"exact_match\": r.data.metrics.get(\"exact_match\"),\n",
    "        \"latency_p95\": r.data.metrics.get(\"latency_p95\"),\n",
    "        \"avg_cost\": r.data.metrics.get(\"avg_cost\"),\n",
    "        \"temperature\": r.data.params.get(\"temperature\"),\n",
    "        \"model_name\": r.data.params.get(\"model_name\"),\n",
    "    })\n",
    "pd.DataFrame(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc593d49",
   "metadata": {},
   "source": [
    "## 6. アーティファクトで“失敗の中身”を見る\n",
    "**ねらい**: スコアだけでなく、**どの質問で誤答したか**まで根拠を持って振り返れる。  \n",
    "**UIで見るポイント**: Run詳細 → **Artifacts → eval → predictions.json** を開く。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6bafcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 例として、直近のgrid-run-1のファイル名を推測して読み込み（ロバストでない簡易版）\n",
    "# うまく見つからない場合は、UIからダウンロードしてNotebookに再アップロードしてください。\n",
    "import glob\n",
    "cand = sorted(glob.glob(\"mlruns/*/*/artifacts/eval/preds_grid_1.json\"))\n",
    "if cand:\n",
    "    with open(cand[-1], \"r\", encoding=\"utf-8\") as f:\n",
    "        preds = json.load(f)\n",
    "    pd.DataFrame(preds)\n",
    "else:\n",
    "    print(\"ローカルパス推測に失敗。UIのArtifactsから確認してください。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c77d731",
   "metadata": {},
   "source": [
    "## 7. UI 操作ガイド（ここを見せる）\n",
    "1. **Experiments** → `mlflow-llm-live-demo` を開く  \n",
    "2. Runを複数選択 → **Compare** で **EM/Latency/Cost** を比較  \n",
    "3. 任意のRunをクリック → **Parameters / Metrics / Artifacts** を確認  \n",
    "4. Artifactsの **eval/predictions.json** を開き、誤答ケースを確認  \n",
    "5. ここまでで「**再現性・比較可能性・可観測性**」を実感してもらう"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58acb9ab",
   "metadata": {},
   "source": [
    "## 8. 実運用のヒント（口頭補足用）\n",
    "- **RAG** では `embed_model`, `chunk_size`, `retriever_k`, `reranker` などを **params** に、  \n",
    "  `retrieval_hit_rate`, `context_precision`, `hallucination_rate` などを **metrics** に。  \n",
    "- **コスト監視**：投入/出力トークン数、P95レイテンシを必ずログ。  \n",
    "- **シークレット**（APIキー）はparamsに入れない。環境変数・Secret管理を利用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773cd8c4",
   "metadata": {},
   "source": [
    "## 9. RetrieverをModelとして登録するデモ\n",
    "**ねらい**: LLM本体はいじらなくても、**再現性が重要なコンポーネント**（検索器）をモデルとして管理できる。  \n",
    "**UIで見るポイント**: Run詳細 → **Artifacts** に `corpus.json`、**Models** に登録済みモデル表示（バージョン管理）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ▼ TF-IDFベースの簡単なRetrieverをモデル化\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import mlflow.pyfunc\n",
    "\n",
    "corpus = [\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"RAG stands for Retrieval-Augmented Generation.\",\n",
    "    \"2+2 equals 4.\",\n",
    "    \"The capital of Japan is Tokyo.\"\n",
    "]\n",
    "doc_ids = [f\"doc{i}\" for i in range(len(corpus))]\n",
    "\n",
    "class TfidfRetriever(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        with open(context.artifacts[\"corpus_path\"], \"r\") as f:\n",
    "            payload = json.load(f)\n",
    "        self.doc_ids = payload[\"doc_ids\"]\n",
    "        self.vectorizer = TfidfVectorizer().fit(payload[\"corpus\"])\n",
    "        self.doc_mat = self.vectorizer.transform(payload[\"corpus\"])\n",
    "        self.top_k = int(context.model_config.get(\"top_k\", 2))\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        q_vec = self.vectorizer.transform(model_input[\"query\"].tolist())\n",
    "        sims = q_vec @ self.doc_mat.T\n",
    "        topk_idx = np.argsort(-sims.toarray(), axis=1)[:, :self.top_k]\n",
    "        results = []\n",
    "        for idxs in topk_idx:\n",
    "            results.append([self.doc_ids[i] for i in idxs])\n",
    "        return pd.Series(results)\n",
    "\n",
    "# アセット保存 → Artifactsとして一緒に記録\n",
    "os.makedirs(\"assets\", exist_ok=True)\n",
    "with open(\"assets/corpus.json\", \"w\") as f:\n",
    "    json.dump({\"corpus\": corpus, \"doc_ids\": doc_ids}, f)\n",
    "\n",
    "with mlflow.start_run(run_name=\"register-retriever-demo\"):\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"retriever_model\",\n",
    "        python_model=TfidfRetriever(),\n",
    "        artifacts={\"corpus_path\": \"assets/corpus.json\"},\n",
    "        model_config={\"top_k\": 2},\n",
    "        input_example=pd.DataFrame({\"query\": [\"What is RAG?\", \"capital of France?\"]})\n",
    "    )\n",
    "\n",
    "print(\"RetrieverをMLflowに登録しました。UIのArtifacts/Modelsから確認できます。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e312a70",
   "metadata": {},
   "source": [
    "## 10. Prompt Registryを使ったプロンプト管理デモ\n",
    "**ねらい**: **プロンプト自体を資産として管理**（バージョン、エイリアス、ロールバック）。  \n",
    "**UIで見るポイント**: `Prompts` タブ（※MLflowバージョン2.7+相当で利用可能）。  \n",
    "**注意**: 環境のMLflowバージョンによりAPIが存在しない場合があります（下のセルは存在チェック付き）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6fab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_text = \"Answer the following question clearly: {{ question }}\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"register-prompt-demo\"):\n",
    "    # バージョン互換性チェック（mlflow.prompts がない場合はスキップ）\n",
    "    ok = hasattr(mlflow, \"prompts\") and hasattr(mlflow.prompts, \"log_prompt\")\n",
    "    if ok:\n",
    "        mlflow.prompts.log_prompt(\n",
    "            \"my_prompt_v1\",\n",
    "            template=prompt_text,\n",
    "            input_variables=[\"question\"]\n",
    "        )\n",
    "        print(\"✅ Prompt Registryに登録しました。UIの Prompts タブで確認できます。\")\n",
    "    else:\n",
    "        print(\"ℹ️ お使いのMLflowでは Prompt Registry API が無効/未提供の可能性があります。\")\n",
    "        print(\"   MLflow 2.7+ 相当のドキュメントをご確認ください。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f514457",
   "metadata": {},
   "source": [
    "## 11. まとめ（このデモで伝えたいこと）\n",
    "- **再現性**：誰が・いつ・どの設定で・何が出たかを**Runに完全記録**  \n",
    "- **比較可能性**：複数Runを**横並び**で比較し、品質/遅延/コストのトレードオフで意思決定  \n",
    "- **可観測性**：Artifactsで**失敗の中身**まで追跡（必要に応じてTracingへ発展）  \n",
    "- **資産化**：Retrieverなど**LLMを支える部品**をModelとして登録／PromptはRegistryで管理  \n",
    "- 小さく始めて、必要に応じて **Model Registry / Prompt Registry / Tracing** に拡張可能"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
