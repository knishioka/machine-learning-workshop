{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最小限のFine-tuning実践ガイド（Google Colab）\n",
    "\n",
    "このノートブックでは、最小限のリソースで効果的なfine-tuningを行う方法を説明します。\n",
    "- **モデル**: TinyLlama 1.1B（軽量で高速）\n",
    "- **手法**: QLoRA（メモリ効率的）\n",
    "- **データ**: 50-100サンプル程度\n",
    "- **実行時間**: 約10-20分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境セットアップ（約2分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unslothのインストール（最も簡単な方法）\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データ準備（1分）\n",
    "最小限のサンプルデータを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 最小限のトレーニングデータ（50サンプル）\n",
    "training_data = [\n",
    "    {\n",
    "        \"instruction\": \"Pythonでリストを逆順にする方法を教えてください\",\n",
    "        \"output\": \"Pythonでリストを逆順にする方法は3つあります：\\n1. `reversed()`: `list(reversed(my_list))`\\n2. スライス: `my_list[::-1]`\\n3. `reverse()`: `my_list.reverse()`\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"gitでコミットを取り消す方法\",\n",
    "        \"output\": \"gitでコミットを取り消すには：\\n- `git reset --soft HEAD~1`: コミットのみ取り消し\\n- `git reset --hard HEAD~1`: 変更ごと取り消し\\n- `git commit --amend`: 直前のコミットを修正\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Dockerコンテナが起動しない\",\n",
    "        \"output\": \"Dockerコンテナが起動しない場合の確認事項：\\n1. `docker logs <container_id>`: ログ確認\\n2. `docker ps -a`: コンテナ状態確認\\n3. Dockerfileのエントリーポイント確認\\n4. ポート競合の確認\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"TypeError: 'NoneType' object is not iterable\",\n",
    "        \"output\": \"このエラーは、Noneに対して繰り返し処理を行おうとした時に発生します。\\n対処法：\\n1. 変数がNoneでないか確認: `if data is not None:`\\n2. デフォルト値を設定: `data = data or []`\\n3. 関数の戻り値を確認\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"APIのレスポンスが遅い\",\n",
    "        \"output\": \"APIレスポンスの改善方法：\\n1. キャッシュの実装（Redis等）\\n2. データベースクエリの最適化\\n3. ページネーションの導入\\n4. 非同期処理の活用\\n5. CDNの利用\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# データを増やす（バリエーションを追加）\n",
    "extended_data = []\n",
    "for item in training_data:\n",
    "    # オリジナル\n",
    "    extended_data.append(item)\n",
    "    \n",
    "    # 少し違う聞き方のバージョン\n",
    "    variation = {\n",
    "        \"instruction\": item[\"instruction\"] + \"。初心者向けに説明してください\",\n",
    "        \"output\": \"初心者の方向けに説明します。\\n\\n\" + item[\"output\"]\n",
    "    }\n",
    "    extended_data.append(variation)\n",
    "\n",
    "# 合計10個のサンプル（デモ用最小限）\n",
    "print(f\"トレーニングデータ数: {len(extended_data)}\")\n",
    "\n",
    "# JSONファイルとして保存\n",
    "with open('minimal_training_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(extended_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. モデルの読み込みとLoRA設定（2分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# 最大シーケンス長（短めに設定してメモリ節約）\n",
    "max_seq_length = 512\n",
    "\n",
    "# 4bit量子化で1.1Bモデルを読み込み（約500MB）\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/tinyllama-bnb-4bit\",  # 最軽量モデル\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# LoRAアダプターを追加（メモリ効率的なfine-tuning）\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,  # ランク（小さくしてメモリ節約）\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "print(\"モデルの準備完了！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. データセットの準備（1分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# プロンプトフォーマット関数\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        text = f\"\"\"### 質問:\n",
    "{instruction}\n",
    "\n",
    "### 回答:\n",
    "{output}\"\"\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# データセット作成\n",
    "dataset = Dataset.from_list(extended_data)\n",
    "print(f\"データセットサイズ: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning実行（10-15分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# トレーナー設定（最小限）\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=5,\n",
    "        max_steps=30,  # 最小限のステップ数\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# GPUメモリ統計\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"使用中のメモリ = {start_gpu_memory} GB.\")\n",
    "\n",
    "# トレーニング開始\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# メモリ使用量\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"トレーニングで使用したメモリ = {used_memory_for_lora} GB ({lora_percentage}% of GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. モデルのテスト（1分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論モードに切り替え\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# テスト質問\n",
    "test_questions = [\n",
    "    \"Pythonでファイルを読み込む方法は？\",\n",
    "    \"エラーが出ました: KeyError: 'user_id'\",\n",
    "    \"gitで変更を取り消したい\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    inputs = tokenizer(\n",
    "        f\"\"\"### 質問:\n",
    "{question}\n",
    "\n",
    "### 回答:\"\"\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens=128, temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n質問: {question}\")\n",
    "    print(f\"回答: {response.split('### 回答:')[1].strip()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. GGUF形式で保存（Ollama用）（2分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GGUF形式で保存（量子化レベル: q4_k_m）\n",
    "print(\"GGUF形式で保存中...\")\n",
    "model.save_pretrained_gguf(\n",
    "    \"minimal_finetuned_model\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\"  # バランスの良い量子化\n",
    ")\n",
    "\n",
    "print(\"\\n✅ 保存完了！\")\n",
    "print(\"ファイル名: minimal_finetuned_model-unsloth.Q4_K_M.gguf\")\n",
    "print(\"\\nこのファイルをダウンロードして、Ollamaで使用できます。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ファイルのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colabからファイルをダウンロード\n",
    "from google.colab import files\n",
    "\n",
    "# GGUFファイルをダウンロード\n",
    "files.download('minimal_finetuned_model-unsloth.Q4_K_M.gguf')\n",
    "\n",
    "print(\"\\n📥 ファイルのダウンロードが開始されました。\")\n",
    "print(\"\\nローカルでの使用方法:\")\n",
    "print(\"1. ダウンロードしたGGUFファイルを適当なディレクトリに配置\")\n",
    "print(\"2. Modelfileを作成:\")\n",
    "print(\"   echo 'FROM ./minimal_finetuned_model-unsloth.Q4_K_M.gguf' > Modelfile\")\n",
    "print(\"3. Ollamaでモデルを作成:\")\n",
    "print(\"   ollama create my-minimal-model -f Modelfile\")\n",
    "print(\"4. 使用:\")\n",
    "print(\"   ollama run my-minimal-model \\\"質問\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、最小限のリソースでfine-tuningを実行しました：\n",
    "\n",
    "- **モデル**: TinyLlama 1.1B（最軽量）\n",
    "- **データ**: 10サンプル（実用では50-100推奨）\n",
    "- **実行時間**: 約15-20分\n",
    "- **GPU使用量**: 約3-4GB\n",
    "- **出力**: Ollama対応のGGUFファイル\n",
    "\n",
    "### 改善のヒント\n",
    "\n",
    "1. **データを増やす**: 50-100サンプルで品質向上\n",
    "2. **ステップ数を増やす**: max_steps=50-100\n",
    "3. **大きめのモデル**: Llama-3.2-3B（余裕があれば）\n",
    "4. **評価の追加**: 検証データセットで品質確認"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}