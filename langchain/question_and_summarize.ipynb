{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c1bdb5-2ec2-4810-839b-acbf73b382f7",
   "metadata": {},
   "source": [
    "## 設定ファイルの読み込み\n",
    "OpanAIのAPIキーを読み込んでいる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c975ebce-9155-4c3a-aee6-242187020413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6b31b8-5847-4de9-b26d-913e13ca9599",
   "metadata": {},
   "source": [
    "## Token数を調べる\n",
    "tiktokenを使ってtoken数を調べてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8c5eaa53-b35e-42bb-89bb-d712234d49ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "efd5a85a-a80c-4905-ba66-5144b5c46761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd807f57-183d-4d1b-ba98-18792a1604c2",
   "metadata": {},
   "source": [
    "ChatGPTでは `cl100k_base` というtokenizerが使われている。\n",
    "\n",
    "実際に `cl100k_base` tokenizerで英文をエンコーディングする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6ab667-658b-46df-b5e9-814e615e7fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[576, 374, 264, 5869, 13]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"this is a pen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3ac6f-4538-457f-9ddd-941e546d5e22",
   "metadata": {},
   "source": [
    "単語及びピリオドが数値にmappingされていることがわかる。\n",
    "例えばthisの数値を調べてみると"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d39c4f8f-e214-4150-867e-111c9ccb5289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[576]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"this\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656575eb-bd7b-48eb-8f7f-efa8f1bc3fbb",
   "metadata": {},
   "source": [
    "となる。このように、それぞれの単語と対応する数値がある。\n",
    "\n",
    "token数はmappingされたベクターの長さを調べれば良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "052d8d5c-aaf8-457c-bcb0-c1599242083c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(\"this is a pen.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c101ac-5a65-4e47-8a9c-9923937c270e",
   "metadata": {},
   "source": [
    "次に、日本語をエンコーディングしてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c5e5e58-6dfb-4acb-bfa8-bfed59c455f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[90115]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"こんにちは\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9492a401-2683-45b2-b440-a34b7408c61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(\"こんにちは\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b14e1ec-e24c-41ef-a625-627a2979a170",
   "metadata": {},
   "source": [
    "「こんにちは」は、token数1となることがわかる。\n",
    "\n",
    "一方で「こんばんは」はどうなるか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dd1ca0c-ab81-48af-8b36-33993021ff4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[69847, 81219, 25827, 15682]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"こんばんは\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5d1083d-f5c9-45cf-89c6-dae3dc690142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(\"こんばんは\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f77031d-532b-4d53-9ef2-babab52c0e6a",
   "metadata": {},
   "source": [
    "「こんばんは」はtoken数が4となることがわかる。\n",
    "\n",
    "このように日本語では、token数は単語によって大きく変わってくる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9962d50-b699-4709-aa4d-532329aa7b6b",
   "metadata": {},
   "source": [
    "## Document Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c76e165-48ac-4a01-9435-ed1f4975efd4",
   "metadata": {},
   "source": [
    "### PDFファイルを読み込むLoader\n",
    "PDFを読むことができるPyPDFLoaderはpypdfをインストールする必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "823ffc63-acb8-4b70-a22c-cb689c1ecfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (3.16.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfb4a113-6125-4d8c-84ee-f4086b8350e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_loader = PyPDFLoader(\"https://arxiv.org/pdf/2210.03629.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11bb4954-bfa4-484d-8ba9-389f064380a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Published as a conference paper at ICLR 2023\\nREAC T: S YNERGIZING REASONING AND ACTING IN\\nLANGUAGE MODELS\\nShunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\\n1Department of Computer Science, Princeton University\\n2Google Research, Brain team\\n1{shunyuy,karthikn}@princeton.edu\\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\\nABSTRACT\\nWhile large language models (LLMs) have demonstrated impressive performance\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\\nplan generation) have primarily been studied as separate topics. In this paper, we\\nexplore the use of LLMs to generate both reasoning traces and task-speciﬁc actions\\nin an interleaved manner, allowing for greater synergy between the two: reasoning\\ntraces help the model induce, track, and update action plans as well as handle\\nexceptions, while actions allow it to interface with and gather additional information\\nfrom external sources such as knowledge bases or environments. We apply our\\napproach, named ReAct , to a diverse set of language and decision making tasks\\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\\nimproved human interpretability and trustworthiness. Concretely, on question\\nanswering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent\\nissues of hallucination and error propagation in chain-of-thought reasoning by\\ninteracting with a simple Wikipedia API, and generating human-like task-solving\\ntrajectories that are more interpretable than baselines without reasoning traces.\\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\\nan absolute success rate of 34% and 10% respectively, while being prompted with\\nonly one or two in-context examples.\\n1 I NTRODUCTION\\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\\nsider the example of cooking up a dish in the kitchen. Between any two speciﬁc actions, we may\\nreason in language in order to track progress (“now that everything is cut, I should heat up the pot of\\nwater”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let\\nme use soy sauce and pepper instead”), and to realize when external information is needed (“how do\\nI prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the\\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What\\ndish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans\\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\\nunseen circumstances or facing information uncertainties.\\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\\n∗Work during Google internship. Projet page with code: https://react-lm.github.io/ .\\n1arXiv:2210.03629v3  [cs.CL]  10 Mar 2023', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 0}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\n$FW\\x03\\x14\\x1d\\x037KLQN>)LUVW\\x03,\\x03QHHG\\x03WR\\x03ILQG\\x03D\\x03SHSSHU\\x03VKDNHU\\x11\\x11\\x11PRUH\\x03OLNHO\\\\\\x03WR\\x03DSSHDU\\x03LQ\\x03FDELQHWV\\x03\\x0b\\x14\\x10\\x19\\x0c\\x0f\\x03FRXQWHUWRSV\\x03\\x0b\\x14\\x10\\x16\\x0c\\x0f\\x03\\x11\\x11\\x11\\x03@$FW\\x03\\x15\\x1d\\x03*R\\x03WR\\x03FDELQHW\\x03\\x14\\x032EV\\x03\\x15\\x1d\\x032Q\\x03WKH\\x03FDELQHW\\x03\\x14\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03YDVH\\x03\\x15\\x11\\x03\\x0b+HUH\\x03WKH\\x03DJHQW\\x03JR\\x03WR\\x03FDELQHW\\x03\\x14\\x0f\\x03WKHQ\\x03FDELQHW\\x03\\x15\\x0f\\x03\\x16\\x0f\\x03WKHQ\\x03FRXQWHUWRS\\x03\\x14\\x03DQG\\x03\\x15\\x0c$FW\\x03\\x19\\x1d\\x03*R\\x03WR\\x03FRXQWHUWRS\\x03\\x16\\x032EV\\x03\\x19\\x1d\\x032Q\\x03WKH\\x03FRXQWHUWRS\\x03\\x16\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03DSSOH\\x03\\x14\\x0f\\x03D\\x03EUHDG\\x03\\x14\\x0f\\x03\\x11\\x11\\x11\\x11\\x11\\x11\\x03D\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x0f\\x03DQG\\x03D\\x03YDVH\\x03\\x16\\x11$FW\\x03\\x1a\\x1d\\x037DNH\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x03IURP\\x03FRXQWHUWRS\\x03\\x16\\x032EV\\x03\\x1a\\x1d\\x03<RX\\x03SLFN\\x03XS\\x03WKH\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x03IURP\\x03WKH\\x03FRXQWHUWRS\\x03\\x16\\x11\\x03$FW\\x03\\x1b\\x1d\\x037KLQN>1RZ\\x03,\\x03ILQG\\x03D\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03SXW\\x03LW\\x03LQ\\x12RQ\\x03GUDZHU\\x03\\x14\\x11@$FW\\x03\\x1c\\x1d\\x03*R\\x03WR\\x03GUDZHU\\x03\\x14\\x032EV\\x03\\x1c\\x1d\\x03\\'UDZHU\\x03\\x14\\x03LV\\x03FORVHG\\x11\\x03$FW\\x03\\x14\\x13\\x1d\\x032SHQ\\x03GUDZHU\\x03\\x14\\x032EV\\x03\\x14\\x13\\x1d\\x03<RX\\x03RSHQ\\x03\\'UDZHU\\x03\\x14\\x03ª$FW\\x03\\x14\\x14\\x1d\\x033XW\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x03LQ\\x12RQ\\x03GUDZHU\\x03\\x14\\x032EV\\x03\\x14\\x14\\x1d\\x03<RX\\x03SXW\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x03LQ\\x12RQ\\x03WKH\\x03GUDZHU\\x03\\x14\\x11\\x0b\\x14D\\x0c\\x036WDQGDUG$QVZHU\\x1d\\x03L3RG\\x0b\\x14E\\x0c\\x03&R7\\x03\\x0b5HDVRQ\\x032QO\\\\\\x0c7KRXJKW\\x1d\\x03/HW\\nV\\x03WKLQN\\x03VWHS\\x03E\\\\\\x03VWHS\\x11\\x03$SSOH\\x035HPRWH\\x03ZDV\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03LQWHUDFW\\x03ZLWK\\x03$SSOH\\x0379\\x11\\x03$SSOH\\x0379\\x03FDQ\\x03EH\\x03FRQWUROOHG\\x03E\\\\\\x03L3KRQH\\x0f\\x03L3DG\\x0f\\x03DQG\\x03L3RG\\x037RXFK\\x11\\x036R\\x03WKH\\x03DQVZHU\\x03LV\\x03L3KRQH\\x0f\\x03L3DG\\x0f\\x03DQG\\x03L3RG\\x037RXFK\\x11$QVZHU\\x1d\\x03L3KRQH\\x0f\\x03L3DG\\x0f\\x03L3RG\\x037RXFK\\x0b\\x14F\\x0c\\x03$FW\\x102QO\\\\\\x03$FW\\x03\\x14\\x1d\\x036HDUFK>$SSOH\\x035HPRWH@\\x032EV\\x03\\x14\\x1d\\x037KH\\x03$SSOH\\x035HPRWH\\x03LV\\x03D\\x03UHPRWH\\x03FRQWURO\\x03ª$FW\\x03\\x15\\x1d\\x036HDUFK>)URQW\\x035RZ@\\x032EV\\x03\\x15\\x1d\\x03&RXOG\\x03QRW\\x03ILQG\\x03>)URQW\\x035RZ@\\x11\\x036LPLODU\\x1d\\x03ª$FW\\x03\\x16\\x1d\\x036HDUFK>)URQW\\x035RZ\\x03\\x0bVRIWZDUH\\x0c@\\x032EV\\x03\\x16\\x1d\\x03)URQW\\x035RZ\\x03LV\\x03D\\x03GLVFRQWLQXHG\\x03PHGLD\\x03FHQWHU\\x03VRIWZDUH\\x03ª$FW\\x03\\x17\\x1d\\x03)LQLVK>\\\\HV@\\x0b\\x14G\\x0c\\x035H$FW\\x03\\x0b5HDVRQ\\x03\\x0e\\x03$FW\\x0c7KRXJKW\\x03\\x14\\x1d\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x03$SSOH\\x035HPRWH\\x03DQG\\x03ILQG\\x03WKH\\x03SURJUDP\\x03LW\\x03ZDV\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03LQWHUDFW\\x03ZLWK\\x11$FW\\x03\\x14\\x1d\\x036HDUFK>$SSOH\\x035HPRWH@2EV\\x03\\x14\\x1d\\x037KH\\x03$SSOH\\x035HPRWH\\x03LV\\x03D\\x03UHPRWH\\x03FRQWURO\\x03LQWURGXFHG\\x03LQ\\x032FWREHU\\x03\\x15\\x13\\x13\\x18\\x03E\\\\\\x03$SSOH\\x03ª\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03FRQWURO\\x03WKH\\x03)URQW\\x035RZ\\x03PHGLD\\x03FHQWHU\\x03SURJUDP\\x03ª7KRXJKW\\x03\\x15\\x1d\\x03$SSOH\\x035HPRWH\\x03ZDV\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03FRQWURO\\x03WKH\\x03)URQW\\x035RZ\\x03PHGLD\\x03FHQWHU\\x03SURJUDP\\x11\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x03)URQW\\x035RZ\\x03QH[W\\x03DQG\\x03ILQG\\x03ZKDW\\x03RWKHU\\x03GHYLFH\\x03FDQ\\x03FRQWURO\\x03LW\\x11$FW\\x03\\x15\\x1d\\x036HDUFK>)URQW\\x035RZ@2EV\\x03\\x15\\x1d\\x03&RXOG\\x03QRW\\x03ILQG\\x03>)URQW\\x035RZ@\\x11\\x036LPLODU\\x1d\\x03>\\n)URQW\\x035RZ\\x036HDW\\x03WR\\x03(DUWK\\n\\x0f\\x03\\n)URQW\\x035RZ\\x030RWRUVSRUWV\\n\\x0f\\n)URQW\\x035RZ\\x03\\x0bVRIWZDUH\\x0c\\n\\x0f\\x03ª@7KRXJKW\\x03\\x16\\x1d\\x03)URQW\\x035RZ\\x03LV\\x03QRW\\x03IRXQG\\x11\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x03)URQW\\x035RZ\\x03\\x0bVRIWZDUH\\x0c\\x03\\x11$FW\\x03\\x16\\x1d\\x036HDUFK>)URQW\\x035RZ\\x03\\x0bVRIWZDUH\\x0c@2EV\\x03\\x16\\x1d\\x03)URQW\\x035RZ\\x03LV\\x03D\\x03GLVFRQWLQXHG\\x03PHGLD\\x03FHQWHU\\x03VRIWZDUH\\x03ª7KRXJKW\\x03\\x17\\x1d\\x03)URQW\\x035RZ\\x03\\x0bVRIWZDUH\\x0c\\x03LV\\x03FRQWUROOHG\\x03E\\\\\\x03DQ\\x03$SSOH\\x035HPRWH\\x03RU\\x03WKH\\x03NH\\\\ERDUG\\x03IXQFWLRQ\\x03NH\\\\V\\x11\\x036R\\x03WKH\\x03DQVZHU\\x03LV\\x03NH\\\\ERDUG\\x03IXQFWLRQ\\x03NH\\\\V\\x11$FW\\x03\\x17\\x1d\\x03)LQLVK>NH\\\\ERDUG\\x03IXQFWLRQ\\x03NH\\\\V@Ҽ4XHVWLRQ\\x1d\\x03$VLGH\\x03IURP\\x03WKH\\x03$SSOH\\x035HPRWH\\x0f\\x03ZKDW\\x03RWKHU\\x03GHYLFH\\x03FDQ\\x03FRQWURO\\x03WKH\\x03SURJUDP\\x03$SSOH\\x035HPRWH\\x03ZDV\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03LQWHUDFW\\x03ZLWK\"\\n\\x0b\\x15D\\x0c\\x03$FW\\x102QO\\\\\\x03$FW\\x03\\x14\\x1d\\x03*R\\x03WR\\x03GUDZHU\\x03\\x14\\x032EV\\x03\\x14\\x1d\\x037KH\\x03GUDZHU\\x03\\x14\\x03LV\\x03FORVHG\\x11\\x03$FW\\x03\\x15\\x1d\\x032SHQ\\x03GUDZHU\\x03\\x14\\x032EV\\x03\\x15\\x1d\\x03<RX\\x03RSHQ\\x03WKH\\x03GUDZHU\\x03\\x14\\x11\\x037KH\\x03GUDZHU\\x03\\x14\\x03LV\\x03RSHQ\\x11\\x03,Q\\x03LW\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03GLVKVSRQJH\\x03\\x15\\x0f\\x03DQG\\x03D\\x03VSRRQ\\x03\\x14\\x11\\x03$FW\\x03\\x16\\x1d\\x03*R\\x03WR\\x03VLQNEDVLQ\\x03\\x14\\x032EV\\x03\\x16\\x1d\\x032Q\\x03WKH\\x03VLQNEDVLQ\\x03\\x14\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03GLVKVSRQJH\\x03\\x16\\x0f\\x03D\\x03VSDWXOD\\x03\\x14\\x0f\\x03DQG\\x03D\\x03VSRRQ\\x03\\x15\\x11\\x03$FW\\x03\\x17\\x1d\\x037DNH\\x03SHSSHUVKDNHU\\x03\\x14\\x03IURP\\x03VLQNEDVLQ\\x03\\x14\\x032EV\\x03\\x17\\x1d\\x031RWKLQJ\\x03KDSSHQV\\x11\\x03$FW\\x03\\x18\\x1d\\x037DNH\\x03SHSSHUVKDNHU\\x03\\x14\\x03IURP\\x03VLQNEDVLQ\\x03\\x14\\x032EV\\x03\\x18\\x1d\\x031RWKLQJ\\x03KDSSHQV\\x11\\x03\\x0b\\x15E\\x0c\\x035H$FW\\x03\\x0b5HDVRQ\\x03\\x0e\\x03$FW\\x0c\\nҼ<RX\\x03DUH\\x03LQ\\x03WKH\\x03PLGGOH\\x03RI\\x03D\\x03URRP\\x11\\x03/RRNLQJ\\x03TXLFNO\\\\\\x03DURXQG\\x03\\\\RX\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03FDELQHW\\x03\\x19\\x0f\\x03D\\x03FDELQHW\\x03\\x14\\x0f\\x03D\\x03FRIIHH\\x03PDFKLQH\\x03\\x14\\x0f\\x03D\\x03FRXQWHUWRS\\x03\\x16\\x0f\\x03\\x03D\\x03VWRYH\\x03EXUQHU\\x03\\x14\\x0f\\x03DQG\\x03D\\x03WRDVWHU\\x03\\x14\\x11\\x03<RXU\\x03WDVN\\x03LV\\x03WR\\x1d\\x033XW\\x03VRPH\\x03SHSSHU\\x03VKDNHU\\x03RQ\\x03D\\x03GUDZHU\\x11\\x0b\\x15\\x0c\\x03$OI:RUOG\\x0b\\x14\\x0c\\x03+RWVSRW\\x034$\\nFigure 1: (1) Comparison of 4 prompting methods, (a) Standard , (b) Chain-of-thought ( CoT,\\nReason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)\\nquestion. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar\\net al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task\\nsolving trajectories generated by the model (Act, Thought) and the environment (Obs).\\nanswers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,\\n2022). However, this “chain-of-thought” reasoning is a static black box, in that the model uses\\nits own internal representations to generate thoughts and is not grounded in the external world,\\nwhich limits its ability to reason reactively or update its knowledge. This can lead to issues like fact\\nhallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,\\nrecent work has explored the use of pre-trained language models for planning and acting in interactive\\nenvironments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with\\na focus on predicting actions via language priors. These approaches usually convert multi-modal\\nobservations into text, use a language model to generate domain-speciﬁc actions or plans, and then\\nuse a controller to choose or execute them. However, they do not employ language models to reason\\nabstractly about high-level goals or maintain a working memory to support acting, barring Huang\\net al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the\\ncurrent state. Beyond such simple embodied tasks to interact with a few blocks, there have not been\\nstudies on how reasoning and acting can be combined in a synergistic manner for general task solving,\\nand if such a combination can bring systematic beneﬁts compared to reasoning or acting alone.\\nIn this work, we present ReAct , a general paradigm to combine reasoning and acting with language\\nmodels for solving diverse language reasoning and decision making tasks (Figure 1). ReAct\\nprompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an\\ninterleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and\\nadjust high-level plans for acting (reason to act), while also interact with the external environments\\n(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).\\n2', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 1}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nWe conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:\\nquestion answering (HotPotQA, Yang et al., 2018), fact veriﬁcation (Fever, Thorne et al., 2018),\\ntext-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao\\net al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact\\nwith, ReAct outperforms vanilla action generation models while being competitive with chain-of-\\nthought reasoning ( CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct\\nandCoT that allows for the use of both internal knowledge and externally obtained information\\nduring reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able\\nto outperform imitation or reinforcement learning methods trained with 103∼105task instances,\\nwith an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate\\nthe importance of sparse, versatile reasoning in decision making by showing consistent advantages\\nover controlled baselines with actions only. Besides general applicability and performance boost,\\nthe combination of reasoning and acting also contributes to model interpretability, trustworthiness,\\nand diagnosability across all domains, as humans can readily distinguish information from model’s\\ninternal knowledge versus external environments, as well as inspect reasoning traces to understand\\nthe decision basis of model actions.\\nTo summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt-\\nbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we\\nperform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a\\nfew-shot learning setup over prior approaches that perform either reasoning or action generation in\\nisolation; (3) we present systematic ablations and analysis to understand the importance of acting in\\nreasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the\\nprompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial ﬁnetuning\\nexperiments showing the potential of ReAct to improve with additional training data. Scaling up\\nReAct to train and operate on more tasks and combining it with complementary paradigms like\\nreinforcement learning could further unlock the potential of large language models.\\n2REAC T: SYNERGIZING REASONING +AC TING\\nConsider a general setup of an agent interacting with an environment for task solving. At time\\nstept, an agent receives an observation ot∈O from the environment and takes an action at∈A\\nfollowing some policy π(at|ct), wherect= (o1,a1,···,ot−1,at−1,ot)is the context to the agent.\\nLearning a policy is challenging when the mapping ct↦→atis highly implicit and requires extensive\\ncomputation. For example, the agent shown in Figure 1(1c) is unable to generate the correct ﬁnal\\naction (Act 4) to ﬁnish the QA task as it requires complex reasoning over the trajectory context\\n(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the\\ncontext that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.\\nThe idea of ReAct is simple: we augment the agent’s action space to ˆA=A∪L , whereLis the\\nspace of language. An action ˆat∈L in the language space, which we will refer to as a thought or a\\nreasoning trace , does not affect the external environment, thus leading to no observation feedback.\\nInstead, a thought ˆataims to compose useful information by reasoning over the current context ct,\\nand update the context ct+1= (ct,ˆat)to support future reasoning or acting. As shown in Figure 1,\\nthere could be various types of useful thoughts, e.g. decomposing task goals and create action plans\\n(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),\\nextracting important parts from observations (1d, Thought2, 4), track progress and transit action plans\\n(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.\\nHowever, as the language space Lis unlimited, learning in this augmented action space is difﬁcult\\nand requires strong language priors. In this paper, we mainly focus on the setup where a frozen\\nlarge language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context\\nexamples to generate both domain-speciﬁc actions and free-form language thoughts for task solving\\n(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and\\nenvironment observations to solve a task instance (see Appendix C). For the tasks where reasoning is\\nof primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the\\ntask-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision\\nmaking tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to\\n1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B.\\n3', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 2}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nappear sparsely in the most relevant positions of a trajectory, so we let the language model decide the\\nasynchronous occurrence of thoughts and actions for itself.\\nSince decision making and reasoning capabilities are integrated into a large language model, ReAct\\nenjoys several unique features: A) Intuitive and easy to design : Designing ReAct prompts is\\nstraightforward as human annotators just type down their thoughts in language on top of their actions\\ntaken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail\\nprompt design for each task in Sections 3 and 4. B) General and ﬂexible : Due to the ﬂexible thought\\nspace and thought-action occurrence format, ReAct works for diverse tasks with distinct action\\nspaces and reasoning needs, including but not limited to QA, fact veriﬁcation, text game, and web\\nnavigation. C) Performant and robust :ReAct shows strong generalization to new task instances\\nwhile learning solely from one to six in-context examples, consistently outperforming baselines with\\nonly reasoning or acting across different domains. We also show in Section 3 additional beneﬁts\\nwhen ﬁnetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.\\nD) Human aligned and controllable :ReAct promises an interpretable sequential decision making\\nand reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,\\nhumans can also control or correct the agent behavior on the go by thought editing, as shown in\\nFigure 5 in Section 4.\\n3 K NOWLEDGE -INTENSIVE REASONING TASKS\\nWe begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact\\nveriﬁcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to\\nretrieve information to support reasoning, while also use reasoning to target what to retrieve next,\\ndemonstrating a synergy of reasoning and acting.\\n3.1 S ETUP\\nDomains We consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-\\nPotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning\\nover two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact veriﬁcation\\nbenchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based\\non if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only\\nsetup for both tasks, where models only receive the question/claim as input without access to support\\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\\nan external environment to support reasoning.\\nAction Space We design a simple Wikipedia web API with three types of actions to support\\ninteractive information retrieval: (1) search [entity ], which returns the ﬁrst 5 sentences from\\nthe corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the\\nWikipedia search engine, (2) lookup [string ], which would return the next sentence in the page\\ncontaining string , simulating Ctrl+F functionality on the browser. (3) finish [answer ], which\\nwould ﬁnish the current task with answer . We note that this action space mostly can only retrieve a\\nsmall part of a passage based on exact passage name, which is signiﬁcantly weaker than state-of-the-\\nart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\\nand force models to retrieve via explicit reasoning in language.\\n3.2 M ETHODS\\nReAct Prompting For HotpotQA and Fever, we randomly select 6 and 3 cases2from the training\\nset and manually compose ReAct -format trajectories to use as few-shot exemplars in the prompts.\\nSimilar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\\nthought), where free-form thoughts are used for various purposes. Speciﬁcally, we use a combination\\nof thoughts that decompose questions (“I need to search x, ﬁnd y, then ﬁnd z”), extract information\\nfrom Wikipedia observations (“x was started in 1844”, “The paragraph does not tell x”), perform\\ncommonsense (“x is not y, so z must instead be...”) or arithmetic reasoning (“1844 < 1989”), guide\\n2We ﬁnd more examples do not improve performance.\\n4', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 3}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nPrompt Methoda HotpotQA Fever\\n(EM) (Acc)\\nStandard 28.7 57.1\\nCoT (Wei et al., 2022) 29.4 56.3\\nCoT-SC (Wang et al., 2022a) 33.4 60.4\\nAct 25.7 58.9\\nReAct 27.4 60.9\\nCoT-SC→ReAct 34.2 64.6\\nReAct→CoT-SC 35.1 62.0\\nSupervised SoTAb67.5 89.5\\nTable 1: PaLM-540B prompting results on\\nHotpotQA and Fever.\\naHotpotQA EM is 27.1, 28.9, 33.8 for Standard ,CoT,\\nCoT-SC in Wang et al. (2022b).\\nb(Zhu et al., 2021; Lewis et al., 2020)\\n0 5 10 15 20\\n#CoT-SC trials2628303234HotpotQA EM\\n0 5 10 15 20\\n#CoT-SC trials47.550.052.555.057.560.062.565.0Fever AccMethod\\nCoT-SC -> ReAct\\nReAct -> CoT-SC\\nCoT-SC\\nReAct\\nCoTFigure 2: PaLM-540B prompting results with respect to\\nnumber of CoT-SC samples used.\\nsearch reformulation (“maybe I can search/look up x instead”), and synthesize the ﬁnal answer (“...so\\nthe answer is x”). See Appendix C for more details.\\nBaselines We systematically ablate ReAct trajectories to build prompts for multiple baselines (with\\nformats as Figure 1(1a-1c)): (a) Standard prompting (Standard ), which removes all thoughts,\\nactions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,\\n2022), which removes actions and observations and serve as a reasoning-only baseline. We also\\nbuild a self-consistency baseline ( CoT-SC ) (Wang et al., 2022a;b) by sampling 21 CoT trajectories\\nwith decoding temperature 0.7 during inference and adopting the majority answer, which is found to\\nconsistently boost performance over CoT. (c)Acting-only prompt (Act), which removes thoughts\\ninReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the\\nInternet to answer questions, though it operates on a different task and action space, and uses imitation\\nand reinforcement learning instead of prompting.\\nCombining Internal and External Knowledge As will be detail in Section 3.3, we observe that\\nthe problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT\\nis more accurate in formulating reasoning structure but can easily suffer from hallucinated facts\\nor thoughts. We therefore propose to incorporate ReAct andCoT-SC , and let the model decide\\nwhen to switch to the other method based on the following heuristics: A) ReAct→CoT-SC : when\\nReAct fails to return an answer within given steps, back off to CoT-SC . We set 7 and 5 steps for\\nHotpotQA and FEVER respectively as we ﬁnd more steps will not improve ReAct performance3.\\nB)CoT-SC→ReAct : when the majority answer among nCoT-SC samples occurs less than n/2\\ntimes (i.e. internal knowledge might not support the task conﬁdently), back off to ReAct .\\nFinetuning Due to the challenge of manually annotating reasoning traces and actions at scale,\\nwe consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories\\nwith correct answers generated by ReAct (also for other baselines) to ﬁnetune smaller language\\nmodels (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on\\ninput questions/claims. More details are in Appendix B.1.\\n3.3 R ESULTS AND OBSERVATIONS\\nReAct outperforms Act consistently Table 1 shows HotpotQA and Fever results using PaLM-\\n540B as the base model with different prompting methods. We note that ReAct is better than Act\\non both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the\\nﬁnal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also conﬁrm the beneﬁt of reasoning\\ntraces for more informed acting.\\n3Of all trajectories with correct ﬁnal answers, those with 7 steps on HotpotQA and 5 steps on FEVER only\\ntake up 0.84% and 1.33% respectively.\\n5', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 4}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nType Deﬁnition ReAct CoT\\nSuccessTrue positive Correct reasoning trace and facts 94% 86%\\nFalse positive Hallucinated reasoning trace or facts 6% 14%\\nFailureReasoning error Wrong reasoning trace (including failing to recover from repetitive steps) 47% 16%\\nSearch result error Search return empty or does not contain useful information 23% -\\nHallucination Hallucinated reasoning trace or facts 0% 56%\\nLabel ambiguity Right prediction but did not match the label precisely 29% 28%\\nTable 2: Types of success and failure modes of ReAct andCoT on HotpotQA, as well as their\\npercentages in randomly selected examples studied by human.\\nReAct vs.CoT On the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly\\nlags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only\\ndiffer by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge\\nis vital. To better understand the behavioral difference between ReAct andCoT on HotpotQA, we\\nrandomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct\\nandCoT respectively (thus 200 examples in total), and manually labeled their success and failure\\nmodes in Table 2. Some key observations are as follows:\\nA)Hallucination is a serious problem for CoT, resulting in much higher false positive rate than\\nReAct (14% vs. 6%) in success mode, and make up its major failure mode (56%). In contrast, the\\nproblem solving trajectory of ReAct is more grounded, fact-driven, and trustworthy, thanks to the\\naccess of an external knowledge base.\\nB)While interleaving reasoning, action and observation steps improves ReAct ’s grounded-\\nness and trustworthiness, such a structural constraint also reduces its ﬂexibility in formulating\\nreasoning steps , leading to more reasoning error rate than CoT. we note that there is one frequent\\nerror pattern speciﬁc to ReAct , in which the model repetitively generates the previous thoughts and\\nactions, and we categorize it as part of “reasoning error” as the model fails to reason about what the\\nproper next action to take and jump out of the loop4.\\nC)ForReAct , successfully retrieving informative knowledge via search is critical. Non-\\ninformative search, which counts for 23% of the error cases, derails the model reasoning and gives\\nit a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between\\nfactuality and ﬂexibility, which motivates our proposed strategies of combining two methods.\\nWe provide examples for each success and failure modes in Appendix E.1. We also ﬁnd some\\nHotpotQA questions may contain outdated answer labels, see Figure 4 for example.\\nReAct +CoT-SC perform best for prompting LLMs Also shown in Table 1, the best prompting\\nmethod on HotpotQA and Fever are ReAct→CoT-SC andCoT-SC→ReAct respectively.\\nFurthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC\\nsamples used. While two ReAct +CoT-SC methods are advantageous at one task each, they both\\nsigniﬁcantly and consistently outperform CoT-SC across different number of samples, reaching\\nCoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of\\nproperly combining model internal knowledge and external knowledge for reasoning tasks.\\nReAct performs best for ﬁne-tuning Figure 3 shows the scaling effect of prompting/ﬁnetuning\\nfour methods ( Standard ,CoT,Act,ReAct ) on HotpotQA. With PaLM-8/62B, prompting ReAct\\nperforms worst among four methods due to the difﬁculty to learn both reasoning and acting from\\nin-context examples. However, when ﬁnetuned with just 3,000 examples, ReAct becomes the best\\nmethod among the four, with PaLM-8B ﬁnetuned ReAct outperforming all PaLM-62B prompting\\nmethods, and PaLM-62B ﬁnetuned ReAct outperforming all 540B prompting methods. In contrast,\\nﬁnetuning Standard orCoT is signiﬁcantly worse than ﬁnetuning ReAct orAct for both PaLM-\\n8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge\\nfacts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a\\nmore generalizable skill for knowledge reasoning. As all prompting methods are still signiﬁcantly\\nfar from domain-speciﬁc state-of-the-art approaches (Table 1), we believe ﬁnetuning with more\\nhuman-written data might be a better way to unleash the power of ReAct .\\n4We suspect that this could be due to the sub-optimal greedy decoding procedure, and future work using\\nbetter decoding (e.g. beam search) might help address this issue.\\n6', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 5}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\n8b 62b 540b\\nsize051015202530HotpotQA EMlearning = prompt\\n8b 62b 540b\\nsizelearning = finetune\\nMethod\\nStandard\\nCoT\\nAct\\nReAct\\nFigure 3: Scaling results for prompting and ﬁnetuning on HotPotQA with ReAct (ours) and baselines.\\n4 D ECISION MAKING TASKS\\nWe also test ReAct on two language-based interactive decision-making tasks, ALFWorld and\\nWebShop, both of which feature complex environments that require agents to act over long horizons\\nwith sparse rewards, warranting the need for reasoning to act and explore effectively.\\nALFWorld ALFWorld (Shridhar et al., 2020b) (Figure 1(2)) is a synthetic text-based game designed\\nto align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of\\ntasks in which an agent needs to achieve a high-level goal (e.g. examine paper under desklamp) by\\nnavigating and interacting with a simulated household via text actions (e.g. go to coffeetable 1, take\\npaper 2, use desklamp 1). A task instance can have more than 50 locations and take an expert policy\\nmore than 50 steps to solve, thus challenging an agent to plan and track subgoals, as well as explore\\nsystematically (e.g. check all desks one by one for desklamp). In particular, one challenge built into\\nALFWorld is the need to determine likely locations for common household items (e.g. desklamps will\\nlikely be on desks, shelfs, or dressers), making this environment a good ﬁt for LLMs to exploit their\\npretrained commonsense knowledge. To prompt ReAct , we randomly annotate three trajectories\\nfrom the training set for each task type, where each trajectory includes sparse thoughts that (1)\\ndecompose the goal, (2) track subgoal completion, (3) determine the next subgoal, and (4) reason via\\ncommonsense where to ﬁnd an object and what to do with it. We show prompts used for ALFWorld\\nin Appendix C.4. Following Shridhar et al. (2020b), we evaluate on 134 unseen evaluation games\\nin a task-speciﬁc setup. For robustness, we construct 6 prompts for each task type through each\\npermutation of 2 annotated trajectories from the 3 we annotate. Act prompts are constructed using\\nthe same trajectories, but without thoughts — since task instances are randomly chosen from the\\ntraining set, it favors neither ReAct norAct and provides a fair and controlled comparison to test the\\nimportance of sparse thoughts. For baselines, we use BUTLER (Shridhar et al., 2020b), an imitation\\nlearning agent trained on 105expert trajectories for each task type5.\\nWebShop CanReAct also interact with noisy real-world language environments for practical\\napplications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping\\nwebsite environment with 1.18M real-world products and 12k human instructions. Unlike ALFWorld,\\nWebshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions,\\nand options crawled from Amazon), and requires an agent to purchase a product based on a user\\ninstruction (e.g. “I am looking for a nightstand with drawers. It should have a nickel ﬁnish, and\\npriced lower than $140”) through web interactions (e.g. search “nightstand drawers”, choose buttons\\nsuch as “color: modern-nickel-white” or “back to search”). This task is evaluated by average score\\n(percentage of desired attributes covered by the chosen product averaged across all episodes) and\\nsuccess rate (percentage of episodes where the chosen product satisﬁes all requirements) on 500 test\\ninstructions. We formulate Act prompts with actions to search, choose product, choose options,\\nand buy, with ReAct prompts additionally reasoning to determine what to explore, when to buy,\\nand what products options are relevant to the instruction. See Table 6 for an example prompt, and\\nTable 10 for model predictions in the Appendix. We compare to an imitation learning (IL) method\\n5Micheli & Fleuret (2021) ﬁnetuned a GPT-2 model on 3553 task instances and achieved a much improved\\nperformance than BUTLER, but it is trained on all task types, thus not included as a baseline.\\n7', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 6}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nMethod Pick Clean Heat Cool Look Pick 2 All\\nAct (best of 6) 88 42 74 67 72 41 45\\nReAct (avg) 65 39 83 76 55 24 57\\nReAct (best of 6) 92 58 96 86 78 41 71\\nReAct-IM (avg) 55 59 60 55 23 24 48\\nReAct-IM (best of 6) 62 68 87 57 39 33 53\\nBUTLER g(best of 8) 33 26 70 76 17 12 22\\nBUTLER (best of 8) 46 39 74 100 22 24 37\\nTable 3: AlfWorld task-speciﬁc success rates (%). BUTLER and\\nBUTLER gresults are from Table 4 of Shridhar et al. (2020b). All\\nmethods use greedy decoding, except that BUTLER uses beam search.Method Score SR\\nAct 62.3 30.1\\nReAct 66.6 40.0\\nIL 59.9 29.1\\nIL+RL 62.4 28.7\\nHuman82.1 59.6Expert\\nTable 4: Score and suc-\\ncess rate (SR) on Web-\\nshop. IL/IL+RL taken\\nfrom Yao et al. (2022).\\ntrained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL)\\nmethod additionally trained with 10,587 training instructions.\\nResults ReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On\\nALFWorld, the best ReAct trial achieves an average success rate of 71%, signiﬁcantly outperforming\\nthe best Act (45%) and BUTLER (37%) trials. In fact, even the worse ReAct trial (48%) beats\\nthe best trial of both methods. Moreover, the advantage of ReAct overAct is consistent across\\nsix controlled trials, with relative performance gain ranging from 33% to 90% and averaging 62%.\\nQualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals\\ninto smaller subgoals, or loses track of the current state of the environment. Example trajectories\\ncomparing ReAct andAct can be found in Appendix D.2.1 and Appendix D.2.2.\\nOn Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With\\nadditional sparse reasoning, ReAct achieves signiﬁcantly better performance, with an absolute 10%\\nimprovement over the previous best success rate. By checking examples, we ﬁnd that ReAct is more\\nlikely to identify instruction-relevant products and options by reasoning to bridge the gap between\\nnoisy observations and actions (e.g. “For ‘space-saving ottoman bench for living room’, the item\\nhas options ‘39x18x18inch’ and ‘blue’ and seems good to buy.”). However, existing methods are\\nstill far from the performance of expert humans (Table 4), who perform signiﬁcantly more product\\nexplorations and query re-formulations that are still challenging for prompting-based methods.\\nOn the value of internal reasoning vs. external feedback To our knowledge, ReAct is the ﬁrst\\ndemonstration of combined reasoning and action using an LLM applied to an interactive environment\\nwithin a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang\\net al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner\\nmonologue”. However, IM’s “inner monologue” is limited to observations of the environment\\nstate and what needs to be completed by the agent for the goal to be satisﬁed. In contrast, the\\nreasoning traces in ReAct for decision making is ﬂexible and sparse, allowing diverse reasoning\\ntypes (see Section 2) to be induced for different tasks.\\nTo demonstrate the differences between ReAct and IM, and to highlight the importance of internal\\nreasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought\\npattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially\\noutperforms IM-style prompting ( ReAct-IM ) (71 vs. 53 overall success rate), with consistent\\nadvantages on ﬁve out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakes\\nin identifying when subgoals were ﬁnished, or what the next subgoal should be, due to a lack of high-\\nlevel goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where\\nan item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning.\\nBoth shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in\\nAppendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example\\ntrajectory in Appendix D.2.3.\\n8', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 7}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\n5 R ELATED WORK\\nLanguage model for reasoning Perhaps the most well-known work of using LLMs for reasoning\\nis Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their\\nown “thinking procedure” for problem solving. Several follow-up works have since been performed,\\nincluding least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shot-\\nCoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,\\n(Madaan & Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and\\nobserved that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other\\nwork has also been extended to more sophisticated reasoning architecture beyond simple prompting.\\nFor example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps\\nof “selection” and “inference”. STaR (Zelikman et al., 2022) bootstraps the reasoning process by\\nﬁnetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswell\\n& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated\\nLM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which ﬁnetunes a LM on\\nintermediate computation steps, also demonstrate improvement on multi-step computation problems.\\nIn contrast to these methods, ReAct performs more than just isolated, ﬁxed reasoning, and integrates\\nmodel actions and their corresponding observations into a coherent stream of inputs for the model to\\nreason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\\nLanguage model for decision making The strong capability of LLMs has enabled them to perform\\ntasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a\\npolicy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,\\n2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to\\ncomplicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct , WebGPT does not\\nexplicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for\\nreinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)\\nand Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl\\net al., 2020) also train LMs to make decision about API calls. Unlike ReAct , they do not explicitly\\nconsider the reasoning procedure either, and also relies on expensive datasets and human feedback\\ncollections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the\\ndecision making process only requires language description of the reasoning procedure.6\\nLLMS have also been increasingly employed in interactive and embodied environments for planning\\nand decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)\\nand Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision\\nmaking. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which\\nis then reranked by an affordance model grounded on the visual environments for ﬁnal prediction.\\nInner Monologue made further improvements by adding the eponymous “inner monologue\", which is\\nimplemented as injected feedback from the environment. To our knowledge, Inner Monologue is the\\nﬁrst work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue\\nthat Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section 4. We\\nalso note that leveraging language as semantically-rich inputs in the process of interactive decision\\nmaking has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti\\net al., 2021; Huang et al., 2022a; Li et al., 2022). It is becoming more evident that with the help of\\nLLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and\\ndecision making. What is more, progress in LLMs has also inspired the development of versatile and\\ngeneralist agents like Reed et al. (2022).\\n6 C ONCLUSION\\nWe have proposed ReAct – a simple yet effective method for synergizing reasoning and acting in\\nlarge language models. Through a diverse set of experiments on multi-hop question-answering, fact\\nchecking, and interactive decision-making tasks, we show that ReAct leads to superior performance\\nwith interpretable decision traces. Despite the simplicity of our method, complex tasks with large\\naction spaces require more demonstrations to learn well, which unfortunately can easily go beyond\\nthe input length limit of in-context learning. We explore the ﬁne-tuning approach on HotpotQA\\n6Human feedback can also be incorporated in a complementary manner but we leave it for future work.\\n9', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 8}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nwith initial promising results, but learning from more high-quality human annotations will be the\\ndesiderata to further improve the performance. Scaling up ReAct with multi-task training and\\ncombining it with complementary paradigms like reinforcement learning could result in stronger\\nagents that further unlock the potential of LLMs for more applications.\\nACKNOWLEDGMENTS\\nWe thank the support and feedback of many people from Google Brain team and Princeton NLP\\nGroup. This work was supported in part by the National Science Foundation under Grant No.\\n2107048. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are\\nthose of the author(s) and do not necessarily reﬂect the views of the National Science Foundation.\\nREPRODUCIBILITY STATEMENT\\nOur main experiments are done on PaLM (Chowdhery et al., 2022), which is not an openly accessible\\nmodel yet. To increase reproducibility, we have included all used prompts in Appendix C, additional\\nexperiments using GPT-3 (Brown et al., 2020) in Appendix A.1, and associated GPT-3 ReAct\\nprompting code at https://anonymous.4open.science/r/ReAct-2268/ .\\nETHICS STATEMENT\\nReAct prompts large language models to generate more human interpretable, diagnosable, and\\ncontrollable task-solving trajectories than previous methods. However, hooking up a large language\\nmodel with an action space to interact with external environments (e.g. the web, physical environ-\\nments) has potential dangers, e.g. looking up inappropriate or private information, or taking harmful\\nactions in an environment. Our experiments minimize such risks by limiting the interactions to\\nspeciﬁc websites (Wikipedia or WebShop) that are free of private information, without any dangerous\\nactions in the action space design (i.e. models cannot really buy products on WebShop the research\\nbenchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing\\nmore extensive experiments in the future.\\nREFERENCES\\nJosh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita\\nChhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim\\nHarley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathew-\\nson, So ˇna Mokrá, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne,\\nDuncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence,\\n2020. URL https://arxiv.org/abs/2012.05672 .\\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally\\nJesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka\\nRao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander\\nToshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and\\nAndy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL\\nhttps://arxiv.org/abs/2204.01691 .\\nBen Alderson-Day and Charles Fernyhough. Inner speech: development, cognitive functions,\\nphenomenology, and neurobiology. Psychological bulletin , 141(5):931, 2015.\\nAlan Baddeley. Working memory. Science , 255(5044):556–559, 1992.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.\\n10', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 9}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\\nAntonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022. URL\\nhttps://arxiv.org/abs/2208.14271 .\\nAntonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large\\nlanguage models for interpretable logical reasoning, 2022. URL https://arxiv.org/abs/\\n2205.09712 .\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics , pp. 3558–3567, Florence, Italy, July 2019. Association for Com-\\nputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/\\nP19-1346 .\\nCharles Fernyhough. Vygotsky, luria, and the social brain. Self and social regulation: Social\\ninteraction and the development of social understanding and executive functions , pp. 56–79, 2010.\\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Mari-\\nbeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham,\\nJonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth\\nDathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green,\\nSoˇna Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel,\\nWilliam Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and\\nGeoffrey Irving. Improving alignment of dialogue agents via targeted human judgements,\\n2022. URL https://storage.googleapis.com/deepmind-media/DeepMind.\\ncom/Authors-Notes/sparrow/sparrow-final.pdf .\\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple\\nlanguage model for task-oriented dialogue. Advances in Neural Information Processing Systems ,\\n33:20179–20191, 2020.\\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\\nplanners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207 ,\\n2022a.\\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\\nplanning with language models. arXiv preprint arXiv:2207.05608 , 2022b.\\nSiddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. Lila: Language-informed\\nlatent actions. In CoRL , pp. 1379–1390, 2021. URL https://proceedings.mlr.press/\\nv164/karamcheti22a.html .\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\\nlanguage models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 , 2022.\\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-\\naugmented language models through few-shot prompting for open-domain question answering.\\narXiv preprint arXiv:2203.05115 , 2022.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented genera-\\ntion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems , 33:\\n9459–9474, 2020.\\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An\\nHuang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba,\\nand Yuke Zhu. Pre-trained language models for interactive decision-making, 2022. URL https:\\n//arxiv.org/abs/2202.01771 .\\n11', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 10}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nAleksandr Romanovich Luria. Ls vygotsky and the problem of localization of functions. Neuropsy-\\nchologia , 3(4):387–392, 1965.\\nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes\\ntwo to tango, 2022. URL https://arxiv.org/abs/2209.07686 .\\nVincent Micheli and François Fleuret. Language models are few-shot butlers. arXiv preprint\\narXiv:2104.07972 , 2021.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\\nGretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:\\nBrowser-assisted question-answering with human feedback, 2021. URL https://arxiv.\\norg/abs/2112.09332 .\\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and\\nAugustus Odena. Show your work: Scratchpads for intermediate computation with language\\nmodels, 2021. URL https://arxiv.org/abs/2112.00114 .\\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\\nEccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,\\nOriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https:\\n//arxiv.org/abs/2205.06175 .\\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,\\nLuke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions\\nfor everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition , pp. 10740–10749, 2020a.\\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew\\nHausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv\\npreprint arXiv:2010.03768 , 2020b.\\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston.\\nLanguage models that seek for knowledge: Modular search & generation for dialogue and prompt\\ncompletion. arXiv preprint arXiv:2203.13224 , 2022a.\\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung,\\nMoya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman\\nGoyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. Blenderbot 3:\\na deployed conversational agent that continually learns to responsibly engage, 2022b. URL\\nhttps://arxiv.org/abs/2208.03188 .\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale\\ndataset for fact extraction and veriﬁcation. arXiv preprint arXiv:1803.05355 , 2018.\\nLev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky , 1:39–285, 1987.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,\\n2022a. URL https://arxiv.org/abs/2203.11171 .\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented\\nensembles in language models. arXiv preprint arXiv:2207.00747 , 2022b.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\\narXiv:2201.11903 , 2022.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,\\nand Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\\nanswering. arXiv preprint arXiv:1809.09600 , 2018.\\n12', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 11}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore:\\nLanguage models for action generation in text-based games. In Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Processing (EMNLP) , pp. 8736–8754, Online, Novem-\\nber 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704.\\nURLhttps://aclanthology.org/2020.emnlp-main.704 .\\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable\\nreal-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206 ,\\n2022.\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with\\nreasoning, 2022. URL https://arxiv.org/abs/2203.14465 .\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\\nOlivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in\\nlarge language models, 2022. URL https://arxiv.org/abs/2205.10625 .\\nYunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information\\nseeking for open-domain question answering. arXiv preprint arXiv:2109.06747 , 2021.\\n13', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 12}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nA A DDITIONAL RESULTS\\nA.1 GPT-3 E XPERIMENTS\\nPaLM-540B GPT-3\\nHotpotQA (exact match) 29.4 30.8\\nALFWorld (success rate %) 70.9 78.4\\nTable 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding).\\nOn HotpotQA, we randomly sample a subset of 500 validation questions. On ALFWorld, we use all\\n134 unseen validation task instances, and use the best prompt set according to PaLM-540B.\\nWe run additional GPT-3 (Brown et al., 2020) experiments to conﬁrm ReAct prompting performance\\nis general across different large language models. As shown in Table 5, GPT-3 (text-davinci-002,\\ngreedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly\\nbecause it is ﬁnetuned with human instruction following. This indicates ReAct prompting is effective\\nacross different large language models on different tasks. The code for these experiments are at\\nhttps://react-lm.github.io/ .\\nA.2 REAC TOBTAINS UP -TO-DATE KNOWLEDGE ON HOTPOT QA\\n\\x0b\\x14D\\x0c\\x036WDQGDUG$QVZHU\\x1d\\x03\\x16\\x0f\\x13\\x13\\x13\\x0b\\x14E\\x0c\\x03&R7\\x03\\x0b5HDVRQ\\x032QO\\\\\\x0c7KRXJKW\\x1d\\x03/HW\\nV\\x03WKLQN\\x03VWHS\\x03E\\\\\\x03VWHS\\x11\\x037KH\\x03KRWHO\\x03WKDW\\x03LV\\x03KRPH\\x03WR\\x03WKH\\x03&LUTXH\\x03GX\\x036ROHLO\\x03VKRZ\\x030\\\\VWHUH\\x03LV\\x037UHDVXUH\\x03,VODQG\\x11\\x037UHDVXUH\\x03,VODQG\\x03KDV\\x03\\x15\\x0f\\x1b\\x1b\\x18\\x03URRPV\\x0f\\x03VR\\x03WKH\\x03DQVZHU\\x03LV\\x03\\x15\\x0f\\x1b\\x1b\\x18\\x11$QVZHU\\x1d\\x03\\x15\\x0f\\x1b\\x1b\\x18\\x0b\\x14F\\x0c\\x03$FW\\x102QO\\\\\\x03$FW\\x03\\x14\\x1d\\x036HDUFK>&LUTXH\\x03GX\\x036ROHLO@\\x032EV\\x03\\x14\\x1d\\x03&LUTXH\\x03GX\\x036ROHLO\\x03LV\\x03D\\x03&DQDGLDQ\\x03HQWHUWDLQPHQW\\x03FRPSDQ\\\\ª$FW\\x03\\x15\\x1d\\x03/RRNXS>0\\\\VWHUH@\\x032EV\\x03\\x15\\x1d\\x031R\\x03PRUH\\x03UHVXOWV$FW\\x03\\x16\\x1d\\x036HDUFK>0\\\\VWHUH\\x03\\x0b&LUTXH\\x03GX\\x036ROHLO\\x03VKRZ\\x0c@\\x032EV\\x03\\x16\\x1d\\x03&RXOG\\x03QRW\\x03ILQGª$FW\\x03\\x17\\x1d\\x036HDUFK>0\\\\VWHUH\\x03\\x0b&LUTXH\\x03GX\\x036ROHLO\\x0c@\\x032EV\\x03\\x17\\x1d\\x030\\\\VWqUH\\x03\\x0bP\\\\\\x10VWHHU\\x0c\\x03\\x03LV\\x03RQH\\x03RI\\x03VL[ª$FW\\x03\\x18\\x1d\\x03/RRNXS>7UHDVXUH\\x03,VODQG\\x03+RWHO\\x03DQG\\x03&DVLQR@\\x03ªª\\x0bHQG\\x03ZLWKRXW\\x03DQVZHU\\x0c\\x0b\\x14G\\x0c\\x035H$FW\\x03\\x0b5HDVRQ\\x03\\x0e\\x03$FW\\x0c7KRXJKW\\x03\\x14\\x1d\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x03&LUTXH\\x03GX\\x036ROHLO\\x03VKRZ\\x030\\\\VWHUH\\x0f\\x03ILQG\\x03WKH\\x03KRWHO\\x03LW\\x03LV\\x03LQ\\x0f\\x03WKHQ\\x03ILQG\\x03WKH\\x03QXPEHU\\x03RI\\x03URRPV\\x03LQ\\x03WKH\\x03KRWHO\\x11$FW\\x03\\x14\\x1d\\x036HDUFK>&LUTXH\\x03GX\\x036ROHLO\\x03VKRZ\\x030\\\\VWHUH@2EV\\x03\\x14\\x1d\\x03&RXOG\\x03QRW\\x03ILQG\\x03&LUTXH\\x03GX\\x036ROHLO\\x03VKRZ\\x030\\\\VWHUH\\x11\\x036LPLODU\\x1d\\x03>\\n&LUTXH\\x03GX\\x036ROHLO\\n\\x03ª7KRXJKW\\x03\\x15\\x1d\\x037R\\x03ILQG\\x03WKH\\x03KRWHO\\x0f\\x03,\\x03FDQ\\x03VHDUFK\\x030\\\\VWHUH\\x03\\x0b&LUTXH\\x03GX\\x036ROHLO\\x0c\\x11$FW\\x03\\x15\\x1d\\x036HDUFK>0\\\\VWHUH\\x03\\x0b&LUTXH\\x03GX\\x036ROHLO\\x0c@2EV\\x03\\x15\\x1d\\x030\\\\VWqUH\\x03\\x0bP\\\\\\x10VWHHU\\x0c\\x03\\x03LV\\x03ª\\x03+HOG\\x03LQ\\x03D\\x03FXVWRP\\x03WKHDWUH\\x03DW\\x03WKH\\x037UHDVXUH\\x03,VODQG\\x03+RWHO\\x03DQG\\x03&DVLQR\\x03ª7KRXJKW\\x03\\x16\\x1d\\x030\\\\VWHUH\\x03LV\\x03KHOG\\x03LQ\\x03D\\x03FXVWRP\\x03WKHDWUH\\x03DW\\x03WKH\\x037UHDVXUH\\x03,VODQG\\x03+RWHO\\x03DQG\\x03&DVLQR\\x11\\x036R\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x037UHDVXUH\\x03,VODQG\\x03+RWHO\\x03DQG\\x03&DVLQR\\x03QH[W\\x03DQG\\x03ILQG\\x03WKH\\x03QXPEHU\\x03RI\\x03URRPV\\x03LQ\\x03WKH\\x03KRWHO\\x11$FW\\x03\\x16\\x1d\\x036HDUFK>7UHDVXUH\\x03,VODQG\\x03+RWHO\\x03DQG\\x03&DVLQR@2EV\\x03\\x16\\x1d\\x037UHDVXUH\\x03,VODQG\\x03+RWHO\\x03DQG\\x03&DVLQR\\x03ª\\x03ZLWK\\x03\\x15\\x0f\\x1b\\x1b\\x17\\x03URRPV\\x03DQG\\x03\\x15\\x15\\x13\\x03VXLWHV\\x03ª7KRXJKW\\x03\\x17\\x1d\\x037UHDVXUH\\x03,VODQG\\x03+RWHO\\x03DQG\\x03&DVLQR\\x03KDV\\x03\\x15\\x0f\\x1b\\x1b\\x17\\x03URRPV\\x03DQG\\x03\\x15\\x15\\x13\\x03VXLWHV\\x11\\x036R\\x03WKH\\x03DQVZHU\\x03LV\\x03\\x16\\x0f\\x14\\x13\\x17\\x11$FW\\x03\\x17\\x1d\\x03)LQLVK>\\x16\\x0f\\x14\\x13\\x17@Ҽ4XHVWLRQ\\x1d\\x03+RZ\\x03PDQ\\\\\\x03URRPV\\x03DUH\\x03LQ\\x03WKH\\x03KRWHO\\x03WKDW\\x03LV\\x03KRPH\\x03WR\\x03WKH\\x03&LUTXH\\x03GX\\x036ROHLO\\x03VKRZ\\x030\\\\VWHUH\"+RWSRW4$\\x03ODEHO\\x1d\\x03\\x15\\x0f\\x19\\x19\\x17\\x0b\\x14\\x0c\\x03+RWVSRW\\x034$2XWGDWHG\\x03\\n8S\\x10WR\\x10GDWH\\x03\\nFigure 4: Another example HotpotQA question, where the original label is outdated. Only ReAct is\\nable to obtain the up-to-date answer thanks to real-world web interaction plus reasoning.\\nDuring trajectory inspection, we also ﬁnd that sometimes ReAct does not agree with dataset labels as\\nthe labels themselves could be outdated. For example, as shown in Figure 4, the question asks about\\nthe size of a hotel, which increased from the HotpotQA construction time. While Standard andCoT\\ngive wrong answers due to hallucination, Act fails despite the access of real-world web interaction,\\ndue to a lack of reasoning to guide how to interact with the Internet for QA. Only ReAct is able to\\nretrieve up-to-date information from the Internet and provide a reasonable answer. Therefore, better\\nincorporation of reasoning abilities might beneﬁt recent Internet-augmented language models (Nakano\\net al., 2021; Lazaridou et al., 2022; Shuster et al., 2022a) for up-to-date task solving.\\nA.3 H UMAN -IN-THE-LOOP BEHAVIOR CORRECTION ON ALFWORLD\\nWe also explore human-in-the-loop interaction with ReAct , to allow a human to inspect and edit\\nReAct ’s reasoning traces. Figure 5 shows that by simply removing a hallucinating sentence in Act\\n17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align\\nwith these human thought edits and succeed in the task. From a human perspective, solving such a\\ntask becomes signiﬁcantly easier, from typing tens of actions to only editing a couple of thoughts,\\nwhich enables new forms of human-machine collaboration. We note that such a policy edit on-the-go\\n14', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 13}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\n\\x0bD\\x0c\\x035H$FW\\x03\\x11\\x11\\x11\\x11\\x11\\x112EV\\x03\\x14\\x19\\x1d\\x03<RX\\x03RSHQ\\x03WKH\\x03GUDZHU\\x03\\x17\\x11\\x037KH\\x03GUDZHU\\x03\\x17\\x03LV\\x03RSHQ\\x11\\x03,Q\\x03LW\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03NH\\\\FKDLQ\\x03\\x16\\x0f\\x03DQG\\x03D\\x03ZDWFK\\x03\\x15\\x11\\x03$FW\\x03\\x14\\x1a\\x1d\\x037KLQN\\x03>1RZ\\x03,\\x03ILQG\\x03WKH\\x03ILUVW\\x03NH\\\\FKDLQ\\x03\\x0b\\x16\\x0c\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03WDNH\\x03LW\\x11\\x03,\\x03FDQ\\x03ILQG\\x03WKH\\x03VHFRQG\\x03NH\\\\FKDLQ\\x03\\x0b\\x15\\x0c\\x03ODWHU\\x03LQ\\x03GUDZHU\\x03\\x17\\x11\\x03@$FW\\x03\\x14\\x1b\\x1d\\x03WDNH\\x03NH\\\\FKDLQ\\x03\\x16\\x03IURP\\x03GUDZHU\\x03\\x17\\x032EV\\x03\\x14\\x1b\\x1d\\x03<RX\\x03SLFN\\x03XS\\x03WKH\\x03NH\\\\FKDLQ\\x03\\x16\\x03IURP\\x03WKH\\x03GUDZHU\\x03\\x17\\x11\\x03\\x11\\x11\\x11\\x11\\x11\\x11$FW\\x03\\x15\\x15\\x1d\\x03SXW\\x03NH\\\\FKDLQ\\x03\\x16\\x03LQ\\x12RQ\\x03VDIH\\x03\\x14\\x032EV\\x03\\x15\\x15\\x1d\\x03<RX\\x03SXW\\x03WKH\\x03NH\\\\FKDLQ\\x03\\x16\\x03LQ\\x12RQ\\x03WKH\\x03VDIH\\x03\\x14\\x11\\x03$FW\\x03\\x15\\x16\\x1d\\x037KLQN\\x03>1RZ\\x03,\\x03SXW\\x03WKH\\x03ILUVW\\x03NH\\\\FKDLQ\\x03LQ\\x03VDIH\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03ILQG\\x03WKH\\x03VHFRQG\\x03NH\\\\FKDLQ\\x11\\x03,\\x03FDQ\\x03GLUHFWO\\\\\\x03JR\\x03WR\\x03GUDZHU\\x03\\x17\\x11\\x03@$FW\\x03\\x15\\x17\\x1d\\x03JR\\x03WR\\x03GUDZHU\\x03\\x17\\x032EV\\x03\\x15\\x17\\x1d\\x037KH\\x03GUDZHU\\x03\\x17\\x03LV\\x03RSHQ\\x11\\x03,Q\\x03LW\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03ZDWFK\\x03\\x15\\x11\\x03$FW\\x03\\x15\\x18\\x1d\\x037KLQN\\x03>1RZ\\x03,\\x03ILQG\\x03WKH\\x03VHFRQG\\x03NH\\\\FKDLQ\\x03\\x0b\\x15\\x0c\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03WDNH\\x03LW\\x11@$FW\\x03\\x15\\x19\\x1d\\x03WDNH\\x03NH\\\\FKDLQ\\x03\\x15\\x03IURP\\x03GUDZHU\\x03\\x17\\x032EV\\x03\\x15\\x19\\x1d\\x031RWKLQJ\\x03KDSSHQV\\x11\\x0bE\\x0c\\x035H$FW\\x03\\x0b+XPDQ\\x037KRXJKW\\x03HGLW\\x0c\\x11\\x11\\x11\\x11\\x11\\x112EV\\x03\\x14\\x19\\x1d\\x03<RX\\x03RSHQ\\x03WKH\\x03GUDZHU\\x03\\x17\\x11\\x037KH\\x03GUDZHU\\x03\\x17\\x03LV\\x03RSHQ\\x11\\x03,Q\\x03LW\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03NH\\\\FKDLQ\\x03\\x16\\x0f\\x03DQG\\x03D\\x03ZDWFK\\x03\\x15\\x11\\x03$FW\\x03\\x14\\x1a\\x1dѕ7KLQNѕ>1RZ\\x03,\\x03ILQG\\x03WKH\\x03ILUVW\\x03NH\\\\FKDLQ\\x03\\x0b\\x16\\x0c\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03WDNH\\x03LW\\x11\\x03,\\x03FDQ\\x03ILQG\\x03WKH\\x03VHFRQG\\x03NH\\\\FKDLQ\\x03\\x0b\\x15\\x0c\\x03ODWHU\\x03LQ\\x03GUDZHU\\x03\\x17\\x11@$FW\\x03\\x14\\x1b\\x1d\\x03WDNH\\x03NH\\\\FKDLQ\\x03\\x16\\x03IURP\\x03GUDZHU\\x03\\x17\\x032EV\\x03\\x14\\x1b\\x1d\\x03<RX\\x03SLFN\\x03XS\\x03WKH\\x03NH\\\\FKDLQ\\x03\\x16\\x03IURP\\x03WKH\\x03GUDZHU\\x03\\x17\\x11\\x03\\x11\\x11\\x11\\x11\\x11\\x11$FW\\x03\\x15\\x15\\x1d\\x03SXW\\x03NH\\\\FKDLQ\\x03\\x16\\x03LQ\\x12RQ\\x03VDIH\\x03\\x14\\x032EV\\x03\\x15\\x15\\x1d\\x03<RX\\x03SXW\\x03WKH\\x03NH\\\\FKDLQ\\x03\\x16\\x03LQ\\x12RQ\\x03WKH\\x03VDIH\\x03\\x14\\x11\\x03$FW\\x03\\x15\\x16\\x1d\\x037KLQNѕ>1RZ\\x03,\\x03SXW\\x03WKH\\x03ILUVW\\x03NH\\\\FKDLQ\\x03LQ\\x03VDIH\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03ILQG\\x03WKH\\x03VHFRQG\\x03NH\\\\FKDLQ\\x0f\\x03ZKLFK\\x03LV\\x03PRUH\\x03OLNHO\\\\\\x03WR\\x03DSSHDU\\x03LQ\\x03GUHVVHU\\x03\\x0b\\x14\\x0c\\x0f\\x03JDUEDJHFDQ\\x03\\x0b\\x14\\x0c\\x0f\\x03VDIH\\x03\\x0b\\x14\\x0c\\x0f\\x03VLGHWDEOH\\x03\\x0b\\x14\\x0c\\x0f\\x03VRID\\x03\\x0b\\x14\\x0c\\x0f\\x03VKHOI\\x03\\x0b\\x14\\x10\\x14\\x15\\x0c\\x11@$FW\\x03\\x15\\x17\\x1d\\x03JR\\x03WR\\x03GUHVVHU\\x03\\x14\\x032EV\\x03\\x15\\x17\\x1d\\x032Q\\x03WKH\\x03GUHVVHU\\x03\\x14\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03ER[\\x03\\x14\\x0f\\x03D\\x03FUHGLWFDUG\\x03\\x14\\x0f\\x03D\\x03NH\\\\FKDLQ\\x03\\x15\\x0f\\x03D\\x03NH\\\\FKDLQ\\x03\\x14\\x0f\\x03D\\x03ODSWRS\\x03\\x14\\x11\\x11\\x11$FW\\x03\\x15\\x18\\x1d\\x037KLQNѕ>1RZ\\x03,\\x03ILQG\\x03WKH\\x03VHFRQG\\x03NH\\\\FKDLQ\\x03\\x0b\\x15\\x0c\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03WDNH\\x03LW\\x11@$FW\\x03\\x15\\x19\\x1d\\x03WDNH\\x03NH\\\\FKDLQ\\x03\\x15\\x03IURP\\x03GUHVVHU\\x03\\x14\\x032EV\\x03\\x15\\x19\\x1d\\x03<RX\\x03SLFN\\x03XS\\x03WKH\\x03NH\\\\FKDLQ\\x03\\x15\\x03IURP\\x03WKH\\x03GUHVVHU\\x03\\x14\\x11\\x03$FW\\x03\\x15\\x1a\\x1d\\x037KLQNѕ>1RZ\\x03,\\x03WDNH\\x03WKH\\x03VHFRQG\\x03NH\\\\FKDLQ\\x03\\x0b\\x15\\x0c\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03SXW\\x03LW\\x03LQ\\x12RQ\\x03VDIH\\x03\\x14\\x11@$FW\\x03\\x15\\x1b\\x1d\\x03JR\\x03WR\\x03VDIH\\x03\\x142EV\\x03\\x15\\x1b\\x1d\\x037KH\\x03VDIH\\x03\\x14\\x03LV\\x03RSHQ\\x11\\x03,Q\\x03LW\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03NH\\\\FKDLQ\\x03\\x16\\x0f\\x03D\\x03VWDWXH\\x03\\x15\\x0f\\x03DQG\\x03D\\x03ZDWFK\\x03\\x16\\x11$FW\\x03\\x15\\x1c\\x1d\\x03SXW\\x03NH\\\\FKDLQ\\x03\\x15\\x03LQ\\x12RQ\\x03VDIH\\x03\\x142EV\\x03\\x15\\x1c\\x1d\\x03<RX\\x03SXW\\x03WKH\\x03NH\\\\FKDLQ\\x03\\x15\\x03LQ\\x12RQ\\x03WKH\\x03VDIH\\x03\\x14\\x11Ҽ<RX\\x03DUH\\x03LQ\\x03WKH\\x03PLGGOH\\x03RI\\x03D\\x03URRP\\x11\\x03/RRNLQJ\\x03TXLFNO\\\\\\x03DURXQG\\x03\\\\RX\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03DUPFKDLU\\x03\\x14\\x0f\\x03D\\x03FDELQHW\\x03\\x17\\x0f\\x03ª\\x03D\\x03FDELQHW\\x03\\x14\\x0f\\x03D\\x03GUDZHU\\x03\\x18\\x0f\\x03ª\\x0f\\x03D\\x03GUDZHU\\x03\\x14\\x0f\\x03D\\x03GUHVVHU\\x03\\x14\\x0f\\x03D\\x03JDUEDJHFDQ\\x03\\x14\\x0f\\x03D\\x03VDIH\\x03\\x14\\x0f\\x03D\\x03VKHOI\\x03\\x14\\x15\\x0f\\x03ª\\x0f\\x03D\\x03VKHOI\\x03\\x14\\x0f\\x03D\\x03VLGHWDEOH\\x03\\x14\\x0f\\x03DQG\\x03D\\x03VRID\\x03\\x14\\x11\\x03<RXU\\x03WDVN\\x03LV\\x03WR\\x1d\\x03SXW\\x03WZR\\x03NH\\\\FKDLQ\\x03LQ\\x03VDIH\\x11$OI:RUOG\\nFigure 5: A human-in-the-loop behavior correction example with ReAct in AlfWorld. (a) ReAct\\ntrajectory fails due to a hallucinating thought (Act 17). (b) By a human simply editing two thoughts\\n(Act 17, 23), the ReAct trajectory produces desirable reasoning traces and actions and succeeds.\\nis difﬁcult for Act and previous RL methods, as a human cannot change the model parameters, and\\nchanging a few actions might not edit the rest of the model behavior. This paradigm is also more than\\nhuman dialogue to update the goal or subgoal as in Huang et al. (2022b) — while editing ReAct\\nthoughts can do these, it can also modify the model’s internal belief, reasoning styles, or anything the\\nﬂexible thought space supports, for better task solving. We believe this is an exciting direction for\\nhuman alignment and leave more systematic study as future work.\\nB E XPERIMENT DETAILS\\nB.1 H OTPOT QA F INETUNING DETAILS\\nFor all ﬁnetuning we use a batch size of 64. On PaLM-8B, we ﬁnetune ReAct andAct methods\\nfor4,000steps and Standard andCoT methods for 2,000steps. On PaLM-62B, we ﬁnetune\\nReAct andAct methods for 4,000steps and Standard andCoT methods for 1,000steps. We\\nﬁndReAct andAct methods generally beneﬁt from more training steps (and more training data),\\nwhile Standard andCoT methods degrade soon after ﬁnetuning.\\nB.2 A LFWORLD IM-S TYLE DETAILS\\nFor the IM-style ablation, the same expert trajectories used in ReAct are reannotated with dense\\nexternal feedback thoughts within these trajectories, that limit ReAct-IM to only think about (1)\\ndecomposing the current goal and (2) the current subgoal that needs to be completed. In particular,\\nReAct-IM lacks thoughts that (1) determine when a subgoal is completed (2) determine what the\\nnext subgoal should be (3) inducing the LLM to refer to its internal pretraining knowledge to identify\\nwhere items can be within the environment.\\n15', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 14}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nC P ROMPTS\\nC.1 H OTPOT QA\\nHotpot QA Prompts\\nOrig inal\\nQuestion What is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nAnswer 1,800 to 7,000 ft\\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nAnswer Richard Nixon\\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nAnswer The Saimaa Gesture\\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\\nAnswer director, screenwriter, actor\\nQuestion Which magazine was started first Arthur’s Magazine or First for Women?\\nAnswer Arthur’s Magazine\\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\\nAnswer Yes\\nAct\\nQuestion What is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nAction 1 Search[Colorado orogeny]\\nObservation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in\\nColorado and surrounding areas.\\nAction 2 Lookup[eastern sector]\\nObservation 2 (Result 1 / 1) The eastern sector extends into the High Plains and is called\\nthe Central Plains orogeny.\\nAction 3 Search[High Plains]\\nObservation 3 High Plains refers to one of two distinct land regions:\\nAction 4 Search[High Plains (United States)]\\nObservation 4 The High Plains are a subregion of the Great Plains. From east to west, the\\nHigh Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\\nm).[3]\\nAction 5 Finish[1,800 to 7,000 ft]\\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nAction 1 Search[Milhouse]\\nObservation 1 Milhouse Mussolini Van Houten is a recurring character in the Fox animated\\ntelevision series The Simpsons voiced by Pamela Hayden and created by Matt\\nGroening.\\nAction 2 Lookup[named after]\\nObservation 2 (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose\\nmiddle name was Milhous.\\nAction 3 Finish[Richard Nixon]\\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nAction 1 Search[Adam Clayton Powell]\\nObservation 1 Could not find [Adam Clayton Powell]. Similar: [’Adam Clayton Powell\\nIII’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr. State Office\\nBuilding’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell\\n(film)’, ’Giancarlo Esposito’].\\nAction 2 Search[Adam Clayton Powell (film)]\\nObservation 2 Adam Clayton Powell is a 1989 American documentary film directed by\\nRichard Kilberg. The film is about the rise and fall of influential\\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired\\nas part of the PBS series The American Experience.\\nAction 3 Finish[The Saimaa Gesture]\\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\\nAction 1 Search[Nicholas Ray]\\nObservation 1 Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,\\n1979) was an American film director, screenwriter, and actor best known for\\nthe 1955 film Rebel Without a Cause.\\nAction 2 Search[Elia Kazan]\\nObservation 2 Elia Kazan was an American film and theatre director, producer, screenwriter\\nand actor.\\nContinued on next page\\n16', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 15}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nHotpot QA Prompts – Continued from previous page\\nAction 3 Finish[director, screenwriter, actor]\\nQuestion Which magazine was started first Arthur’s Magazine or First for Women?\\nAction 1 Search[Arthur’s Magazine]\\nObservation 1 Arthur’s Magazine (1844-1846) was an American literary periodical published\\nin Philadelphia in the 19th century.\\nAction 2 Search[First for Women]\\nObservation 2 First for Women is a woman’s magazine published by Bauer Media Group in the\\nUSA.[1] The magazine was started in 1989.\\nAction 3 Finish[Arthur’s Magazine]\\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\\nAction 1 Search[Pavel Urysohn]\\nObservation 1 Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet\\nmathematician who is best known for his contributions in dimension theory.\\nAction 2 Search[Leonid Levin]\\nObservation 2 Leonid Anatolievich Levin is a Soviet-American mathematician and computer\\nscientist.\\nAction 3 Finish[yes]\\nCoT\\nQuestion What is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nThought Let’s think step by step. The eastern sector of Colorado orogeny extends\\ninto the High Plains. High Plains rise in elevation from around 1,800 to\\n7,000 ft, so the answer is 1,800 to 7,000 ft.\\nAnswer 1,800 to 7,000 ft\\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nThought Let’s think step by step. Milhouse was named after U.S. president Richard\\nNixon, so the answer is Richard Nixon.\\nAnswer Richard Nixon\\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nThought Let’s think step by step. Adam Clayton Powell (film) is a documentary about\\nan African-American politician, not Finnish rock groups. So the documentary\\nabout Finnish rock groups must instead be The Saimaa Gesture.\\nAnswer The Saimaa Gesture\\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\\nThought Let’s think step by step. Professions of Nicholas Ray are director,\\nscreenwriter, and actor. Professions of Elia Kazan are director, producer,\\nscreenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in\\ncommon is director, screenwriter, and actor.\\nAnswer director, screenwriter, actor\\nQuestion Which magazine was started first Arthur’s Magazine or First for Women?\\nThought Let’s think step by step. Arthur’s Magazine was started in 1844. First\\nfor Women was started in 1989. 1844 (Arthur’s Magazine) < 1989 (First for\\nWomen), so Arthur’s Magazine was started first.\\nAnswer Arthur’s Magazine\\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\\nThought Let’s think step by step. Pavel Urysohn is a mathematician. Leonid Levin\\nis a mathematician and computer scientist. So Pavel Urysohn and Leonid\\nLevin have the same type of work.\\nAnswer Yes\\nReAct\\nQuestion What is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nThought 1 I need to search Colorado orogeny, find the area that the eastern sector\\nof the Colorado orogeny extends into, then find the elevation range of the\\narea.\\nAction 1 Search[Colorado orogeny]\\nObservation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in\\nColorado and surrounding areas.\\nThought 2 It does not mention the eastern sector. So I need to look up eastern\\nsector.\\nAction 2 Lookup[eastern sector]\\nObservation 2 (Result 1 / 1) The eastern sector extends into the High Plains and is called\\nthe Central Plains orogeny.\\nThought 3 The eastern sector of Colorado orogeny extends into the High Plains. So I\\nneed to search High Plains and find its elevation range.\\nAction 3 Search[High Plains]\\nObservation 3 High Plains refers to one of two distinct land regions\\nThought 4 I need to instead search High Plains (United States).\\nAction 4 Search[High Plains (United States)]\\nContinued on next page\\n17', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 16}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nHotpot QA Prompts – Continued from previous page\\nObservation 4 The High Plains are a subregion of the Great Plains. From east to west, the\\nHigh Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\\nm).[3]\\nThought 5 High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer\\nis 1,800 to 7,000 ft.\\nAction 5 Finish[1,800 to 7,000 ft]\\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nThought 1 The question simplifies to \"The Simpsons\" character Milhouse is named after\\nwho. I only need to search Milhouse and find who it is named after.\\nAction 1 Search[Milhouse]\\nObservation 1 Milhouse Mussolini Van Houten is a recurring character in the Fox animated\\ntelevision series The Simpsons voiced by Pamela Hayden and created by Matt\\nGroening.\\nThought 2 The paragraph does not tell who Milhouse is named after, maybe I can look up\\n\"named after\".\\nAction 2 Lookup[named after]\\nObservation 2 (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose\\nmiddle name was Milhous.\\nThought 3 Milhouse was named after U.S. president Richard Nixon, so the answer is\\nRichard Nixon.\\nAction 3 Finish[Richard Nixon]\\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nThought 1 I need to search Adam Clayton Powell and The Saimaa Gesture, and find which\\ndocumentary is about Finnish rock groups.\\nAction 1 Search[Adam Clayton Powell]\\nObservation 1 Could not find [Adam Clayton Powell]. Similar: [’Adam Clayton Powell\\nIII’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr. State Office\\nBuilding’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell\\n(film)’, ’Giancarlo Esposito’].\\nThought 2 To find the documentary, I can search Adam Clayton Powell (film).\\nAction 2 Search[Adam Clayton Powell (film)]\\nObservation 2 Adam Clayton Powell is a 1989 American documentary film directed by\\nRichard Kilberg. The film is about the rise and fall of influential\\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired\\nas part of the PBS series The American Experience.\\nThought 3 Adam Clayton Powell (film) is a documentary about an African-American\\npolitician, not Finnish rock groups. So the documentary about Finnish rock\\ngroups must instead be The Saimaa Gesture.\\nAction 3 Finish[The Saimaa Gesture]\\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\\nThought 1 I need to search Nicholas Ray and Elia Kazan, find their professions, then\\nfind the profession they have in common.\\nAction 1 Search[Nicholas Ray]\\nObservation 1 Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,\\n1979) was an American film director, screenwriter, and actor best known for\\nthe 1955 film Rebel Without a Cause.\\nThought 2 Professions of Nicholas Ray are director, screenwriter, and actor. I need\\nto search Elia Kazan next and find his professions.\\nAction 2 Search[Elia Kazan]\\nObservation 2 Elia Kazan was an American film and theatre director, producer, screenwriter\\nand actor.\\nThought 3 Professions of Elia Kazan are director, producer, screenwriter, and actor.\\nSo profession Nicholas Ray and Elia Kazan have in common is director,\\nscreenwriter, and actor.\\nAction 3 Finish[director, screenwriter, actor]\\nQuestion Which magazine was started first Arthur’s Magazine or First for Women?\\nThought 1 I need to search Arthur’s Magazine and First for Women, and find which was\\nstarted first.\\nAction 1 Search[Arthur’s Magazine]\\nObservation 1 Arthur’s Magazine (1844-1846) was an American literary periodical published\\nin Philadelphia in the 19th century.\\nThought 2 Arthur’s Magazine was started in 1844. I need to search First for Women\\nnext.\\nAction 2 Search[First for Women]\\nObservation 2 First for Women is a woman’s magazine published by Bauer Media Group in the\\nUSA.[1] The magazine was started in 1989.\\nThought 3 First for Women was started in 1989. 1844 (Arthur’s Magazine) < 1989 (First\\nfor Women), so Arthur’s Magazine was started first.\\nAction 3 Finish[Arthur’s Magazine]\\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\\nThought 1 I need to search Pavel Urysohn and Leonid Levin, find their types of work,\\nthen find if they are the same.\\nAction 1 Search[Pavel Urysohn]\\nContinued on next page\\n18', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 17}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nHotpot QA Prompts – Continued from previous page\\nObservation 1 Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet\\nmathematician who is best known for his contributions in dimension theory.\\nThought 2 Pavel Urysohn is a mathematician. I need to search Leonid Levin next and\\nfind its type of work.\\nAction 2 Search[Leonid Levin]\\nObservation 2 Leonid Anatolievich Levin is a Soviet-American mathematician and computer\\nscientist.\\nThought 3 Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn\\nand Leonid Levin have the same type of work.\\nAction 3 Finish[yes]\\n19', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 18}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nC.2 F EVER\\nFEVER Prompts\\nOrig inal Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nAnswer SUPPORTS\\nClaim Stranger Things is set in Bloomington, Indiana.\\nAnswer REFUTES\\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\\nAnswer NOT ENOUGH INFO\\nAct Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nAction2 Search[Nikolaj Coster-Waldau]\\nObservation 1 Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and\\nproducer. He graduated from the Danish National School of Performing Arts\\nin Copenhagen in 1993,[1] and had his breakthrough role in Denmark with\\nthe film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy\\ndrama series Game of Thrones, for which he received two Primetime Emmy\\nAward nominations for Outstanding Supporting Actor in a Drama Series..\\nCoster-Waldau has appeared in numerous films in his native Denmark and\\nScandinavia, including Headhunters (2011) and A Thousand Times Good Night\\n(2013). In the U.S, his debut film role was in the war film Black Hawk Down\\n(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a\\ndetective in the short-lived Fox television series New Amsterdam (2008), and\\nappeared in the 2009 Fox television film Virtuality, originally intended as\\na pilot.\\nAction 2 Finish[SUPPORTS]\\nClaim Stranger Things is set in Bloomington, Indiana.\\nAction 1 Search[Stranger Things]\\nObservation 1 Stranger Things is an American science fiction horror drama television\\nseries created by the Duffer Brothers. Set in the 1980s, primarily in\\nthe fictional town of Hawkins, Indiana, the series centers on a number of\\nmysteries and supernatural events occurring around the town and their impact\\non an ensemble of child and adult characters.\\nAction 2 Finish[REFUTES]\\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\\nAction 1 Search[Beautiful]\\nObservation 1 Could not find [Beautiful]. Similar: [’Beautiful’, ’Beautiful, Beautiful’,\\n’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is\\nBeautiful’].\\nAction 2 Search[Beautiful (Christina Aguilera song)\\nObservation 2 \"Beautiful\" is a song recorded by American singer Christina Aguilera for her\\nfourth studio album, Stripped (2002).\\nAction 3 Lookup[Billboard Hot 100]\\nObservation 3 (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the\\nUnited States, where it was certified Gold for 500,000 units shipped.\\nAction 4 Finish[NOT ENOUGH INFO]\\nCoT Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nThought Nikolaj William Coster-Waldau appeared in the 2009 Fox television film\\nVirtuality, so he has worked with the Fox Broadcasting Company.\\nAnswer SUPPORTS\\nClaim Stranger Things is set in Bloomington, Indiana.\\nThought Stranger Things is in the fictional town of Hawkins, Indiana, not in\\nBloomington, Indiana.\\nAnswer REFUTES\\nContinued on next page\\n20', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 19}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nFever Prompts – Continued from previous page\\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\\nThought The song peaked at number two on the Billboard Hot 100 in the United States,\\nbut not sure if it was in 2003.\\nAnswer NOT ENOUGH INFO\\nReAct Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nThought 1 I need to search Nikolaj Coster-Waldau and find if he has worked with the\\nFox Broadcasting Company.\\nAction 1 Search[Nikolaj Coster-Waldau]\\nObservation 1 Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and\\nproducer. He graduated from the Danish National School of Performing Arts\\nin Copenhagen in 1993,[1] and had his breakthrough role in Denmark with\\nthe film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy\\ndrama series Game of Thrones, for which he received two Primetime Emmy\\nAward nominations for Outstanding Supporting Actor in a Drama Series..\\nCoster-Waldau has appeared in numerous films in his native Denmark and\\nScandinavia, including Headhunters (2011) and A Thousand Times Good Night\\n(2013). In the U.S, his debut film role was in the war film Black Hawk Down\\n(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a\\ndetective in the short-lived Fox television series New Amsterdam (2008), and\\nappeared in the 2009 Fox television film Virtuality, originally intended as\\na pilot.\\nThought 2 Because he \"appeared in the 2009 Fox television film Virtuality\", he should\\nhave worked with the Fox Broadcasting Company.\\nAction 2 Finish[SUPPORTS]\\nClaim Stranger Things is set in Bloomington, Indiana.\\nThought 1 I should search for Stranger Things, and see if it is set in Bloomington,\\nIndiana.\\nAction 1 Search[Stranger Things]\\nObservation 1 Stranger Things is an American science fiction horror drama television\\nseries created by the Duffer Brothers. Set in the 1980s, primarily in\\nthe fictional town of Hawkins, Indiana, the series centers on a number of\\nmysteries and supernatural events occurring around the town and their impact\\non an ensemble of child and adult characters.\\nThought 2 The observation says that it is set in a \"fictional town of Hawkins,\\nIndiana\", so it is not set in Bloomington.\\nAction 2 Finish[REFUTES]\\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\\nThought 1 I need to search the song Beautiful and find if it reached number two on the\\nBillboard Hot 100 in 2003.\\nAction 1 Search[Beautiful]\\nObservation 1 Could not find [Beautiful]. Similar: [’Beautiful’, ’Beautiful, Beautiful’,\\n’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is\\nBeautiful’].\\nThought 2 From suggestions, I should search \"Beautiful (Christina Aguilera song)\" to\\nfind the song.\\nAction 2 Search[Beautiful (Christina Aguilera song)]\\nObservation 2 \"Beautiful\" is a song recorded by American singer Christina Aguilera for her\\nfourth studio album, Stripped (2002).\\nThought 3 It does not mention Billboard, so I need to look up \"Billboard Hot 100\" to\\nfind if it reached number two on it in 2003.\\nAction 3 Lookup[Billboard Hot 100]\\nObservation 3 (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the\\nUnited States, where it was certified Gold for 500,000 units shipped.\\nThought 4 It only says the song peaked at number two on the Billboard Hot 100, but not\\nif it was in 2003. I am not sure if this claim is true or not.\\nAction 4 Finish[NOT ENOUGH INFO]\\n21', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 20}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nC.3 W EBSHOP\\nTable 6: Prompts used for the Webshop experiments in Section 4. We use the prompt on the left for\\nAct, and use the prompt on the right (add reasoning) for ReAct .\\nInstruction : i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars\\nAct ReAct\\nAction :search[\\n3 ounce bright citrus deodorant sensitive skin]\\nObservation:[\\nBack to Search]\\nPage 1 (Total results: 50)[\\nNext]\\n[\\nB078GWRC1J]\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99[\\nB078GTKVXY]\\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99[\\nB08KBVJ4XN]\\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men\\n, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar\\n& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,\\n2.7 oz, 2-Pack)\\n$15.95\\nAction :click[\\nB078GWRC1J]\\nObservation :[\\nBack to Search]\\n[\\nPrev]\\nscent [assorted scents][bright citrus][calming lavender][ginger\\n[fresh] simply non-scents]\\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2\\n-pack)]\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\nPrice: $10.99\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction :click [bright citrus]\\nObservation : You have clicked bright citrus.\\nAction :click [3 ounce (pack of 1)]\\nObservation : You have clicked 3 ounce (pack of 1).\\nAction :click [Buy Now]Action :search[\\n3 ounce bright citrus deodorant sensitive skin\\nObservation:[\\nBack to Search]\\nPage 1 (Total results: 50)[\\nNext]\\n[\\nB078GWRC1J]\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99[\\nB078GTKVXY]\\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99[\\nB08KBVJ4XN]\\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men\\n, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar\\n& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,\\n2.7 oz, 2-Pack)\\n$15.95\\nAction :think [B078GWRC1J and B078GTKVXY are bright citrus\\ndeodorant less then 50 dollars. I can check B078GWRC1J ﬁrst.\\nObservation : OK.\\nAction :click[\\nB078GWRC1J]\\nObservation :[\\nBack to Search]\\n[\\nPrev]\\nscent [assorted scents][bright citrus][calming lavender][ginger\\n[fresh] simply non-scents]\\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2\\n-pack)]\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\nPrice: $10.99\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction :think [For 3 ounce bottle of bright citrus deodorant\\nfor sensitive skin, the item has options ’bright citrus’\\nand ’3 ounce (pack of 1)’ and seems good to buy.]\\nObservation : OK.\\nAction :click [bright citrus]\\nObservation : You have clicked bright citrus.\\nAction :click [3 ounce (pack of 1)]\\nObservation : You have clicked 3 ounce (pack of 1).\\nAction :click [Buy Now]\\n22', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 21}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nC.4 ALFW ORLD\\nTable 7: An Act prompt on the ALFWorld clean task. No thoughts are provided.\\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\\nYour task is to: put a clean lettuce in diningtable.\\n>go to fridge 1\\nThe fridge 1 is closed.\\n>open fridge 1\\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\\npotato 3, and a potato 2.\\n>go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\\n>take lettuce 1 from diningtable 1\\nYou pick up the lettuce 1 from the diningtable 1.\\n>go to sinkbasin 1\\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\\n>clean lettuce 1 with sinkbasin 1\\nYou clean the lettuce 1 using the sinkbasin 1.\\n>think: Now I clean a lettuce (1). Next, I need to put it in/on diningtable 1.\\nOK.\\n>go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\\nspatula 3, a tomato 2, and a tomato 1.\\n>put lettuce 1 in/on diningtable 1\\nYou put the lettuce 1 in/on the diningtable 1.\\n23', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 22}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nTable 8: An ReAct prompt on the ALFWorld clean task.\\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\\nYour task is to: put a clean lettuce in diningtable.\\n> think: To solve the task, I need to find and take a lettuce, then clean it with\\nsinkbasin, then put it in diningtable.\\nOK.\\n> think: First I need to find a lettuce. A lettuce is more likely to appear in\\nfridge (1), diningtable (1), sinkbasin (1), stoveburner (1-3), cabinet (1-13). I\\ncan check one by one, starting with fridge 1.\\nOK.\\n> go to fridge 1\\nThe fridge 1 is closed.\\n> open fridge 1\\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\\npotato 3, and a potato 2.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\\n> think: Now I find a lettuce (1). Next, I need to take it.\\nOK.\\n> take lettuce 1 from diningtable 1\\nYou pick up the lettuce 1 from the diningtable 1.\\n> think: Now I take a lettuce (1). Next, I need to go to sinkbasin (1) and clean\\nit.\\nOK.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\\n> clean lettuce 1 with sinkbasin 1\\nYou clean the lettuce 1 using the sinkbasin 1.\\n> think: Now I clean a lettuce (1). Next, I need to put it in/on diningtable 1.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\\nspatula 3, a tomato 2, and a tomato 1.\\n> put lettuce 1 in/on diningtable 1\\nYou put the lettuce 1 in/on the diningtable 1.\\n24', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 23}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nTable 9: An ReAct-IM prompt on the ALFWorld clean task.\\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\\nYour task is to: put a clean lettuce in diningtable.\\n> think: To solve the task, I need to find and take a lettuce, then clean it with\\nsinkbasin, then put it in diningtable.\\nOK.\\n> think: First I need to find a lettuce.\\nOK.\\n> go to fridge 1\\nThe fridge 1 is closed.\\n> think: First I need to find a lettuce.\\nOK.\\n> open fridge 1\\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\\npotato 3, and a potato 2.\\n> think: First I need to find a lettuce.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\\n> think: First I need to find a lettuce.\\nOK.\\n> take lettuce 1 from diningtable 1\\nYou pick up the lettuce 1 from the diningtable 1.\\n> think: I need to clean this lettuce (1) using sinkbasin 1.\\nOK.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\\n> think: I need to clean this lettuce (1) using sinkbasin 1.\\nOK.\\n> clean lettuce 1 with sinkbasin 1\\nYou clean the lettuce 1 using the sinkbasin 1.\\n> think: I need to put this cleaned lettuce (1) in/on diningtable 1.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\\nspatula 3, a tomato 2, and a tomato 1.\\n> think: I need to put this cleaned lettuce (1) in/on diningtable 1.\\nOK.\\n> put lettuce 1 in/on diningtable 1\\nYou put the lettuce 1 in/on the diningtable 1.\\nD T RAJECTORIES\\nD.1 FEVER T RAJECTORIES\\nWe provide some trajectories produced by ReAct ,CoTandActon several randomly selected exam-\\nples from the FEVER dev set below. Search results are omitted to space.\\n25', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 24}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nExample 2491 (gt: REFUTES)\\nReAct\\nClaim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.\\nThought 1: I need to search Bermuda Triangle and find if it is in the Pacific Ocean.\\nAction 1: Search[Bermuda Triangle]\\nObservation 1: Reportedly haunted locations...\\nThought 2: The observation says that it is in the western part of the North Atlantic\\nOcean, so it is not in the Pacific Ocean.\\nAction 2: Finish[REFUTES]\\nObservation 2: Episode finished\\nAct\\nClaim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.\\nAction 1 : Search[Bermuda Triangle]\\nObservation 1 : Reportedly haunted locations:...\\nAction 2: Finish[REFUTES]\\nObservation 2: Episode finished\\nCoT\\nClaim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.\\nThought: Let’s think step by step. The Bermuda Triangle is a loosely-defined region\\nin the Atlantic Ocean. The Pacific Ocean is not the Atlantic Ocean.\\nAnswer: REFUTES\\nExample 5908 (gt: SUPPORTS)\\nReAct\\nClaim: Princess Mononoke is a film.\\nThought 1: I need to search Princess Mononoke and find if it is a film.\\nAction 1: Search[Princess Mononoke]\\nObservation 1: Princess Mononoke ...\\nThought 2: From the observation, it says that Princess Mononoke is a film.\\nAction 2: Finish[SUPPORTS]\\nObservation 2: Episode finished\\nAct\\nClaim: Princess Mononoke is a film.\\nAction 1: Search[Princess Mononoke]\\nObservation 1: Princess Mononoke ...\\nAction 2: Finish[SUPPORTS]\\nObservation 2: Episode finished\\nCoT\\nClaim: Princess Mononoke is a film.\\nThought: Let’s think step by step. Princess Mononoke is a movie, and movies are\\nfilms.\\nAnswer: SUPPORTS\\nExample 1951 (gt: REFUTES)\\nReAct\\nClaim: Soyuz was part of the American space program.\\nThought 1: I need to search Soyuz and find if it was part of the American space\\nprogram.\\nAction 1: Search[Soyuz]\\nObservation 1: Soyuz is a transliteration...\\nThought 2: It does not mention the American space program, so I need to search\\n\"American space program\" to find if it was part of it.\\nAction 2: Search[American space program]\\nObservation 2: The space policy...\\nThought 3: It does not mention Soyuz, so I am not sure if this claim is true or not.\\nAction 3: Finish[NOT ENOUGH INFO]\\nObservation 3: Episode finished\\nAct\\nClaim: Soyuz was part of the American space program.\\nAction 1: Search[Soyuz]\\nObservation 1: Soyuz is a transliteration...\\nAction 2: Finish[NOT ENOUGH INFO]\\nObservation 2: Episode finished\\nCoT\\nClaim: Soyuz was part of the American space program.\\nThought: Let’s think step by step. Soyuz is a Russian spacecraft. The American\\nspace program is NASA. NASA and Russia have worked together on the\\nInternational Space Station.\\nAnswer: SUPPORTS\\n26', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 25}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nExample 3208 (gt: REFUTES)\\nReAct\\nClaim: Reign Over Me is an American film made in 2010.\\nThought 1: I need to search for Reign Over Me and find if it is an American film made\\nin 2010.\\nAction 1: Search[Reign Over Me]\\nObservation 1: Reign Over Me ...\\nThought 2: The observation says that it is an American film made in 2007, so it is not\\nmade in 2010.\\nAction 2: Finish[REFUTES]\\nObservation 2: Episode finished\\nAct\\nClaim: Reign Over Me is an American film made in 2010.\\nAction 1: Search[Reign Over Me]\\nAction 2: Finish[SUPPORTS]\\nObservation 2: Episode finished\\nCoT\\nClaim: Reign Over Me is an American film made in 2010.\\nThought: Let’s think step by step. First, is Reign Over Me an American film? Yes,\\nit is. Second, was it made in 2010? Yes, it was.\\nAnswer: SUPPORTS\\nD.2 ALFW ORLD TRAJECTORIES\\nWe provide an example trajectory of ReAct ,Act, andReAct-IM on the same ALFWorld game,\\npick_clean_then_place_in_recep-Knife-None-CounterTop-10/\\ntrial_T20190909_110445_675754 .\\nD.2.1 REAC TTRAJECTORY\\nReAct is able to correctly ﬁnd the knife, clean it, and then place it on a countertop.\\n27', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 26}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nReAct ALFWorld trajectory\\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to: put a clean knife in countertop.\\n> think: To solve the task, I need to find and take a knife, then clean it with\\nsinkbasin, then put it in countertop.\\nOK.\\n> think: First I need to find a knife. A knife is more likely to appear in cabinet\\n(1-6), drawer (1-3), countertop (1-3), fridge (1), garbagecan (1), shelf (1-3),\\nsinkbasin (1), stoveburner (1-4), toaster (1). I can check one by one, starting with\\ncabinet 1.\\nOK.\\n> go to cabinet 1\\nOn the cabinet 1, you see a bowl 1.\\n> go to cabinet 2\\nThe cabinet 2 is closed.\\n> go to cabinet 3\\nOn the cabinet 3, you see a glassbottle 1.\\n> go to cabinet 4\\nOn the cabinet 4, you see a mug 1.\\n> go to cabinet 5\\nThe cabinet 5 is closed.\\n> go to cabinet 6\\nThe cabinet 6 is closed.\\n> go to drawer 1\\nThe drawer 1 is closed.\\n> go to drawer 2\\nThe drawer 2 is closed.\\n> go to drawer 3\\nThe drawer 3 is closed.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n> think: Now I find a knife (1). Next, I need to take it.\\nOK.\\n> take knife 1 from countertop 2\\nYou pick up the knife 1 from the countertop 2.\\n> think: Now I take a knife (1). Next, I need to go to sinkbasin (1) and clean it.\\nOK.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a fork 3, a lettuce 3, and a spatula 2.\\n> clean knife 1 with sinkbasin 1\\nYou clean the knife 1 using the sinkbasin 1.\\n> think: Now I clean a knife (1). Next, I need to put it in/on countertop 1.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> put knife 1 in/on countertop 1\\nYou put the knife 1 in/on the countertop 1.\\nD.2.2 AC TTRAJECTORY\\nAct is able to correctly ﬁnd the knife, but tries to clean it using a sinkbasin before actually going\\nto the sinkbasin. From there on, the model is unable to recover, and is stuck repeating the same\\nsequence of commands. With ReAct , an existing thought would have concretely identiﬁed that the\\nknife had been taken, and that the next subgoal was to go to a sinkbasin to clean it.\\n28', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 27}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nAct ALFWorld trajectory\\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to: put a clean knife in countertop.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nYou pick up the knife 1 from the countertop 2.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\nD.2.3 REAC T-IM TRAJECTORY\\nReAct-IM is able to correctly ﬁnd the knife, but is unable to clean it as well. The LLM then\\ngets stuck trying to place this knife on a countertop. One possible issue is the incorrect thought\\nthink: I need to find a clean knife. , which seems to trick the LLM in believing\\nthat the knife is already cleaned. In ReAct , this sequence of subgoals would be concretely captured\\nby each thought.\\n29', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 28}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nReAct-IM ALFWorld trajectory\\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to: put a clean knife in countertop.\\n> think: To solve the task, I need to find and take a clean knife, then put it in\\ncountertop.\\nOK.\\n> think: I need to find a clean knife.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> think: I need to find a clean knife.\\nOK.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n> think: I need to find a clean knife.\\nOK.\\n> take knife 1 from countertop 2\\nYou pick up the knife 1 from the countertop 2.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nYou put the knife 1 in/on the countertop 1.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n30', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 29}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nD.3 W EBSHOP TRAJECTORIES\\nTable 10: Example trajectories for Webshop predicted by Act andReAct . Compared to Act,\\nReAct uses reasoning to ﬁnd products that satisfy all target attributes.\\nInstruction : get me a sixteen pack of apple cinnamon freeze dried banana chips, and price lower than 50.00 dollars\\nAct ReAct\\nAction :search[\\nsixteen pack apple cinnamon freeze dried banana\\nchips]\\nObservation:[\\nBack to Search]\\nPage 1 (Total results: 50)[\\nNext]\\n[\\nB0061IVFZE]\\nBrothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -\\nOunce (Pack of 100)\\n$85.0[\\nB096H2P6G2]\\nMoon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -\\n(Variety Pack)\\n$18.99[\\nB092JLLYK6]\\nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\\nFor School Lunches or an On-The-Go Snack - No Sugar Added,\\nNon GMO, Gluten Free, Nothing Artiﬁcial (0.53oz) 6-Pack\\n$12.99\\nAction :click[\\nB0061IVFZE]\\nObservation :[\\nBack to Search]\\n[\\nPrev]\\nﬂavor name [asian pear][banana][fuji apple & cinnamon][strawberry\\nbanana] Brothers-ALL-Natural Fruit Crisps, Strawberry Banana,\\n0.42 Ounce (Pack of 100)\\nPrice: $85.0\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction :click [Buy Now]Action :search[\\nsixteen pack apple cinnamon freeze dried banana\\nchips]\\nObservation:[\\nBack to Search]\\nPage 1 (Total results: 50)[\\nNext]\\n[\\nB0061IVFZE]\\nBrothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -\\nOunce (Pack of 100)\\n$85.0[\\nB096H2P6G2]\\nMoon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -\\n(Variety Pack)\\n$18.99[\\nB092JLLYK6]\\nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\\nFor School Lunches or an On-The-Go Snack - No Sugar Added,\\nNon GMO, Gluten Free, Nothing Artiﬁcial (0.53oz) 6-Pack\\n$12.99\\nAction :think [B0061IVFZE is strawberry banana, not apple cinnamon.\\nB096H2P6G2 is fruit snacks, not freeze dried banana chips.\\nB092JLLYK6 is banana crisps, not apple cinnamon. I can check\\nB092JLLYK6 ﬁrst.\\nObservation : OK.\\nAction :click[\\nB092JLLYK6]\\nObservation :[\\nBack to Search]\\n[\\nPrev]\\nﬂavor name [apple][apple cinnamon][banana][cantaloupe][peach][pear]\\n[strawberry][strawberry banana][sampler variety pack][mega variety\\npack][orchard variety pack]\\nsize [0.53 ounce (pack of 6)][0.53 ounce (pack of 8)][0.53 ounce (pack\\nof 16)]\\nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\\nFor School Lunches or an On-The-Go Snack - No Sugar Added, Non\\nGMO, Gluten Free, Nothing Artiﬁcial (0.53oz) 6-Pack\\nPrice: $12.99\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction :think [For sixteen pack of apple cinnamon freeze dried banana\\nchips, the item has options ’apple cinnamon’ and ’0.53 ounce (pack of\\n16)’ and seems good to buy.]\\nObservation : OK.\\nAction :click [apple cinnamon]\\nObservation : You have clicked apple cinnamon.\\nAction :click [0.53 ounce (pack of 16)]\\nObservation : You have clicked 0.53 ounce (pack of 16).\\nAction :click [Buy Now]\\nScore : 0.125 Score : 1.0\\n31', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 30}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nE M ORE ANALYSIS\\nE.1 S UCCESS AND FAILURE MODES ANALYSIS\\nWe provide some examples corresponding to the success and error mode analysis given in Sec. 3.3.\\nSearch results and non-representative steps are omitted to reduce space.\\nSuccess: True positive\\nReAct\\nQuestion: Author David Chanoff has collaborated with a U.S. Navy admiral who served as\\nthe ambassador to the United Kingdom under which President?\\nThought 1: I need to search David Chanoff and find the U.S. Navy admiral he\\ncollaborated with.\\nAction 1: Search[David Chanoff]\\nObservation 1: David Chanoff is a noted author...\\nAction 2: Search[William J. Crowe]\\nObservation 2: William J. Crowe is...\\nThought 3: William J. Crowe served as the ambassador to the United Kingdom under\\nPresident Bill Clinton. So the answer is Bill Clinton.\\nAction 3: Finish[Bill Clinton]\\nObservation 3: Episode finished\\nCoT\\nQuestion: What was the name of the team that represented an Arizona university in\\nthe conference that covers a broad expanse of the western United States,\\nincluding Arizona, California, and New Mexico?\\nThought: Let’s think step by step. The name of the team that represented an Arizona\\nuniversity in the conference that covers a broad expanse of the western\\nUnited States, including Arizona, California, and New Mexico is Arizona\\nState Sun Devils.\\nAnswer: Arizona State Sun Devils\\nSuccess: False positive\\nReAct\\nQuestion: Did the board game San Marco or About Time come out first?\\nThought 1: I need to search San Marco and About Time, and find which came out first.\\nAction 1: Search[San Marco]\\nObservation 1: San Marco is one of the ... The was built in venice.\\nThought 2: San Marco is a sestiere of Venice. I need to search About Time next.\\nAction 2: Search[About Time]\\nObservation 2: Could not find [About Time]. Similar: [...].\\nThought 3: About Time is a 2013 film. So San Marco came out first.\\nAction 3: Finish[San Marco]\\nObservation 3: Episode finished\\nCoT\\nQuestion: Was Vanderbilt University or Emory University founded first?\\nThought: Let’s think step by step. Vanderbilt University was founded in 1873. Emory\\nUniversity was founded in 1836. 1873 (Vanderbilt University) > 1836 (Emory\\nUniversity), so Vanderbilt University was founded first.\\nAnswer: Emory University\\nFailure: Reasoning error\\nReAct\\nQuestion: What was the middle name of the actress who was cast alongside Harvey\\nFierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura\\nBell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur in\\nHairspray?\\nThought 1: I need to search Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell,\\nMary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and\\nMarissa Jaret Winokur, find the actress that was cast alongside them in\\nHairspray, then find its middle name.\\nAction 1: Search[Harvey Fierstein]\\nObservation 1: Harvey Forbes Fierstein ...\\nThought 2: Harvey Fierstein is an actor, playwright, and screenwriter. I need to\\nsearch Linda Hart next...\\nAction 2: Search[Linda Hart]...\\nCoT\\nQuestion: Who is older Danny Green or James Worthy?\\nThought: Let’s think step by step. Danny Green was born in 1987. James Worthy was\\nborn in 1961. 1987 (Danny Green) > 1961 (James Worthy), so Danny Green is\\nolder.\\nAnswer: Danny Green\\n32', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 31}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2023\\nFailure: Search error\\nReAct\\nQuestion: What does the goddess associated with the goddess frigg consists of what\\ntales?\\nThought 1: I need to search goddess frigg and find its associated goddess.\\nAction 1: Search[goddess frigg]\\nObservation 1: Could not find goddess frigg. Similar: [’Frigg’, ’Frigg and Freyja common\\norigin hypothesis’, ’Freyja (section Relation to Frigg and other goddesses\\nand figures)’, ’Eir (redirect from Eir (goddess))’, ’Baldr’].\\nThought 2: I need to search Frigg instead...\\nFailure: Hallucination\\nCoT\\nQuestion: What was the first year a scientific journal published by an organization\\nlocated in the Bronx was published?\\nThought: Let’s think step by step. The first year a scientific journal published by\\nan organization located in the Bronx was published was 1916.\\nAnswer: 1916 (Label: 1909)\\nFailure: Label ambiguity\\nReAct\\nQuestion: What nationality is the man who developed the dictator game?\\nThought 1: I need to search dictator game, find the man who developed it, then find his\\nnationality.\\nAction 1: Search[dictator game]\\nObservation 1: The dictator game is a popular experimental instrument in social psychology\\nand economics...\\nAction 3: Search[Amos Tversky]\\nObservation 3: Amos Nathan Tversky...\\nThought 4: Amos Tversky is an Israeli cognitive and mathematical psychologist. So the\\nman who developed the dictator game is Israeli.\\nAction 4: Finish[Israeli] (label: Israel-American)\\nCoT\\nQuestion: Kurt Fuller from Stockton, CA appeared in which American Detective Comedy\\nDrama television series created by Steve Franks?\\nThought: Let’s think step by step. Kurt Fuller from Stockton, CA appeared in Psych.\\nPsych is an American Detective Comedy Drama television series created by\\nSteve Franks.\\nAnswer: Psych (label: Psych is an American detective comedy-drama)\\n33', metadata={'source': '/var/folders/qx/skm_3qm93bg39tjh5fyb02y80000gn/T/tmpb78ma328/tmp.pdf', 'page': 32})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_by_pdf_loader = pdf_loader.load()\n",
    "arxiv_by_pdf_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5333f4a7-b88b-4f3d-aaa4-00cefe675343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arxiv_by_pdf_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8199d96b-1e73-4a64-b26c-6fa781742c0c",
   "metadata": {},
   "source": [
    "1ページごとPDFのコンテンツが読み込まれている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73016873-c090-40b5-ad13-47c9b5dfc65b",
   "metadata": {},
   "source": [
    "読み込まれたコンテンツを確認するためには、 `page_content` を利用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "96ff32d1-cd98-4c61-8d29-2bfe73dd300a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published as a conference paper at ICLR 2023\\nREAC T: S YNERGIZING REASONING AND ACTING IN\\nLANGUAGE MODELS\\nShunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\\n1Department of Computer Science, Princeton University\\n2Google Research, Brain team\\n1{shunyuy,karthikn}@princeton.edu\\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\\nABSTRACT\\nWhile large language models (LLMs) have demonstrated impressive performance\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\\nplan generation) have primarily been studied as separate topics. In this paper, we\\nexplore the use of LLMs to generate both reasoning traces and task-speciﬁc actions\\nin an interleaved manner, allowing for greater synergy between the two: reasoning\\ntraces help the model induce, track, and update action plans as well as handle\\nexceptions, while actions allow it to interface with and gather additional information\\nfrom external sources such as knowledge bases or environments. We apply our\\napproach, named ReAct , to a diverse set of language and decision making tasks\\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\\nimproved human interpretability and trustworthiness. Concretely, on question\\nanswering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent\\nissues of hallucination and error propagation in chain-of-thought reasoning by\\ninteracting with a simple Wikipedia API, and generating human-like task-solving\\ntrajectories that are more interpretable than baselines without reasoning traces.\\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\\nan absolute success rate of 34% and 10% respectively, while being prompted with\\nonly one or two in-context examples.\\n1 I NTRODUCTION\\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\\nsider the example of cooking up a dish in the kitchen. Between any two speciﬁc actions, we may\\nreason in language in order to track progress (“now that everything is cut, I should heat up the pot of\\nwater”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let\\nme use soy sauce and pepper instead”), and to realize when external information is needed (“how do\\nI prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the\\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What\\ndish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans\\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\\nunseen circumstances or facing information uncertainties.\\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\\n∗Work during Google internship. Projet page with code: https://react-lm.github.io/ .\\n1arXiv:2210.03629v3  [cs.CL]  10 Mar 2023'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_by_pdf_loader[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672a1ec-4afe-41f3-b65c-8efe0d1cbe26",
   "metadata": {},
   "source": [
    "### arXivから論文を取得するLoader\n",
    "ArxivLoaderはarxivとpymupdfをインストールする必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b09c3d6-d827-4442-ba9f-1f645af66d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (2.0.0)\n",
      "Requirement already satisfied: pymupdf in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (1.23.5)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: requests==2.31.0 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from arxiv) (2.31.0)\n",
      "Requirement already satisfied: sgmllib3k in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (2023.7.22)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.5 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from pymupdf) (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d49bda9-432c-44ce-937c-8448df469015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "arxiv_loader = ArxivLoader(\"2210.03629\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d3cd755-8719-4903-bf52-11347912d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_by_arxiv_loader = arxiv_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "45993bc5-c6ca-4c7a-b01e-c4fa77bcf69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arxiv_by_arxiv_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ede05d-f1c0-4add-9a24-61b6b5bd766e",
   "metadata": {},
   "source": [
    "ArxivLoaderでは１つのDocumentとして読み込まれる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "05468080-0a9c-4e94-aba5-1c9a631eef72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published as a conference paper at ICLR 2023\\nREACT: SYNERGIZING REASONING AND ACTING IN\\nLANGUAGE MODELS\\nShunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\\n1Department of Computer Science, Princeton University\\n2Google Research, Brain team\\n1{shunyuy,karthikn}@princeton.edu\\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\\nABSTRACT\\nWhile large language models (LLMs) have demonstrated impressive performance\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\\nplan generation) have primarily been studied as separate topics. In this paper, we\\nexplore the use of LLMs to generate both reasoning traces and task-speciﬁc actions\\nin an interleaved manner, allowing for greater synergy between the two: reasoning\\ntraces help the model induce, track, and update action plans as well as handle\\nexceptions, while actions allow it to interface with and gather additional information\\nfrom external sources such as knowledge bases or environments. We apply our\\napproach, named ReAct, to a diverse set of language and decision making tasks\\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\\nimproved human interpretability and trustworthiness. Concretely, on question\\nanswering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent\\nissues of hallucination and error propagation in chain-of-thought reasoning by\\ninteracting with a simple Wikipedia API, and generating human-like task-solving\\ntrajectories that are more interpretable than baselines without reasoning traces.\\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\\nan absolute success rate of 34% and 10% respectively, while being prompted with\\nonly one or two in-context examples.\\n1\\nINTRODUCTION\\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\\nsider the example of cooking up a dish in the kitchen. Between any two speciﬁc actions, we may\\nreason in language in order to track progress (“now that everything is cut, I should heat up the pot of\\nwater”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let\\nme use soy sauce and pepper instead”), and to realize when external information is needed (“how do\\nI prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the\\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What\\ndish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans\\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\\nunseen circumstances or facing information uncertainties.\\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\\n∗Work during Google internship. Projet page with code: https://react-lm.github.io/.\\n1\\narXiv:2210.03629v3  [cs.CL]  10 Mar 2023\\nPublished as a conference paper at ICLR 2023\\n�������������������������������������������������������� �\\n����������������������������������������������������������\\n��\\n���������������������� �\\n������������������������������������������ �\\n����������������������������������������������������������������������������\\n������������������������� �\\n���������������������������������������������������������\\n�\\n���������������������������������������\\n��������������������������������������������� �\\n�������������������������������������������������������������\\n�\\n����������������������������������������������������������\\n�\\n���������������������� �\\n��������������������� �\\n�������������������������� �\\n��������������������� �\\n���������������������������\\n������������������������������������������ �\\n���������������������������������������������������\\n�������������\\n������������\\n����������������������\\n������������������������� �\\n��������� ���������������� �\\n���������������������� �\\n����������������������� �\\n�������������������������� �\\n������������������������� �\\n����������������������� �\\n���������������������� �\\n������\\n�������������������������� �\\n�����\\n��������������\\n��������������������������� �\\n�������������������������� ���\\n����������������\\n����������������������� ��\\n���������������������������� �\\n����������������\\n����������������������� �\\n���������� ��\\n��������������������� �\\n������������������������� �\\n����������\\n������������������\\n�������������������������\\n����������������������������������������������������� �\\n����������������������������������������������������\\n���������������������������\\n���������������������������������������������������������\\n�\\n����������������������������������������������������������\\n�\\n���������������������� ����������\\n����������������������������������������������������������\\n�\\n�����������������������������������������������������\\n������\\n���������������������������������������������������\\n������������������������\\n�������������������������������������������������������\\n�\\n����������������������������������������� ��������� �\\n���������� �����\\n���������������������������������������������������� ������\\n�������������� ��\\n���������������������������������� �\\n����������������������������������������������������������\\n���������������������������������������������������������\\n�\\n������������������������������������� ����������������� �\\n�����������������������\\n������������������������������������ �\\n�\\n��������������������������������������������������������\\n�\\n���������������������������������������������������\\n�\\n��������������������������\\n��������������\\n��������������������� �\\n������������������������������ �\\n�������������������� �\\n����������������������������������������������������������\\n�\\n��������������������������������������\\n�\\n������������������������ �\\n����������������������������������������������������\\n�\\n������������������������� �\\n������������������������������������������� �\\n����������������������� �\\n������������������������������������������� �\\n����������������������� �\\n�������������������������\\n�\\n�������������������������������������������������������\\n�\\n����������������������������������������������������������\\n�\\n���������������������������������������������������\\n�\\n����������������������������������������������������\\n������������\\n��������������\\nFigure 1: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought (CoT,\\nReason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)\\nquestion. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar\\net al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task\\nsolving trajectories generated by the model (Act, Thought) and the environment (Obs).\\nanswers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,\\n2022). However, this “chain-of-thought” reasoning is a static black box, in that the model uses\\nits own internal representations to generate thoughts and is not grounded in the external world,\\nwhich limits its ability to reason reactively or update its knowledge. This can lead to issues like fact\\nhallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,\\nrecent work has explored the use of pre-trained language models for planning and acting in interactive\\nenvironments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with\\na focus on predicting actions via language priors. These approaches usually convert multi-modal\\nobservations into text, use a language model to generate domain-speciﬁc actions or plans, and then\\nuse a controller to choose or execute them. However, they do not employ language models to reason\\nabstractly about high-level goals or maintain a working memory to support acting, barring Huang\\net al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the\\ncurrent state. Beyond such simple embodied tasks to interact with a few blocks, there have not been\\nstudies on how reasoning and acting can be combined in a synergistic manner for general task solving,\\nand if such a combination can bring systematic beneﬁts compared to reasoning or acting alone.\\nIn this work, we present ReAct, a general paradigm to combine reasoning and acting with language\\nmodels for solving diverse language reasoning and decision making tasks (Figure 1). ReAct\\nprompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an\\ninterleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and\\nadjust high-level plans for acting (reason to act), while also interact with the external environments\\n(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).\\n2\\nPublished as a conference paper at ICLR 2023\\nWe conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:\\nquestion answering (HotPotQA, Yang et al., 2018), fact veriﬁcation (Fever, Thorne et al., 2018),\\ntext-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao\\net al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact\\nwith, ReAct outperforms vanilla action generation models while being competitive with chain-of-\\nthought reasoning (CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct\\nand CoT that allows for the use of both internal knowledge and externally obtained information\\nduring reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able\\nto outperform imitation or reinforcement learning methods trained with 103 ∼ 105 task instances,\\nwith an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate\\nthe importance of sparse, versatile reasoning in decision making by showing consistent advantages\\nover controlled baselines with actions only. Besides general applicability and performance boost,\\nthe combination of reasoning and acting also contributes to model interpretability, trustworthiness,\\nand diagnosability across all domains, as humans can readily distinguish information from model’s\\ninternal knowledge versus external environments, as well as inspect reasoning traces to understand\\nthe decision basis of model actions.\\nTo summarize, our key contributions are the following: (1) we introduce ReAct, a novel prompt-\\nbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we\\nperform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a\\nfew-shot learning setup over prior approaches that perform either reasoning or action generation in\\nisolation; (3) we present systematic ablations and analysis to understand the importance of acting in\\nreasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the\\nprompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial ﬁnetuning\\nexperiments showing the potential of ReAct to improve with additional training data. Scaling up\\nReAct to train and operate on more tasks and combining it with complementary paradigms like\\nreinforcement learning could further unlock the potential of large language models.\\n2\\nREAC T: SYNERGIZING REASONING + ACTING\\nConsider a general setup of an agent interacting with an environment for task solving. At time\\nstep t, an agent receives an observation ot ∈ O from the environment and takes an action at ∈ A\\nfollowing some policy π(at|ct), where ct = (o1, a1, · · · , ot−1, at−1, ot) is the context to the agent.\\nLearning a policy is challenging when the mapping ct �→ at is highly implicit and requires extensive\\ncomputation. For example, the agent shown in Figure 1(1c) is unable to generate the correct ﬁnal\\naction (Act 4) to ﬁnish the QA task as it requires complex reasoning over the trajectory context\\n(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the\\ncontext that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.\\nThe idea of ReAct is simple: we augment the agent’s action space to ˆ\\nA = A ∪ L, where L is the\\nspace of language. An action ˆat ∈ L in the language space, which we will refer to as a thought or a\\nreasoning trace, does not affect the external environment, thus leading to no observation feedback.\\nInstead, a thought ˆat aims to compose useful information by reasoning over the current context ct,\\nand update the context ct+1 = (ct, ˆat) to support future reasoning or acting. As shown in Figure 1,\\nthere could be various types of useful thoughts, e.g. decomposing task goals and create action plans\\n(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),\\nextracting important parts from observations (1d, Thought2, 4), track progress and transit action plans\\n(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.\\nHowever, as the language space L is unlimited, learning in this augmented action space is difﬁcult\\nand requires strong language priors. In this paper, we mainly focus on the setup where a frozen\\nlarge language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context\\nexamples to generate both domain-speciﬁc actions and free-form language thoughts for task solving\\n(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and\\nenvironment observations to solve a task instance (see Appendix C). For the tasks where reasoning is\\nof primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the\\ntask-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision\\nmaking tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to\\n1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B.\\n3\\nPublished as a conference paper at ICLR 2023\\nappear sparsely in the most relevant positions of a trajectory, so we let the language model decide the\\nasynchronous occurrence of thoughts and actions for itself.\\nSince decision making and reasoning capabilities are integrated into a large language model, ReAct\\nenjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is\\nstraightforward as human annotators just type down their thoughts in language on top of their actions\\ntaken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail\\nprompt design for each task in Sections 3 and 4. B) General and ﬂexible: Due to the ﬂexible thought\\nspace and thought-action occurrence format, ReAct works for diverse tasks with distinct action\\nspaces and reasoning needs, including but not limited to QA, fact veriﬁcation, text game, and web\\nnavigation. C) Performant and robust: ReAct shows strong generalization to new task instances\\nwhile learning solely from one to six in-context examples, consistently outperforming baselines with\\nonly reasoning or acting across different domains. We also show in Section 3 additional beneﬁts\\nwhen ﬁnetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.\\nD) Human aligned and controllable: ReAct promises an interpretable sequential decision making\\nand reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,\\nhumans can also control or correct the agent behavior on the go by thought editing, as shown in\\nFigure 5 in Section 4.\\n3\\nKNOWLEDGE-INTENSIVE REASONING TASKS\\nWe begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact\\nveriﬁcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to\\nretrieve information to support reasoning, while also use reasoning to target what to retrieve next,\\ndemonstrating a synergy of reasoning and acting.\\n3.1\\nSETUP\\nDomains\\nWe consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-\\nPotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning\\nover two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact veriﬁcation\\nbenchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based\\non if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only\\nsetup for both tasks, where models only receive the question/claim as input without access to support\\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\\nan external environment to support reasoning.\\nAction Space\\nWe design a simple Wikipedia web API with three types of actions to support\\ninteractive information retrieval: (1) search[entity], which returns the ﬁrst 5 sentences from\\nthe corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the\\nWikipedia search engine, (2) lookup[string], which would return the next sentence in the page\\ncontaining string, simulating Ctrl+F functionality on the browser. (3) finish[answer], which\\nwould ﬁnish the current task with answer. We note that this action space mostly can only retrieve a\\nsmall part of a passage based on exact passage name, which is signiﬁcantly weaker than state-of-the-\\nart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\\nand force models to retrieve via explicit reasoning in language.\\n3.2\\nMETHODS\\nReAct Prompting\\nFor HotpotQA and Fever, we randomly select 6 and 3 cases2 from the training\\nset and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.\\nSimilar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\\nthought), where free-form thoughts are used for various purposes. Speciﬁcally, we use a combination\\nof thoughts that decompose questions (“I need to search x, ﬁnd y, then ﬁnd z”), extract information\\nfrom Wikipedia observations (“x was started in 1844”, “The paragraph does not tell x”), perform\\ncommonsense (“x is not y, so z must instead be...”) or arithmetic reasoning (“1844 < 1989”), guide\\n2We ﬁnd more examples do not improve performance.\\n4\\nPublished as a conference paper at ICLR 2023\\nPrompt Methoda\\nHotpotQA\\nFever\\n(EM)\\n(Acc)\\nStandard\\n28.7\\n57.1\\nCoT (Wei et al., 2022)\\n29.4\\n56.3\\nCoT-SC (Wang et al., 2022a)\\n33.4\\n60.4\\nAct\\n25.7\\n58.9\\nReAct\\n27.4\\n60.9\\nCoT-SC → ReAct\\n34.2\\n64.6\\nReAct→ CoT-SC\\n35.1\\n62.0\\nSupervised SoTAb\\n67.5\\n89.5\\nTable 1: PaLM-540B prompting results on\\nHotpotQA and Fever.\\naHotpotQA EM is 27.1, 28.9, 33.8 for Standard, CoT,\\nCoT-SC in Wang et al. (2022b).\\nb(Zhu et al., 2021; Lewis et al., 2020)\\n0\\n5\\n10\\n15\\n20\\n#CoT-SC trials\\n26\\n28\\n30\\n32\\n34\\nHotpotQA EM\\n0\\n5\\n10\\n15\\n20\\n#CoT-SC trials\\n47.5\\n50.0\\n52.5\\n55.0\\n57.5\\n60.0\\n62.5\\n65.0\\nFever Acc\\nMethod\\nCoT-SC -> ReAct\\nReAct -> CoT-SC\\nCoT-SC\\nReAct\\nCoT\\nFigure 2: PaLM-540B prompting results with respect to\\nnumber of CoT-SC samples used.\\nsearch reformulation (“maybe I can search/look up x instead”), and synthesize the ﬁnal answer (“...so\\nthe answer is x”). See Appendix C for more details.\\nBaselines\\nWe systematically ablate ReAct trajectories to build prompts for multiple baselines (with\\nformats as Figure 1(1a-1c)): (a) Standard prompting (Standard), which removes all thoughts,\\nactions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,\\n2022), which removes actions and observations and serve as a reasoning-only baseline. We also\\nbuild a self-consistency baseline (CoT-SC) (Wang et al., 2022a;b) by sampling 21 CoT trajectories\\nwith decoding temperature 0.7 during inference and adopting the majority answer, which is found to\\nconsistently boost performance over CoT. (c) Acting-only prompt (Act), which removes thoughts\\nin ReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the\\nInternet to answer questions, though it operates on a different task and action space, and uses imitation\\nand reinforcement learning instead of prompting.\\nCombining Internal and External Knowledge\\nAs will be detail in Section 3.3, we observe that\\nthe problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT\\nis more accurate in formulating reasoning structure but can easily suffer from hallucinated facts\\nor thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide\\nwhen to switch to the other method based on the following heuristics: A) ReAct → CoT-SC: when\\nReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for\\nHotpotQA and FEVER respectively as we ﬁnd more steps will not improve ReAct performance3.\\nB) CoT-SC → ReAct: when the majority answer among n CoT-SC samples occurs less than n/2\\ntimes (i.e. internal knowledge might not support the task conﬁdently), back off to ReAct.\\nFinetuning\\nDue to the challenge of manually annotating reasoning traces and actions at scale,\\nwe consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories\\nwith correct answers generated by ReAct (also for other baselines) to ﬁnetune smaller language\\nmodels (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on\\ninput questions/claims. More details are in Appendix B.1.\\n3.3\\nRESULTS AND OBSERVATIONS\\nReAct outperforms Act consistently\\nTable 1 shows HotpotQA and Fever results using PaLM-\\n540B as the base model with different prompting methods. We note that ReAct is better than Act\\non both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the\\nﬁnal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also conﬁrm the beneﬁt of reasoning\\ntraces for more informed acting.\\n3Of all trajectories with correct ﬁnal answers, those with 7 steps on HotpotQA and 5 steps on FEVER only\\ntake up 0.84% and 1.33% respectively.\\n5\\nPublished as a conference paper at ICLR 2023\\nType\\nDeﬁnition\\nReAct\\nCoT\\nSuccess\\nTrue positive\\nCorrect reasoning trace and facts\\n94%\\n86%\\nFalse positive\\nHallucinated reasoning trace or facts\\n6%\\n14%\\nFailure\\nReasoning error\\nWrong reasoning trace (including failing to recover from repetitive steps)\\n47%\\n16%\\nSearch result error\\nSearch return empty or does not contain useful information\\n23%\\n-\\nHallucination\\nHallucinated reasoning trace or facts\\n0%\\n56%\\nLabel ambiguity\\nRight prediction but did not match the label precisely\\n29%\\n28%\\nTable 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their\\npercentages in randomly selected examples studied by human.\\nReAct vs. CoT\\nOn the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly\\nlags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only\\ndiffer by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge\\nis vital. To better understand the behavioral difference between ReAct and CoT on HotpotQA, we\\nrandomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct\\nand CoT respectively (thus 200 examples in total), and manually labeled their success and failure\\nmodes in Table 2. Some key observations are as follows:\\nA) Hallucination is a serious problem for CoT, resulting in much higher false positive rate than\\nReAct (14% vs. 6%) in success mode, and make up its major failure mode (56%). In contrast, the\\nproblem solving trajectory of ReActis more grounded, fact-driven, and trustworthy, thanks to the\\naccess of an external knowledge base.\\nB) While interleaving reasoning, action and observation steps improves ReAct’s grounded-\\nness and trustworthiness, such a structural constraint also reduces its ﬂexibility in formulating\\nreasoning steps, leading to more reasoning error rate than CoT. we note that there is one frequent\\nerror pattern speciﬁc to ReAct, in which the model repetitively generates the previous thoughts and\\nactions, and we categorize it as part of “reasoning error” as the model fails to reason about what the\\nproper next action to take and jump out of the loop4.\\nC) For ReAct, successfully retrieving informative knowledge via search is critical. Non-\\ninformative search, which counts for 23% of the error cases, derails the model reasoning and gives\\nit a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between\\nfactuality and ﬂexibility, which motivates our proposed strategies of combining two methods.\\nWe provide examples for each success and failure modes in Appendix E.1. We also ﬁnd some\\nHotpotQA questions may contain outdated answer labels, see Figure 4 for example.\\nReAct + CoT-SC perform best for prompting LLMs\\nAlso shown in Table 1, the best prompting\\nmethod on HotpotQA and Fever are ReAct → CoT-SC and CoT-SC → ReAct respectively.\\nFurthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC\\nsamples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both\\nsigniﬁcantly and consistently outperform CoT-SC across different number of samples, reaching\\nCoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of\\nproperly combining model internal knowledge and external knowledge for reasoning tasks.\\nReAct performs best for ﬁne-tuning\\nFigure 3 shows the scaling effect of prompting/ﬁnetuning\\nfour methods (Standard, CoT, Act, ReAct) on HotpotQA. With PaLM-8/62B, prompting ReAct\\nperforms worst among four methods due to the difﬁculty to learn both reasoning and acting from\\nin-context examples. However, when ﬁnetuned with just 3,000 examples, ReAct becomes the best\\nmethod among the four, with PaLM-8B ﬁnetuned ReAct outperforming all PaLM-62B prompting\\nmethods, and PaLM-62B ﬁnetuned ReAct outperforming all 540B prompting methods. In contrast,\\nﬁnetuning Standard or CoT is signiﬁcantly worse than ﬁnetuning ReAct or Act for both PaLM-\\n8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge\\nfacts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a\\nmore generalizable skill for knowledge reasoning. As all prompting methods are still signiﬁcantly\\nfar from domain-speciﬁc state-of-the-art approaches (Table 1), we believe ﬁnetuning with more\\nhuman-written data might be a better way to unleash the power of ReAct.\\n4We suspect that this could be due to the sub-optimal greedy decoding procedure, and future work using\\nbetter decoding (e.g. beam search) might help address this issue.\\n6\\nPublished as a conference paper at ICLR 2023\\n8b\\n62b\\n540b\\nsize\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nHotpotQA EM\\nlearning = prompt\\n8b\\n62b\\n540b\\nsize\\nlearning = finetune\\nMethod\\nStandard\\nCoT\\nAct\\nReAct\\nFigure 3: Scaling results for prompting and ﬁnetuning on HotPotQA with ReAct (ours) and baselines.\\n4\\nDECISION MAKING TASKS\\nWe also test ReAct on two language-based interactive decision-making tasks, ALFWorld and\\nWebShop, both of which feature complex environments that require agents to act over long horizons\\nwith sparse rewards, warranting the need for reasoning to act and explore effectively.\\nALFWorld\\nALFWorld (Shridhar et al., 2020b) (Figure 1(2)) is a synthetic text-based game designed\\nto align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of\\ntasks in which an agent needs to achieve a high-level goal (e.g. examine paper under desklamp) by\\nnavigating and interacting with a simulated household via text actions (e.g. go to coffeetable 1, take\\npaper 2, use desklamp 1). A task instance can have more than 50 locations and take an expert policy\\nmore than 50 steps to solve, thus challenging an agent to plan and track subgoals, as well as explore\\nsystematically (e.g. check all desks one by one for desklamp). In particular, one challenge built into\\nALFWorld is the need to determine likely locations for common household items (e.g. desklamps will\\nlikely be on desks, shelfs, or dressers), making this environment a good ﬁt for LLMs to exploit their\\npretrained commonsense knowledge. To prompt ReAct, we randomly annotate three trajectories\\nfrom the training set for each task type, where each trajectory includes sparse thoughts that (1)\\ndecompose the goal, (2) track subgoal completion, (3) determine the next subgoal, and (4) reason via\\ncommonsense where to ﬁnd an object and what to do with it. We show prompts used for ALFWorld\\nin Appendix C.4. Following Shridhar et al. (2020b), we evaluate on 134 unseen evaluation games\\nin a task-speciﬁc setup. For robustness, we construct 6 prompts for each task type through each\\npermutation of 2 annotated trajectories from the 3 we annotate. Act prompts are constructed using\\nthe same trajectories, but without thoughts — since task instances are randomly chosen from the\\ntraining set, it favors neither ReAct nor Act and provides a fair and controlled comparison to test the\\nimportance of sparse thoughts. For baselines, we use BUTLER (Shridhar et al., 2020b), an imitation\\nlearning agent trained on 105 expert trajectories for each task type5.\\nWebShop\\nCan ReAct also interact with noisy real-world language environments for practical\\napplications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping\\nwebsite environment with 1.18M real-world products and 12k human instructions. Unlike ALFWorld,\\nWebshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions,\\nand options crawled from Amazon), and requires an agent to purchase a product based on a user\\ninstruction (e.g. “I am looking for a nightstand with drawers. It should have a nickel ﬁnish, and\\npriced lower than $140”) through web interactions (e.g. search “nightstand drawers”, choose buttons\\nsuch as “color: modern-nickel-white” or “back to search”). This task is evaluated by average score\\n(percentage of desired attributes covered by the chosen product averaged across all episodes) and\\nsuccess rate (percentage of episodes where the chosen product satisﬁes all requirements) on 500 test\\ninstructions. We formulate Act prompts with actions to search, choose product, choose options,\\nand buy, with ReAct prompts additionally reasoning to determine what to explore, when to buy,\\nand what products options are relevant to the instruction. See Table 6 for an example prompt, and\\nTable 10 for model predictions in the Appendix. We compare to an imitation learning (IL) method\\n5Micheli & Fleuret (2021) ﬁnetuned a GPT-2 model on 3553 task instances and achieved a much improved\\nperformance than BUTLER, but it is trained on all task types, thus not included as a baseline.\\n7\\nPublished as a conference paper at ICLR 2023\\nMethod\\nPick\\nClean\\nHeat\\nCool\\nLook\\nPick 2\\nAll\\nAct (best of 6)\\n88\\n42\\n74\\n67\\n72\\n41\\n45\\nReAct (avg)\\n65\\n39\\n83\\n76\\n55\\n24\\n57\\nReAct (best of 6)\\n92\\n58\\n96\\n86\\n78\\n41\\n71\\nReAct-IM (avg)\\n55\\n59\\n60\\n55\\n23\\n24\\n48\\nReAct-IM (best of 6)\\n62\\n68\\n87\\n57\\n39\\n33\\n53\\nBUTLERg (best of 8)\\n33\\n26\\n70\\n76\\n17\\n12\\n22\\nBUTLER (best of 8)\\n46\\n39\\n74\\n100\\n22\\n24\\n37\\nTable 3: AlfWorld task-speciﬁc success rates (%). BUTLER and\\nBUTLERg results are from Table 4 of Shridhar et al. (2020b). All\\nmethods use greedy decoding, except that BUTLER uses beam search.\\nMethod\\nScore\\nSR\\nAct\\n62.3\\n30.1\\nReAct\\n66.6\\n40.0\\nIL\\n59.9\\n29.1\\nIL+RL\\n62.4\\n28.7\\nHuman\\n82.1\\n59.6\\nExpert\\nTable 4: Score and suc-\\ncess rate (SR) on Web-\\nshop. IL/IL+RL taken\\nfrom Yao et al. (2022).\\ntrained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL)\\nmethod additionally trained with 10,587 training instructions.\\nResults\\nReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On\\nALFWorld, the best ReAct trial achieves an average success rate of 71%, signiﬁcantly outperforming\\nthe best Act (45%) and BUTLER (37%) trials. In fact, even the worse ReAct trial (48%) beats\\nthe best trial of both methods. Moreover, the advantage of ReAct over Act is consistent across\\nsix controlled trials, with relative performance gain ranging from 33% to 90% and averaging 62%.\\nQualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals\\ninto smaller subgoals, or loses track of the current state of the environment. Example trajectories\\ncomparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2.\\nOn Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With\\nadditional sparse reasoning, ReAct achieves signiﬁcantly better performance, with an absolute 10%\\nimprovement over the previous best success rate. By checking examples, we ﬁnd that ReAct is more\\nlikely to identify instruction-relevant products and options by reasoning to bridge the gap between\\nnoisy observations and actions (e.g. “For ‘space-saving ottoman bench for living room’, the item\\nhas options ‘39x18x18inch’ and ‘blue’ and seems good to buy.”). However, existing methods are\\nstill far from the performance of expert humans (Table 4), who perform signiﬁcantly more product\\nexplorations and query re-formulations that are still challenging for prompting-based methods.\\nOn the value of internal reasoning vs. external feedback\\nTo our knowledge, ReAct is the ﬁrst\\ndemonstration of combined reasoning and action using an LLM applied to an interactive environment\\nwithin a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang\\net al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner\\nmonologue”. However, IM’s “inner monologue” is limited to observations of the environment\\nstate and what needs to be completed by the agent for the goal to be satisﬁed. In contrast, the\\nreasoning traces in ReAct for decision making is ﬂexible and sparse, allowing diverse reasoning\\ntypes (see Section 2) to be induced for different tasks.\\nTo demonstrate the differences between ReAct and IM, and to highlight the importance of internal\\nreasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought\\npattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially\\noutperforms IM-style prompting (ReAct-IM) (71 vs. 53 overall success rate), with consistent\\nadvantages on ﬁve out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakes\\nin identifying when subgoals were ﬁnished, or what the next subgoal should be, due to a lack of high-\\nlevel goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where\\nan item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning.\\nBoth shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in\\nAppendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example\\ntrajectory in Appendix D.2.3.\\n8\\nPublished as a conference paper at ICLR 2023\\n5\\nRELATED WORK\\nLanguage model for reasoning\\nPerhaps the most well-known work of using LLMs for reasoning\\nis Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their\\nown “thinking procedure” for problem solving. Several follow-up works have since been performed,\\nincluding least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shot-\\nCoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,\\n(Madaan & Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and\\nobserved that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other\\nwork has also been extended to more sophisticated reasoning architecture beyond simple prompting.\\nFor example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps\\nof “selection” and “inference”. STaR (Zelikman et al., 2022) bootstraps the reasoning process by\\nﬁnetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswell\\n& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated\\nLM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which ﬁnetunes a LM on\\nintermediate computation steps, also demonstrate improvement on multi-step computation problems.\\nIn contrast to these methods, ReAct performs more than just isolated, ﬁxed reasoning, and integrates\\nmodel actions and their corresponding observations into a coherent stream of inputs for the model to\\nreason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\\nLanguage model for decision making\\nThe strong capability of LLMs has enabled them to perform\\ntasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a\\npolicy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,\\n2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to\\ncomplicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not\\nexplicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for\\nreinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)\\nand Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl\\net al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly\\nconsider the reasoning procedure either, and also relies on expensive datasets and human feedback\\ncollections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the\\ndecision making process only requires language description of the reasoning procedure.6\\nLLMS have also been increasingly employed in interactive and embodied environments for planning\\nand decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)\\nand Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision\\nmaking. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which\\nis then reranked by an affordance model grounded on the visual environments for ﬁnal prediction.\\nInner Monologue made further improvements by adding the eponymous “inner monologue\", which is\\nimplemented as injected feedback from the environment. To our knowledge, Inner Monologue is the\\nﬁrst work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue\\nthat Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section 4. We\\nalso note that leveraging language as semantically-rich inputs in the process of interactive decision\\nmaking has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti\\net al., 2021; Huang et al., 2022a; Li et al., 2022). It is becoming more evident that with the help of\\nLLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and\\ndecision making. What is more, progress in LLMs has also inspired the development of versatile and\\ngeneralist agents like Reed et al. (2022).\\n6\\nCONCLUSION\\nWe have proposed ReAct – a simple yet effective method for synergizing reasoning and acting in\\nlarge language models. Through a diverse set of experiments on multi-hop question-answering, fact\\nchecking, and interactive decision-making tasks, we show that ReAct leads to superior performance\\nwith interpretable decision traces. Despite the simplicity of our method, complex tasks with large\\naction spaces require more demonstrations to learn well, which unfortunately can easily go beyond\\nthe input length limit of in-context learning. We explore the ﬁne-tuning approach on HotpotQA\\n6Human feedback can also be incorporated in a complementary manner but we leave it for future work.\\n9\\nPublished as a conference paper at ICLR 2023\\nwith initial promising results, but learning from more high-quality human annotations will be the\\ndesiderata to further improve the performance. Scaling up ReAct with multi-task training and\\ncombining it with complementary paradigms like reinforcement learning could result in stronger\\nagents that further unlock the potential of LLMs for more applications.\\nACKNOWLEDGMENTS\\nWe thank the support and feedback of many people from Google Brain team and Princeton NLP\\nGroup. This work was supported in part by the National Science Foundation under Grant No.\\n2107048. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are\\nthose of the author(s) and do not necessarily reﬂect the views of the National Science Foundation.\\nREPRODUCIBILITY STATEMENT\\nOur main experiments are done on PaLM (Chowdhery et al., 2022), which is not an openly accessible\\nmodel yet. To increase reproducibility, we have included all used prompts in Appendix C, additional\\nexperiments using GPT-3 (Brown et al., 2020) in Appendix A.1, and associated GPT-3 ReAct\\nprompting code at https://anonymous.4open.science/r/ReAct-2268/.\\nETHICS STATEMENT\\nReAct prompts large language models to generate more human interpretable, diagnosable, and\\ncontrollable task-solving trajectories than previous methods. However, hooking up a large language\\nmodel with an action space to interact with external environments (e.g. the web, physical environ-\\nments) has potential dangers, e.g. looking up inappropriate or private information, or taking harmful\\nactions in an environment. Our experiments minimize such risks by limiting the interactions to\\nspeciﬁc websites (Wikipedia or WebShop) that are free of private information, without any dangerous\\nactions in the action space design (i.e. models cannot really buy products on WebShop the research\\nbenchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing\\nmore extensive experiments in the future.\\nREFERENCES\\nJosh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita\\nChhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim\\nHarley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathew-\\nson, Soˇna Mokrá, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne,\\nDuncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence,\\n2020. URL https://arxiv.org/abs/2012.05672.\\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally\\nJesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka\\nRao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander\\nToshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and\\nAndy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL\\nhttps://arxiv.org/abs/2204.01691.\\nBen Alderson-Day and Charles Fernyhough.\\nInner speech: development, cognitive functions,\\nphenomenology, and neurobiology. Psychological bulletin, 141(5):931, 2015.\\nAlan Baddeley. Working memory. Science, 255(5044):556–559, 1992.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\\n10\\nPublished as a conference paper at ICLR 2023\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\\nAntonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022. URL\\nhttps://arxiv.org/abs/2208.14271.\\nAntonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large\\nlanguage models for interpretable logical reasoning, 2022. URL https://arxiv.org/abs/\\n2205.09712.\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pp. 3558–3567, Florence, Italy, July 2019. Association for Com-\\nputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/\\nP19-1346.\\nCharles Fernyhough. Vygotsky, luria, and the social brain. Self and social regulation: Social\\ninteraction and the development of social understanding and executive functions, pp. 56–79, 2010.\\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Mari-\\nbeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham,\\nJonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth\\nDathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green,\\nSoˇna Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel,\\nWilliam Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and\\nGeoffrey Irving.\\nImproving alignment of dialogue agents via targeted human judgements,\\n2022.\\nURL https://storage.googleapis.com/deepmind-media/DeepMind.\\ncom/Authors-Notes/sparrow/sparrow-final.pdf.\\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple\\nlanguage model for task-oriented dialogue. Advances in Neural Information Processing Systems,\\n33:20179–20191, 2020.\\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\\nplanners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207,\\n2022a.\\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022b.\\nSiddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. Lila: Language-informed\\nlatent actions. In CoRL, pp. 1379–1390, 2021. URL https://proceedings.mlr.press/\\nv164/karamcheti22a.html.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\\nlanguage models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-\\naugmented language models through few-shot prompting for open-domain question answering.\\narXiv preprint arXiv:2203.05115, 2022.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented genera-\\ntion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:\\n9459–9474, 2020.\\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An\\nHuang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba,\\nand Yuke Zhu. Pre-trained language models for interactive decision-making, 2022. URL https:\\n//arxiv.org/abs/2202.01771.\\n11\\nPublished as a conference paper at ICLR 2023\\nAleksandr Romanovich Luria. Ls vygotsky and the problem of localization of functions. Neuropsy-\\nchologia, 3(4):387–392, 1965.\\nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes\\ntwo to tango, 2022. URL https://arxiv.org/abs/2209.07686.\\nVincent Micheli and François Fleuret. Language models are few-shot butlers. arXiv preprint\\narXiv:2104.07972, 2021.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\\nGretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:\\nBrowser-assisted question-answering with human feedback, 2021. URL https://arxiv.\\norg/abs/2112.09332.\\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and\\nAugustus Odena. Show your work: Scratchpads for intermediate computation with language\\nmodels, 2021. URL https://arxiv.org/abs/2112.00114.\\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\\nEccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,\\nOriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https:\\n//arxiv.org/abs/2205.06175.\\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,\\nLuke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions\\nfor everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pp. 10740–10749, 2020a.\\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew\\nHausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv\\npreprint arXiv:2010.03768, 2020b.\\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston.\\nLanguage models that seek for knowledge: Modular search & generation for dialogue and prompt\\ncompletion. arXiv preprint arXiv:2203.13224, 2022a.\\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung,\\nMoya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman\\nGoyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. Blenderbot 3:\\na deployed conversational agent that continually learns to responsibly engage, 2022b. URL\\nhttps://arxiv.org/abs/2208.03188.\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale\\ndataset for fact extraction and veriﬁcation. arXiv preprint arXiv:1803.05355, 2018.\\nLev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky, 1:39–285, 1987.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,\\n2022a. URL https://arxiv.org/abs/2203.11171.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented\\nensembles in language models. arXiv preprint arXiv:2207.00747, 2022b.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\\narXiv:2201.11903, 2022.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,\\nand Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\\nanswering. arXiv preprint arXiv:1809.09600, 2018.\\n12\\nPublished as a conference paper at ICLR 2023\\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore:\\nLanguage models for action generation in text-based games. In Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Processing (EMNLP), pp. 8736–8754, Online, Novem-\\nber 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704.\\nURL https://aclanthology.org/2020.emnlp-main.704.\\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable\\nreal-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206,\\n2022.\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with\\nreasoning, 2022. URL https://arxiv.org/abs/2203.14465.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\\nOlivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in\\nlarge language models, 2022. URL https://arxiv.org/abs/2205.10625.\\nYunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information\\nseeking for open-domain question answering. arXiv preprint arXiv:2109.06747, 2021.\\n13\\nPublished as a conference paper at ICLR 2023\\nA\\nADDITIONAL RESULTS\\nA.1\\nGPT-3 EXPERIMENTS\\nPaLM-540B\\nGPT-3\\nHotpotQA (exact match)\\n29.4\\n30.8\\nALFWorld (success rate %)\\n70.9\\n78.4\\nTable 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding).\\nOn HotpotQA, we randomly sample a subset of 500 validation questions. On ALFWorld, we use all\\n134 unseen validation task instances, and use the best prompt set according to PaLM-540B.\\nWe run additional GPT-3 (Brown et al., 2020) experiments to conﬁrm ReAct prompting performance\\nis general across different large language models. As shown in Table 5, GPT-3 (text-davinci-002,\\ngreedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly\\nbecause it is ﬁnetuned with human instruction following. This indicates ReAct prompting is effective\\nacross different large language models on different tasks. The code for these experiments are at\\nhttps://react-lm.github.io/.\\nA.2\\nREACT OBTAINS UP-TO-DATE KNOWLEDGE ON HOTPOTQA\\n�������������\\n�������������\\n����������������������\\n������������������������� �\\n�������������������������� �\\n��������������������� �\\n���������������������� �\\n����������������� �������� �\\n�������������������������� �\\n��������������������\\n�������������\\n��������������\\n������������������������������ ��\\n���������������������������� �\\n�������������������������������\\n�����������������������\\n����������������������\\n����������������������� ��������� �\\n������������ ��\\n����������������������\\n�������������������������������� �\\n���������\\n����������������������������� �\\n�����������\\n����������������������������� �\\n����������������� �\\n����������������������\\n�������������������������\\n����������������������������������������������������������\\n�\\n���������������������������������������������������������\\n�\\n����������\\n������������������������������������������� �\\n����������������������������������������������������\\n�\\n������������������������������\\n����������������������������������������������������������\\n�\\n���������� �\\n���������������������������������������� �\\n�������������������������������� ������������������������� ���\\n��������������������������������������\\n����������������������������������������������������� �\\n�����������������������������������������������������\\n�\\n���������������������������������������������������������\\n�\\n����������������������\\n���������������������������������������������� �\\n���������������� �������������������������� ���������������� �\\n����������������\\n�����������������������������������������������������������\\n�\\n���������������������������������������\\n��������������������\\n�\\n���������������������������������������������������������\\n�\\n����������������������������������\\n���������������������\\n��������������\\n���������\\n�����������\\nFigure 4: Another example HotpotQA question, where the original label is outdated. Only ReAct is\\nable to obtain the up-to-date answer thanks to real-world web interaction plus reasoning.\\nDuring trajectory inspection, we also ﬁnd that sometimes ReAct does not agree with dataset labels as\\nthe labels themselves could be outdated. For example, as shown in Figure 4, the question asks about\\nthe size of a hotel, which increased from the HotpotQA construction time. While Standard and CoT\\ngive wrong answers due to hallucination, Act fails despite the access of real-world web interaction,\\ndue to a lack of reasoning to guide how to interact with the Internet for QA. Only ReAct is able to\\nretrieve up-to-date information from the Internet and provide a reasonable answer. Therefore, better\\nincorporation of reasoning abilities might beneﬁt recent Internet-augmented language models (Nakano\\net al., 2021; Lazaridou et al., 2022; Shuster et al., 2022a) for up-to-date task solving.\\nA.3\\nHUMAN-IN-THE-LOOP BEHAVIOR CORRECTION ON ALFWORLD\\nWe also explore human-in-the-loop interaction with ReAct, to allow a human to inspect and edit\\nReAct’s reasoning traces. Figure 5 shows that by simply removing a hallucinating sentence in Act\\n17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align\\nwith these human thought edits and succeed in the task. From a human perspective, solving such a\\ntask becomes signiﬁcantly easier, from typing tens of actions to only editing a couple of thoughts,\\nwhich enables new forms of human-machine collaboration. We note that such a policy edit on-the-go\\n14\\nPublished as a conference paper at ICLR 2023\\n����������\\n������\\n���������������������������������������������������������������\\n�\\n�������������������������������� �\\n�����������������������������������������������������������������\\n�\\n��������������������������������������������������������������\\n��\\n������������������������������������� �\\n�����������������������������������������������������\\n�\\n������\\n����������������������������������� �\\n������������������������������������������������ �\\n�����������������������������������������������������������������\\n�\\n�����������������������������������������������������������\\n��\\n���������������������� �\\n�������������������������������������������������������\\n�\\n���������������������������������������������������������������\\n�\\n����������� �\\n������������������������������������� �\\n������������������������\\n������������������������������\\n������\\n���������������������������������������������������������������������\\n�\\n�������������������������� �\\n���������������������������������������������������������������������������\\n��\\n���������������������������������������������������\\n�\\n������������������������������������� �\\n�����������������������������������������������������\\n�\\n������\\n����������������������������������� �\\n������������������������������������������������ �\\n�������������������������������������������������������������������������\\n�\\n������������������� ������������������������������������������������\\n�\\n���������������������������������������������������������������\\n��\\n����������������������� �\\n��������������������������������������������������������������������������\\n�\\n�������������������������\\n������������������������������������������������������������������������\\n����\\n�������������������������������������� �\\n������������������������������������������������������\\n�\\n��������������������������������������������������������������������������\\n������������� �\\n��������������������\\n��������������������������������������������������������������������������\\n�\\n��������\\n�����������������������������������\\n������������������������������������������������\\n�\\n������������������������������������������������������������\\n�\\n�������������������������������������������������������������\\n�\\n�������������������������������������������������������������\\n�\\n����������������������������������������������\\n�\\n������������������������������������������\\n��������\\nFigure 5: A human-in-the-loop behavior correction example with ReAct in AlfWorld. (a) ReAct\\ntrajectory fails due to a hallucinating thought (Act 17). (b) By a human simply editing two thoughts\\n(Act 17, 23), the ReAct trajectory produces desirable reasoning traces and actions and succeeds.\\nis difﬁcult for Act and previous RL methods, as a human cannot change the model parameters, and\\nchanging a few actions might not edit the rest of the model behavior. This paradigm is also more than\\nhuman dialogue to update the goal or subgoal as in Huang et al. (2022b) — while editing ReAct\\nthoughts can do these, it can also modify the model’s internal belief, reasoning styles, or anything the\\nﬂexible thought space supports, for better task solving. We believe this is an exciting direction for\\nhuman alignment and leave more systematic study as future work.\\nB\\nEXPERIMENT DETAILS\\nB.1\\nHOTPOTQA FINETUNING DETAILS\\nFor all ﬁnetuning we use a batch size of 64. On PaLM-8B, we ﬁnetune ReAct and Act methods\\nfor 4, 000 steps and Standard and CoT methods for 2, 000 steps. On PaLM-62B, we ﬁnetune\\nReAct and Act methods for 4, 000 steps and Standard and CoT methods for 1, 000 steps. We\\nﬁnd ReAct and Act methods generally beneﬁt from more training steps (and more training data),\\nwhile Standard and CoT methods degrade soon after ﬁnetuning.\\nB.2\\nALFWORLD IM-STYLE DETAILS\\nFor the IM-style ablation, the same expert trajectories used in ReAct are reannotated with dense\\nexternal feedback thoughts within these trajectories, that limit ReAct-IM to only think about (1)\\ndecomposing the current goal and (2) the current subgoal that needs to be completed. In particular,\\nReAct-IM lacks thoughts that (1) determine when a subgoal is completed (2) determine what the\\nnext subgoal should be (3) inducing the LLM to refer to its internal pretraining knowledge to identify\\nwhere items can be within the environment.\\n15\\nPublished as a conference paper at ICLR 2023\\nC\\nPROMPTS\\nC.1\\nHOTPOTQA\\nHotpot QA Prompts\\nOriginal\\nQuestion\\nWhat is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nAnswer\\n1,800 to 7,000 ft\\nQuestion\\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nAnswer\\nRichard Nixon\\nQuestion\\nWhich documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nAnswer\\nThe Saimaa Gesture\\nQuestion\\nWhat profession does Nicholas Ray and Elia Kazan have in common?\\nAnswer\\ndirector, screenwriter, actor\\nQuestion\\nWhich magazine was started first Arthur’s Magazine or First for Women?\\nAnswer\\nArthur’s Magazine\\nQuestion\\nWere Pavel Urysohn and Leonid Levin known for the same type of work?\\nAnswer\\nYes\\nAct\\nQuestion\\nWhat is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nAction 1\\nSearch[Colorado orogeny]\\nObservation 1\\nThe Colorado orogeny was an episode of mountain building (an orogeny) in\\nColorado and surrounding areas.\\nAction 2\\nLookup[eastern sector]\\nObservation 2\\n(Result 1 / 1) The eastern sector extends into the High Plains and is called\\nthe Central Plains orogeny.\\nAction 3\\nSearch[High Plains]\\nObservation 3\\nHigh Plains refers to one of two distinct land regions:\\nAction 4\\nSearch[High Plains (United States)]\\nObservation 4\\nThe High Plains are a subregion of the Great Plains.\\nFrom east to west, the\\nHigh Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\\nm).[3]\\nAction 5\\nFinish[1,800 to 7,000 ft]\\nQuestion\\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nAction 1\\nSearch[Milhouse]\\nObservation 1\\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated\\ntelevision series The Simpsons voiced by Pamela Hayden and created by Matt\\nGroening.\\nAction 2\\nLookup[named after]\\nObservation 2\\n(Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose\\nmiddle name was Milhous.\\nAction 3\\nFinish[Richard Nixon]\\nQuestion\\nWhich documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nAction 1\\nSearch[Adam Clayton Powell]\\nObservation 1\\nCould not find [Adam Clayton Powell].\\nSimilar:\\n[’Adam Clayton Powell\\nIII’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr.\\nState Office\\nBuilding’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell\\n(film)’, ’Giancarlo Esposito’].\\nAction 2\\nSearch[Adam Clayton Powell (film)]\\nObservation 2\\nAdam Clayton Powell is a 1989 American documentary film directed by\\nRichard Kilberg.\\nThe film is about the rise and fall of influential\\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired\\nas part of the PBS series The American Experience.\\nAction 3\\nFinish[The Saimaa Gesture]\\nQuestion\\nWhat profession does Nicholas Ray and Elia Kazan have in common?\\nAction 1\\nSearch[Nicholas Ray]\\nObservation 1\\nNicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,\\n1979) was an American film director, screenwriter, and actor best known for\\nthe 1955 film Rebel Without a Cause.\\nAction 2\\nSearch[Elia Kazan]\\nObservation 2\\nElia Kazan was an American film and theatre director, producer, screenwriter\\nand actor.\\nContinued on next page\\n16\\nPublished as a conference paper at ICLR 2023\\nHotpot QA Prompts – Continued from previous page\\nAction 3\\nFinish[director, screenwriter, actor]\\nQuestion\\nWhich magazine was started first Arthur’s Magazine or First for Women?\\nAction 1\\nSearch[Arthur’s Magazine]\\nObservation 1\\nArthur’s Magazine (1844-1846) was an American literary periodical published\\nin Philadelphia in the 19th century.\\nAction 2\\nSearch[First for Women]\\nObservation 2\\nFirst for Women is a woman’s magazine published by Bauer Media Group in the\\nUSA.[1] The magazine was started in 1989.\\nAction 3\\nFinish[Arthur’s Magazine]\\nQuestion\\nWere Pavel Urysohn and Leonid Levin known for the same type of work?\\nAction 1\\nSearch[Pavel Urysohn]\\nObservation 1\\nPavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet\\nmathematician who is best known for his contributions in dimension theory.\\nAction 2\\nSearch[Leonid Levin]\\nObservation 2\\nLeonid Anatolievich Levin is a Soviet-American mathematician and computer\\nscientist.\\nAction 3\\nFinish[yes]\\nCoT\\nQuestion\\nWhat is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nThought\\nLet’s think step by step.\\nThe eastern sector of Colorado orogeny extends\\ninto the High Plains.\\nHigh Plains rise in elevation from around 1,800 to\\n7,000 ft, so the answer is 1,800 to 7,000 ft.\\nAnswer\\n1,800 to 7,000 ft\\nQuestion\\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nThought\\nLet’s think step by step.\\nMilhouse was named after U.S. president Richard\\nNixon, so the answer is Richard Nixon.\\nAnswer\\nRichard Nixon\\nQuestion\\nWhich documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nThought\\nLet’s think step by step.\\nAdam Clayton Powell (film) is a documentary about\\nan African-American politician, not Finnish rock groups.\\nSo the documentary\\nabout Finnish rock groups must instead be The Saimaa Gesture.\\nAnswer\\nThe Saimaa Gesture\\nQuestion\\nWhat profession does Nicholas Ray and Elia Kazan have in common?\\nThought\\nLet’s think step by step.\\nProfessions of Nicholas Ray are director,\\nscreenwriter, and actor.\\nProfessions of Elia Kazan are director, producer,\\nscreenwriter, and actor.\\nSo profession Nicholas Ray and Elia Kazan have in\\ncommon is director, screenwriter, and actor.\\nAnswer\\ndirector, screenwriter, actor\\nQuestion\\nWhich magazine was started first Arthur’s Magazine or First for Women?\\nThought\\nLet’s think step by step.\\nArthur’s Magazine was started in 1844.\\nFirst\\nfor Women was started in 1989.\\n1844 (Arthur’s Magazine) < 1989 (First for\\nWomen), so Arthur’s Magazine was started first.\\nAnswer\\nArthur’s Magazine\\nQuestion\\nWere Pavel Urysohn and Leonid Levin known for the same type of work?\\nThought\\nLet’s think step by step.\\nPavel Urysohn is a mathematician.\\nLeonid Levin\\nis a mathematician and computer scientist.\\nSo Pavel Urysohn and Leonid\\nLevin have the same type of work.\\nAnswer\\nYes\\nReAct\\nQuestion\\nWhat is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nThought 1\\nI need to search Colorado orogeny, find the area that the eastern sector\\nof the Colorado orogeny extends into, then find the elevation range of the\\narea.\\nAction 1\\nSearch[Colorado orogeny]\\nObservation 1\\nThe Colorado orogeny was an episode of mountain building (an orogeny) in\\nColorado and surrounding areas.\\nThought 2\\nIt does not mention the eastern sector.\\nSo I need to look up eastern\\nsector.\\nAction 2\\nLookup[eastern sector]\\nObservation 2\\n(Result 1 / 1) The eastern sector extends into the High Plains and is called\\nthe Central Plains orogeny.\\nThought 3\\nThe eastern sector of Colorado orogeny extends into the High Plains.\\nSo I\\nneed to search High Plains and find its elevation range.\\nAction 3\\nSearch[High Plains]\\nObservation 3\\nHigh Plains refers to one of two distinct land regions\\nThought 4\\nI need to instead search High Plains (United States).\\nAction 4\\nSearch[High Plains (United States)]\\nContinued on next page\\n17\\nPublished as a conference paper at ICLR 2023\\nHotpot QA Prompts – Continued from previous page\\nObservation 4\\nThe High Plains are a subregion of the Great Plains.\\nFrom east to west, the\\nHigh Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\\nm).[3]\\nThought 5\\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer\\nis 1,800 to 7,000 ft.\\nAction 5\\nFinish[1,800 to 7,000 ft]\\nQuestion\\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nThought 1\\nThe question simplifies to \"The Simpsons\" character Milhouse is named after\\nwho.\\nI only need to search Milhouse and find who it is named after.\\nAction 1\\nSearch[Milhouse]\\nObservation 1\\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated\\ntelevision series The Simpsons voiced by Pamela Hayden and created by Matt\\nGroening.\\nThought 2\\nThe paragraph does not tell who Milhouse is named after, maybe I can look up\\n\"named after\".\\nAction 2\\nLookup[named after]\\nObservation 2\\n(Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose\\nmiddle name was Milhous.\\nThought 3\\nMilhouse was named after U.S. president Richard Nixon, so the answer is\\nRichard Nixon.\\nAction 3\\nFinish[Richard Nixon]\\nQuestion\\nWhich documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nThought 1\\nI need to search Adam Clayton Powell and The Saimaa Gesture, and find which\\ndocumentary is about Finnish rock groups.\\nAction 1\\nSearch[Adam Clayton Powell]\\nObservation 1\\nCould not find [Adam Clayton Powell].\\nSimilar:\\n[’Adam Clayton Powell\\nIII’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr.\\nState Office\\nBuilding’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell\\n(film)’, ’Giancarlo Esposito’].\\nThought 2\\nTo find the documentary, I can search Adam Clayton Powell (film).\\nAction 2\\nSearch[Adam Clayton Powell (film)]\\nObservation 2\\nAdam Clayton Powell is a 1989 American documentary film directed by\\nRichard Kilberg.\\nThe film is about the rise and fall of influential\\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired\\nas part of the PBS series The American Experience.\\nThought 3\\nAdam Clayton Powell (film) is a documentary about an African-American\\npolitician, not Finnish rock groups.\\nSo the documentary about Finnish rock\\ngroups must instead be The Saimaa Gesture.\\nAction 3\\nFinish[The Saimaa Gesture]\\nQuestion\\nWhat profession does Nicholas Ray and Elia Kazan have in common?\\nThought 1\\nI need to search Nicholas Ray and Elia Kazan, find their professions, then\\nfind the profession they have in common.\\nAction 1\\nSearch[Nicholas Ray]\\nObservation 1\\nNicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,\\n1979) was an American film director, screenwriter, and actor best known for\\nthe 1955 film Rebel Without a Cause.\\nThought 2\\nProfessions of Nicholas Ray are director, screenwriter, and actor.\\nI need\\nto search Elia Kazan next and find his professions.\\nAction 2\\nSearch[Elia Kazan]\\nObservation 2\\nElia Kazan was an American film and theatre director, producer, screenwriter\\nand actor.\\nThought 3\\nProfessions of Elia Kazan are director, producer, screenwriter, and actor.\\nSo profession Nicholas Ray and Elia Kazan have in common is director,\\nscreenwriter, and actor.\\nAction 3\\nFinish[director, screenwriter, actor]\\nQuestion\\nWhich magazine was started first Arthur’s Magazine or First for Women?\\nThought 1\\nI need to search Arthur’s Magazine and First for Women, and find which was\\nstarted first.\\nAction 1\\nSearch[Arthur’s Magazine]\\nObservation 1\\nArthur’s Magazine (1844-1846) was an American literary periodical published\\nin Philadelphia in the 19th century.\\nThought 2\\nArthur’s Magazine was started in 1844.\\nI need to search First for Women\\nnext.\\nAction 2\\nSearch[First for Women]\\nObservation 2\\nFirst for Women is a woman’s magazine published by Bauer Media Group in the\\nUSA.[1] The magazine was started in 1989.\\nThought 3\\nFirst for Women was started in 1989.\\n1844 (Arthur’s Magazine) < 1989 (First\\nfor Women), so Arthur’s Magazine was started first.\\nAction 3\\nFinish[Arthur’s Magazine]\\nQuestion\\nWere Pavel Urysohn and Leonid Levin known for the same type of work?\\nThought 1\\nI need to search Pavel Urysohn and Leonid Levin, find their types of work,\\nthen find if they are the same.\\nAction 1\\nSearch[Pavel Urysohn]\\nContinued on next page\\n18\\nPublished as a conference paper at ICLR 2023\\nHotpot QA Prompts – Continued from previous page\\nObservation 1\\nPavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet\\nmathematician who is best known for his contributions in dimension theory.\\nThought 2\\nPavel Urysohn is a mathematician.\\nI need to search Leonid Levin next and\\nfind its type of work.\\nAction 2\\nSearch[Leonid Levin]\\nObservation 2\\nLeonid Anatolievich Levin is a Soviet-American mathematician and computer\\nscientist.\\nThought 3\\nLeonid Levin is a mathematician and computer scientist.\\nSo Pavel Urysohn\\nand Leonid Levin have the same type of work.\\nAction 3\\nFinish[yes]\\n19\\nPublished as a conference paper at ICLR 2023\\nC.2\\nFEVER\\nFEVER Prompts\\nOriginal\\nDetermine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim\\nNikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nAnswer\\nSUPPORTS\\nClaim\\nStranger Things is set in Bloomington, Indiana.\\nAnswer\\nREFUTES\\nClaim\\nBeautiful reached number two on the Billboard Hot 100 in 2003.\\nAnswer\\nNOT ENOUGH INFO\\nAct\\nDetermine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim\\nNikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nAction2\\nSearch[Nikolaj Coster-Waldau]\\nObservation 1\\nNikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and\\nproducer.\\nHe graduated from the Danish National School of Performing Arts\\nin Copenhagen in 1993,[1] and had his breakthrough role in Denmark with\\nthe film Nightwatch (1994).\\nHe played Jaime Lannister in the HBO fantasy\\ndrama series Game of Thrones, for which he received two Primetime Emmy\\nAward nominations for Outstanding Supporting Actor in a Drama Series..\\nCoster-Waldau has appeared in numerous films in his native Denmark and\\nScandinavia, including Headhunters (2011) and A Thousand Times Good Night\\n(2013).\\nIn the U.S, his debut film role was in the war film Black Hawk Down\\n(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a\\ndetective in the short-lived Fox television series New Amsterdam (2008), and\\nappeared in the 2009 Fox television film Virtuality, originally intended as\\na pilot.\\nAction 2\\nFinish[SUPPORTS]\\nClaim\\nStranger Things is set in Bloomington, Indiana.\\nAction 1\\nSearch[Stranger Things]\\nObservation 1\\nStranger Things is an American science fiction horror drama television\\nseries created by the Duffer Brothers.\\nSet in the 1980s, primarily in\\nthe fictional town of Hawkins, Indiana, the series centers on a number of\\nmysteries and supernatural events occurring around the town and their impact\\non an ensemble of child and adult characters.\\nAction 2\\nFinish[REFUTES]\\nClaim\\nBeautiful reached number two on the Billboard Hot 100 in 2003.\\nAction 1\\nSearch[Beautiful]\\nObservation 1\\nCould not find [Beautiful].\\nSimilar:\\n[’Beautiful’, ’Beautiful, Beautiful’,\\n’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is\\nBeautiful’].\\nAction 2\\nSearch[Beautiful (Christina Aguilera song)\\nObservation 2\\n\"Beautiful\" is a song recorded by American singer Christina Aguilera for her\\nfourth studio album, Stripped (2002).\\nAction 3\\nLookup[Billboard Hot 100]\\nObservation 3\\n(Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the\\nUnited States, where it was certified Gold for 500,000 units shipped.\\nAction 4\\nFinish[NOT ENOUGH INFO]\\nCoT\\nDetermine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim\\nNikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nThought\\nNikolaj William Coster-Waldau appeared in the 2009 Fox television film\\nVirtuality, so he has worked with the Fox Broadcasting Company.\\nAnswer\\nSUPPORTS\\nClaim\\nStranger Things is set in Bloomington, Indiana.\\nThought\\nStranger Things is in the fictional town of Hawkins, Indiana, not in\\nBloomington, Indiana.\\nAnswer\\nREFUTES\\nContinued on next page\\n20\\nPublished as a conference paper at ICLR 2023\\nFever Prompts – Continued from previous page\\nClaim\\nBeautiful reached number two on the Billboard Hot 100 in 2003.\\nThought\\nThe song peaked at number two on the Billboard Hot 100 in the United States,\\nbut not sure if it was in 2003.\\nAnswer\\nNOT ENOUGH INFO\\nReAct\\nDetermine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim\\nNikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nThought 1\\nI need to search Nikolaj Coster-Waldau and find if he has worked with the\\nFox Broadcasting Company.\\nAction 1\\nSearch[Nikolaj Coster-Waldau]\\nObservation 1\\nNikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and\\nproducer.\\nHe graduated from the Danish National School of Performing Arts\\nin Copenhagen in 1993,[1] and had his breakthrough role in Denmark with\\nthe film Nightwatch (1994).\\nHe played Jaime Lannister in the HBO fantasy\\ndrama series Game of Thrones, for which he received two Primetime Emmy\\nAward nominations for Outstanding Supporting Actor in a Drama Series..\\nCoster-Waldau has appeared in numerous films in his native Denmark and\\nScandinavia, including Headhunters (2011) and A Thousand Times Good Night\\n(2013).\\nIn the U.S, his debut film role was in the war film Black Hawk Down\\n(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a\\ndetective in the short-lived Fox television series New Amsterdam (2008), and\\nappeared in the 2009 Fox television film Virtuality, originally intended as\\na pilot.\\nThought 2\\nBecause he \"appeared in the 2009 Fox television film Virtuality\", he should\\nhave worked with the Fox Broadcasting Company.\\nAction 2\\nFinish[SUPPORTS]\\nClaim\\nStranger Things is set in Bloomington, Indiana.\\nThought 1\\nI should search for Stranger Things, and see if it is set in Bloomington,\\nIndiana.\\nAction 1\\nSearch[Stranger Things]\\nObservation 1\\nStranger Things is an American science fiction horror drama television\\nseries created by the Duffer Brothers.\\nSet in the 1980s, primarily in\\nthe fictional town of Hawkins, Indiana, the series centers on a number of\\nmysteries and supernatural events occurring around the town and their impact\\non an ensemble of child and adult characters.\\nThought 2\\nThe observation says that it is set in a \"fictional town of Hawkins,\\nIndiana\", so it is not set in Bloomington.\\nAction 2\\nFinish[REFUTES]\\nClaim\\nBeautiful reached number two on the Billboard Hot 100 in 2003.\\nThought 1\\nI need to search the song Beautiful and find if it reached number two on the\\nBillboard Hot 100 in 2003.\\nAction 1\\nSearch[Beautiful]\\nObservation 1\\nCould not find [Beautiful].\\nSimilar:\\n[’Beautiful’, ’Beautiful, Beautiful’,\\n’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is\\nBeautiful’].\\nThought 2\\nFrom suggestions, I should search \"Beautiful (Christina Aguilera song)\" to\\nfind the song.\\nAction 2\\nSearch[Beautiful (Christina Aguilera song)]\\nObservation 2\\n\"Beautiful\" is a song recorded by American singer Christina Aguilera for her\\nfourth studio album, Stripped (2002).\\nThought 3\\nIt does not mention Billboard, so I need to look up \"Billboard Hot 100\" to\\nfind if it reached number two on it in 2003.\\nAction 3\\nLookup[Billboard Hot 100]\\nObservation 3\\n(Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the\\nUnited States, where it was certified Gold for 500,000 units shipped.\\nThought 4\\nIt only says the song peaked at number two on the Billboard Hot 100, but not\\nif it was in 2003.\\nI am not sure if this claim is true or not.\\nAction 4\\nFinish[NOT ENOUGH INFO]\\n21\\nPublished as a conference paper at ICLR 2023\\nC.3\\nWEBSHOP\\nTable 6: Prompts used for the Webshop experiments in Section 4. We use the prompt on the left for\\nAct, and use the prompt on the right (add reasoning) for ReAct.\\nInstruction: i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars\\nAct\\nReAct\\nAction: search\\n�\\n3 ounce bright citrus deodorant sensitive skin\\n�\\nObservation:\\n�\\nBack to Search\\n�\\nPage 1 (Total results: 50)\\n�\\nNext\\n�\\n�\\nB078GWRC1J\\n�\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99\\n�\\nB078GTKVXY\\n�\\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99\\n�\\nB08KBVJ4XN\\n�\\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men\\n, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar\\n& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,\\n2.7 oz, 2-Pack)\\n$15.95\\nAction: click\\n�\\nB078GWRC1J\\n�\\nObservation:\\n�\\nBack to Search\\n�\\n�\\nPrev\\n�\\nscent [assorted scents][bright citrus][calming lavender][ginger\\n[fresh] simply non-scents]\\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2\\n-pack)]\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\nPrice: $10.99\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction: click[bright citrus]\\nObservation: You have clicked bright citrus.\\nAction: click[3 ounce (pack of 1)]\\nObservation: You have clicked 3 ounce (pack of 1).\\nAction: click[Buy Now]\\nAction: search\\n�\\n3 ounce bright citrus deodorant sensitive skin\\nObservation:\\n�\\nBack to Search\\n�\\nPage 1 (Total results: 50)\\n�\\nNext\\n�\\n�\\nB078GWRC1J\\n�\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99\\n�\\nB078GTKVXY\\n�\\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99\\n�\\nB08KBVJ4XN\\n�\\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men\\n, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar\\n& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,\\n2.7 oz, 2-Pack)\\n$15.95\\nAction: think[B078GWRC1J and B078GTKVXY are bright citrus\\ndeodorant less then 50 dollars. I can check B078GWRC1J ﬁrst.\\nObservation: OK.\\nAction: click\\n�\\nB078GWRC1J\\n�\\nObservation:\\n�\\nBack to Search\\n�\\n�\\nPrev\\n�\\nscent [assorted scents][bright citrus][calming lavender][ginger\\n[fresh] simply non-scents]\\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2\\n-pack)]\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\nPrice: $10.99\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction: think[For 3 ounce bottle of bright citrus deodorant\\nfor sensitive skin, the item has options ’bright citrus’\\nand ’3 ounce (pack of 1)’ and seems good to buy.]\\nObservation: OK.\\nAction: click[bright citrus]\\nObservation: You have clicked bright citrus.\\nAction: click[3 ounce (pack of 1)]\\nObservation: You have clicked 3 ounce (pack of 1).\\nAction: click[Buy Now]\\n22\\nPublished as a conference paper at ICLR 2023\\nC.4\\nALFWORLD\\nTable 7: An Act prompt on the ALFWorld clean task. No thoughts are provided.\\nYou are in the middle of a room.\\nLooking quickly around you, you see a cabinet\\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\\nYour task is to:\\nput a clean lettuce in diningtable.\\n> go to fridge 1\\nThe fridge 1 is closed.\\n> open fridge 1\\nYou open the fridge 1.\\nThe fridge 1 is open.\\nIn it, you see a cup 3, a egg 2, a\\npotato 3, and a potato 2.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\\n> take lettuce 1 from diningtable 1\\nYou pick up the lettuce 1 from the diningtable 1.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\\n> clean lettuce 1 with sinkbasin 1\\nYou clean the lettuce 1 using the sinkbasin 1.\\n> think:\\nNow I clean a lettuce (1).\\nNext, I need to put it in/on diningtable 1.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\\nspatula 3, a tomato 2, and a tomato 1.\\n> put lettuce 1 in/on diningtable 1\\nYou put the lettuce 1 in/on the diningtable 1.\\n23\\nPublished as a conference paper at ICLR 2023\\nTable 8: An ReAct prompt on the ALFWorld clean task.\\nYou are in the middle of a room.\\nLooking quickly around you, you see a cabinet\\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\\nYour task is to:\\nput a clean lettuce in diningtable.\\n> think:\\nTo solve the task, I need to find and take a lettuce, then clean it with\\nsinkbasin, then put it in diningtable.\\nOK.\\n> think:\\nFirst I need to find a lettuce.\\nA lettuce is more likely to appear in\\nfridge (1), diningtable (1), sinkbasin (1), stoveburner (1-3), cabinet (1-13).\\nI\\ncan check one by one, starting with fridge 1.\\nOK.\\n> go to fridge 1\\nThe fridge 1 is closed.\\n> open fridge 1\\nYou open the fridge 1.\\nThe fridge 1 is open.\\nIn it, you see a cup 3, a egg 2, a\\npotato 3, and a potato 2.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\\n> think:\\nNow I find a lettuce (1).\\nNext, I need to take it.\\nOK.\\n> take lettuce 1 from diningtable 1\\nYou pick up the lettuce 1 from the diningtable 1.\\n> think:\\nNow I take a lettuce (1).\\nNext, I need to go to sinkbasin (1) and clean\\nit.\\nOK.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\\n> clean lettuce 1 with sinkbasin 1\\nYou clean the lettuce 1 using the sinkbasin 1.\\n> think:\\nNow I clean a lettuce (1).\\nNext, I need to put it in/on diningtable 1.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\\nspatula 3, a tomato 2, and a tomato 1.\\n> put lettuce 1 in/on diningtable 1\\nYou put the lettuce 1 in/on the diningtable 1.\\n24\\nPublished as a conference paper at ICLR 2023\\nTable 9: An ReAct-IM prompt on the ALFWorld clean task.\\nYou are in the middle of a room.\\nLooking quickly around you, you see a cabinet\\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\\nYour task is to:\\nput a clean lettuce in diningtable.\\n> think:\\nTo solve the task, I need to find and take a lettuce, then clean it with\\nsinkbasin, then put it in diningtable.\\nOK.\\n> think:\\nFirst I need to find a lettuce.\\nOK.\\n> go to fridge 1\\nThe fridge 1 is closed.\\n> think:\\nFirst I need to find a lettuce.\\nOK.\\n> open fridge 1\\nYou open the fridge 1.\\nThe fridge 1 is open.\\nIn it, you see a cup 3, a egg 2, a\\npotato 3, and a potato 2.\\n> think:\\nFirst I need to find a lettuce.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\\n> think:\\nFirst I need to find a lettuce.\\nOK.\\n> take lettuce 1 from diningtable 1\\nYou pick up the lettuce 1 from the diningtable 1.\\n> think:\\nI need to clean this lettuce (1) using sinkbasin 1.\\nOK.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\\n> think:\\nI need to clean this lettuce (1) using sinkbasin 1.\\nOK.\\n> clean lettuce 1 with sinkbasin 1\\nYou clean the lettuce 1 using the sinkbasin 1.\\n> think:\\nI need to put this cleaned lettuce (1) in/on diningtable 1.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\\nspatula 3, a tomato 2, and a tomato 1.\\n> think:\\nI need to put this cleaned lettuce (1) in/on diningtable 1.\\nOK.\\n> put lettuce 1 in/on diningtable 1\\nYou put the lettuce 1 in/on the diningtable 1.\\nD\\nTRAJECTORIES\\nD.1\\nFEVER TRAJECTORIES\\nWe provide some trajectories produced by ReAct, CoTand Acton several randomly selected exam-\\nples from the FEVER dev set below. Search results are omitted to space.\\n25\\nPublished as a conference paper at ICLR 2023\\nExample 2491 (gt: REFUTES)\\nReAct\\nClaim:\\nBermuda Triangle is a loosely-defined region in the Pacific Ocean.\\nThought 1:\\nI need to search Bermuda Triangle and find if it is in the Pacific Ocean.\\nAction 1:\\nSearch[Bermuda Triangle]\\nObservation 1:\\nReportedly haunted locations...\\nThought 2:\\nThe observation says that it is in the western part of the North Atlantic\\nOcean, so it is not in the Pacific Ocean.\\nAction 2:\\nFinish[REFUTES]\\nObservation 2:\\nEpisode finished\\nAct\\nClaim:\\nBermuda Triangle is a loosely-defined region in the Pacific Ocean.\\nAction 1\\n:\\nSearch[Bermuda Triangle]\\nObservation 1\\n:\\nReportedly haunted locations:...\\nAction 2:\\nFinish[REFUTES]\\nObservation 2:\\nEpisode finished\\nCoT\\nClaim:\\nBermuda Triangle is a loosely-defined region in the Pacific Ocean.\\nThought:\\nLet’s think step by step.\\nThe Bermuda Triangle is a loosely-defined region\\nin the Atlantic Ocean.\\nThe Pacific Ocean is not the Atlantic Ocean.\\nAnswer:\\nREFUTES\\nExample 5908 (gt: SUPPORTS)\\nReAct\\nClaim:\\nPrincess Mononoke is a film.\\nThought 1:\\nI need to search Princess Mononoke and find if it is a film.\\nAction 1:\\nSearch[Princess Mononoke]\\nObservation 1:\\nPrincess Mononoke ...\\nThought 2:\\nFrom the observation, it says that Princess Mononoke is a film.\\nAction 2:\\nFinish[SUPPORTS]\\nObservation 2:\\nEpisode finished\\nAct\\nClaim:\\nPrincess Mononoke is a film.\\nAction 1:\\nSearch[Princess Mononoke]\\nObservation 1:\\nPrincess Mononoke ...\\nAction 2:\\nFinish[SUPPORTS]\\nObservation 2:\\nEpisode finished\\nCoT\\nClaim:\\nPrincess Mononoke is a film.\\nThought:\\nLet’s think step by step.\\nPrincess Mononoke is a movie, and movies are\\nfilms.\\nAnswer:\\nSUPPORTS\\nExample 1951 (gt: REFUTES)\\nReAct\\nClaim:\\nSoyuz was part of the American space program.\\nThought 1:\\nI need to search Soyuz and find if it was part of the American space\\nprogram.\\nAction 1:\\nSearch[Soyuz]\\nObservation 1:\\nSoyuz is a transliteration...\\nThought 2:\\nIt does not mention the American space program, so I need to search\\n\"American space program\" to find if it was part of it.\\nAction 2:\\nSearch[American space program]\\nObservation 2:\\nThe space policy...\\nThought 3:\\nIt does not mention Soyuz, so I am not sure if this claim is true or not.\\nAction 3:\\nFinish[NOT ENOUGH INFO]\\nObservation 3:\\nEpisode finished\\nAct\\nClaim:\\nSoyuz was part of the American space program.\\nAction 1:\\nSearch[Soyuz]\\nObservation 1:\\nSoyuz is a transliteration...\\nAction 2:\\nFinish[NOT ENOUGH INFO]\\nObservation 2:\\nEpisode finished\\nCoT\\nClaim:\\nSoyuz was part of the American space program.\\nThought:\\nLet’s think step by step.\\nSoyuz is a Russian spacecraft.\\nThe American\\nspace program is NASA. NASA and Russia have worked together on the\\nInternational Space Station.\\nAnswer:\\nSUPPORTS\\n26\\nPublished as a conference paper at ICLR 2023\\nExample 3208 (gt: REFUTES)\\nReAct\\nClaim:\\nReign Over Me is an American film made in 2010.\\nThought 1:\\nI need to search for Reign Over Me and find if it is an American film made\\nin 2010.\\nAction 1:\\nSearch[Reign Over Me]\\nObservation 1:\\nReign Over Me ...\\nThought 2:\\nThe observation says that it is an American film made in 2007, so it is not\\nmade in 2010.\\nAction 2:\\nFinish[REFUTES]\\nObservation 2:\\nEpisode finished\\nAct\\nClaim:\\nReign Over Me is an American film made in 2010.\\nAction 1:\\nSearch[Reign Over Me]\\nAction 2:\\nFinish[SUPPORTS]\\nObservation 2:\\nEpisode finished\\nCoT\\nClaim:\\nReign Over Me is an American film made in 2010.\\nThought:\\nLet’s think step by step.\\nFirst, is Reign Over Me an American film?\\nYes,\\nit is.\\nSecond, was it made in 2010?\\nYes, it was.\\nAnswer:\\nSUPPORTS\\nD.2\\nALFWORLD TRAJECTORIES\\nWe provide an example trajectory of ReAct, Act, and ReAct-IM on the same ALFWorld game,\\npick_clean_then_place_in_recep-Knife-None-CounterTop-10/\\ntrial_T20190909_110445_675754.\\nD.2.1\\nREACT TRAJECTORY\\nReAct is able to correctly ﬁnd the knife, clean it, and then place it on a countertop.\\n27\\nPublished as a conference paper at ICLR 2023\\nReAct ALFWorld trajectory\\nYou are in the middle of a room.\\nLooking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to:\\nput a clean knife in countertop.\\n> think:\\nTo solve the task, I need to find and take a knife, then clean it with\\nsinkbasin, then put it in countertop.\\nOK.\\n> think:\\nFirst I need to find a knife.\\nA knife is more likely to appear in cabinet\\n(1-6), drawer (1-3), countertop (1-3), fridge (1), garbagecan (1), shelf (1-3),\\nsinkbasin (1), stoveburner (1-4), toaster (1).\\nI can check one by one, starting with\\ncabinet 1.\\nOK.\\n> go to cabinet 1\\nOn the cabinet 1, you see a bowl 1.\\n> go to cabinet 2\\nThe cabinet 2 is closed.\\n> go to cabinet 3\\nOn the cabinet 3, you see a glassbottle 1.\\n> go to cabinet 4\\nOn the cabinet 4, you see a mug 1.\\n> go to cabinet 5\\nThe cabinet 5 is closed.\\n> go to cabinet 6\\nThe cabinet 6 is closed.\\n> go to drawer 1\\nThe drawer 1 is closed.\\n> go to drawer 2\\nThe drawer 2 is closed.\\n> go to drawer 3\\nThe drawer 3 is closed.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n> think:\\nNow I find a knife (1).\\nNext, I need to take it.\\nOK.\\n> take knife 1 from countertop 2\\nYou pick up the knife 1 from the countertop 2.\\n> think:\\nNow I take a knife (1).\\nNext, I need to go to sinkbasin (1) and clean it.\\nOK.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a fork 3, a lettuce 3, and a spatula 2.\\n> clean knife 1 with sinkbasin 1\\nYou clean the knife 1 using the sinkbasin 1.\\n> think:\\nNow I clean a knife (1).\\nNext, I need to put it in/on countertop 1.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> put knife 1 in/on countertop 1\\nYou put the knife 1 in/on the countertop 1.\\nD.2.2\\nACT TRAJECTORY\\nAct is able to correctly ﬁnd the knife, but tries to clean it using a sinkbasin before actually going\\nto the sinkbasin. From there on, the model is unable to recover, and is stuck repeating the same\\nsequence of commands. With ReAct, an existing thought would have concretely identiﬁed that the\\nknife had been taken, and that the next subgoal was to go to a sinkbasin to clean it.\\n28\\nPublished as a conference paper at ICLR 2023\\nAct ALFWorld trajectory\\nYou are in the middle of a room.\\nLooking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to:\\nput a clean knife in countertop.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nYou pick up the knife 1 from the countertop 2.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\nD.2.3\\nREACT-IM TRAJECTORY\\nReAct-IM is able to correctly ﬁnd the knife, but is unable to clean it as well. The LLM then\\ngets stuck trying to place this knife on a countertop. One possible issue is the incorrect thought\\nthink:\\nI need to find a clean knife., which seems to trick the LLM in believing\\nthat the knife is already cleaned. In ReAct, this sequence of subgoals would be concretely captured\\nby each thought.\\n29\\nPublished as a conference paper at ICLR 2023\\nReAct-IM ALFWorld trajectory\\nYou are in the middle of a room.\\nLooking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to:\\nput a clean knife in countertop.\\n> think:\\nTo solve the task, I need to find and take a clean knife, then put it in\\ncountertop.\\nOK.\\n> think:\\nI need to find a clean knife.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> think:\\nI need to find a clean knife.\\nOK.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n> think:\\nI need to find a clean knife.\\nOK.\\n> take knife 1 from countertop 2\\nYou pick up the knife 1 from the countertop 2.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nYou put the knife 1 in/on the countertop 1.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n30\\nPublished as a conference paper at ICLR 2023\\nD.3\\nWEBSHOP TRAJECTORIES\\nTable 10: Example trajectories for Webshop predicted by Act and ReAct. Compared to Act,\\nReAct uses reasoning to ﬁnd products that satisfy all target attributes.\\nInstruction: get me a sixteen pack of apple cinnamon freeze dried banana chips, and price lower than 50.00 dollars\\nAct\\nReAct\\nAction: search\\n�\\nsixteen pack apple cinnamon freeze dried banana\\nchips\\n�\\nObservation:\\n�\\nBack to Search\\n�\\nPage 1 (Total results: 50)\\n�\\nNext\\n�\\n�\\nB0061IVFZE\\n�\\nBrothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -\\nOunce (Pack of 100)\\n$85.0\\n�\\nB096H2P6G2\\n�\\nMoon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -\\n(Variety Pack)\\n$18.99\\n�\\nB092JLLYK6\\n�\\nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\\nFor School Lunches or an On-The-Go Snack - No Sugar Added,\\nNon GMO, Gluten Free, Nothing Artiﬁcial (0.53oz) 6-Pack\\n$12.99\\nAction: click\\n�\\nB0061IVFZE\\n�\\nObservation:\\n�\\nBack to Search\\n�\\n�\\nPrev\\n�\\nﬂavor name [asian pear][banana][fuji apple & cinnamon][strawberry\\nbanana] Brothers-ALL-Natural Fruit Crisps, Strawberry Banana,\\n0.42 Ounce (Pack of 100)\\nPrice: $85.0\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction: click[Buy Now]\\nAction: search\\n�\\nsixteen pack apple cinnamon freeze dried banana\\nchips\\n�\\nObservation:\\n�\\nBack to Search\\n�\\nPage 1 (Total results: 50)\\n�\\nNext\\n�\\n�\\nB0061IVFZE\\n�\\nBrothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -\\nOunce (Pack of 100)\\n$85.0\\n�\\nB096H2P6G2\\n�\\nMoon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -\\n(Variety Pack)\\n$18.99\\n�\\nB092JLLYK6\\n�\\nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\\nFor School Lunches or an On-The-Go Snack - No Sugar Added,\\nNon GMO, Gluten Free, Nothing Artiﬁcial (0.53oz) 6-Pack\\n$12.99\\nAction: think[B0061IVFZE is strawberry banana, not apple cinnamon.\\nB096H2P6G2 is fruit snacks, not freeze dried banana chips.\\nB092JLLYK6 is banana crisps, not apple cinnamon. I can check\\nB092JLLYK6 ﬁrst.\\nObservation: OK.\\nAction: click\\n�\\nB092JLLYK6\\n�\\nObservation:\\n�\\nBack to Search\\n�\\n�\\nPrev\\n�\\nﬂavor name [apple][apple cinnamon][banana][cantaloupe][peach][pear]\\n[strawberry][strawberry banana][sampler variety pack][mega variety\\npack][orchard variety pack]\\nsize [0.53 ounce (pack of 6)][0.53 ounce (pack of 8)][0.53 ounce (pack\\nof 16)]\\nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\\nFor School Lunches or an On-The-Go Snack - No Sugar Added, Non\\nGMO, Gluten Free, Nothing Artiﬁcial (0.53oz) 6-Pack\\nPrice: $12.99\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction: think[For sixteen pack of apple cinnamon freeze dried banana\\nchips, the item has options ’apple cinnamon’ and ’0.53 ounce (pack of\\n16)’ and seems good to buy.]\\nObservation: OK.\\nAction: click[apple cinnamon]\\nObservation: You have clicked apple cinnamon.\\nAction: click[0.53 ounce (pack of 16)]\\nObservation: You have clicked 0.53 ounce (pack of 16).\\nAction: click[Buy Now]\\nScore: 0.125\\nScore: 1.0\\n31\\nPublished as a conference paper at ICLR 2023\\nE\\nMORE ANALYSIS\\nE.1\\nSUCCESS AND FAILURE MODES ANALYSIS\\nWe provide some examples corresponding to the success and error mode analysis given in Sec. 3.3.\\nSearch results and non-representative steps are omitted to reduce space.\\nSuccess: True positive\\nReAct\\nQuestion:\\nAuthor David Chanoff has collaborated with a U.S. Navy admiral who served as\\nthe ambassador to the United Kingdom under which President?\\nThought 1:\\nI need to search David Chanoff and find the U.S. Navy admiral he\\ncollaborated with.\\nAction 1:\\nSearch[David Chanoff]\\nObservation 1:\\nDavid Chanoff is a noted author...\\nAction 2:\\nSearch[William J. Crowe]\\nObservation 2:\\nWilliam J. Crowe is...\\nThought 3:\\nWilliam J. Crowe served as the ambassador to the United Kingdom under\\nPresident Bill Clinton.\\nSo the answer is Bill Clinton.\\nAction 3:\\nFinish[Bill Clinton]\\nObservation 3:\\nEpisode finished\\nCoT\\nQuestion:\\nWhat was the name of the team that represented an Arizona university in\\nthe conference that covers a broad expanse of the western United States,\\nincluding Arizona, California, and New Mexico?\\nThought:\\nLet’s think step by step.\\nThe name of the team that represented an Arizona\\nuniversity in the conference that covers a broad expanse of the western\\nUnited States, including Arizona, California, and New Mexico is Arizona\\nState Sun Devils.\\nAnswer:\\nArizona State Sun Devils\\nSuccess: False positive\\nReAct\\nQuestion:\\nDid the board game San Marco or About Time come out first?\\nThought 1:\\nI need to search San Marco and About Time, and find which came out first.\\nAction 1:\\nSearch[San Marco]\\nObservation 1:\\nSan Marco is one of the ...\\nThe was built in venice.\\nThought 2:\\nSan Marco is a sestiere of Venice.\\nI need to search About Time next.\\nAction 2:\\nSearch[About Time]\\nObservation 2:\\nCould not find [About Time].\\nSimilar:\\n[...].\\nThought 3:\\nAbout Time is a 2013 film.\\nSo San Marco came out first.\\nAction 3:\\nFinish[San Marco]\\nObservation 3:\\nEpisode finished\\nCoT\\nQuestion:\\nWas Vanderbilt University or Emory University founded first?\\nThought:\\nLet’s think step by step.\\nVanderbilt University was founded in 1873.\\nEmory\\nUniversity was founded in 1836.\\n1873 (Vanderbilt University) > 1836 (Emory\\nUniversity), so Vanderbilt University was founded first.\\nAnswer:\\nEmory University\\nFailure: Reasoning error\\nReAct\\nQuestion:\\nWhat was the middle name of the actress who was cast alongside Harvey\\nFierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura\\nBell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur in\\nHairspray?\\nThought 1:\\nI need to search Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell,\\nMary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and\\nMarissa Jaret Winokur, find the actress that was cast alongside them in\\nHairspray, then find its middle name.\\nAction 1:\\nSearch[Harvey Fierstein]\\nObservation 1:\\nHarvey Forbes Fierstein ...\\nThought 2:\\nHarvey Fierstein is an actor, playwright, and screenwriter.\\nI need to\\nsearch Linda Hart next...\\nAction 2:\\nSearch[Linda Hart]...\\nCoT\\nQuestion:\\nWho is older Danny Green or James Worthy?\\nThought:\\nLet’s think step by step.\\nDanny Green was born in 1987.\\nJames Worthy was\\nborn in 1961.\\n1987 (Danny Green) > 1961 (James Worthy), so Danny Green is\\nolder.\\nAnswer:\\nDanny Green\\n32\\nPublished as a conference paper at ICLR 2023\\nFailure: Search error\\nReAct\\nQuestion:\\nWhat does the goddess associated with the goddess frigg consists of what\\ntales?\\nThought 1:\\nI need to search goddess frigg and find its associated goddess.\\nAction 1:\\nSearch[goddess frigg]\\nObservation 1:\\nCould not find goddess frigg.\\nSimilar:\\n[’Frigg’, ’Frigg and Freyja common\\norigin hypothesis’, ’Freyja (section Relation to Frigg and other goddesses\\nand figures)’, ’Eir (redirect from Eir (goddess))’, ’Baldr’].\\nThought 2:\\nI need to search Frigg instead...\\nFailure: Hallucination\\nCoT\\nQuestion:\\nWhat was the first year a scientific journal published by an organization\\nlocated in the Bronx was published?\\nThought:\\nLet’s think step by step.\\nThe first year a scientific journal published by\\nan organization located in the Bronx was published was 1916.\\nAnswer:\\n1916 (Label:\\n1909)\\nFailure: Label ambiguity\\nReAct\\nQuestion:\\nWhat nationality is the man who developed the dictator game?\\nThought 1:\\nI need to search dictator game, find the man who developed it, then find his\\nnationality.\\nAction 1:\\nSearch[dictator game]\\nObservation 1:\\nThe dictator game is a popular experimental instrument in social psychology\\nand economics...\\nAction 3:\\nSearch[Amos Tversky]\\nObservation 3:\\nAmos Nathan Tversky...\\nThought 4:\\nAmos Tversky is an Israeli cognitive and mathematical psychologist.\\nSo the\\nman who developed the dictator game is Israeli.\\nAction 4:\\nFinish[Israeli] (label:\\nIsrael-American)\\nCoT\\nQuestion:\\nKurt Fuller from Stockton, CA appeared in which American Detective Comedy\\nDrama television series created by Steve Franks?\\nThought:\\nLet’s think step by step.\\nKurt Fuller from Stockton, CA appeared in Psych.\\nPsych is an American Detective Comedy Drama television series created by\\nSteve Franks.\\nAnswer:\\nPsych (label:\\nPsych is an American detective comedy-drama)\\n33\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_by_arxiv_loader[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fceb37-e651-4b74-8342-b36b61316330",
   "metadata": {},
   "source": [
    "分割したい場合は `load_and_split` を利用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "23c7b116-dda3-4baf-bb19-d8b9d1c45ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Published as a conference paper at ICLR 2023\\nREACT: SYNERGIZING REASONING AND ACTING IN\\nLANGUAGE MODELS\\nShunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\\n1Department of Computer Science, Princeton University\\n2Google Research, Brain team\\n1{shunyuy,karthikn}@princeton.edu\\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\\nABSTRACT\\nWhile large language models (LLMs) have demonstrated impressive performance\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\\nplan generation) have primarily been studied as separate topics. In this paper, we\\nexplore the use of LLMs to generate both reasoning traces and task-speciﬁc actions\\nin an interleaved manner, allowing for greater synergy between the two: reasoning\\ntraces help the model induce, track, and update action plans as well as handle\\nexceptions, while actions allow it to interface with and gather additional information\\nfrom external sources such as knowledge bases or environments. We apply our\\napproach, named ReAct, to a diverse set of language and decision making tasks\\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\\nimproved human interpretability and trustworthiness. Concretely, on question\\nanswering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent\\nissues of hallucination and error propagation in chain-of-thought reasoning by\\ninteracting with a simple Wikipedia API, and generating human-like task-solving\\ntrajectories that are more interpretable than baselines without reasoning traces.\\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\\nan absolute success rate of 34% and 10% respectively, while being prompted with\\nonly one or two in-context examples.\\n1\\nINTRODUCTION\\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\\nsider the example of cooking up a dish in the kitchen. Between any two speciﬁc actions, we may\\nreason in language in order to track progress (“now that everything is cut, I should heat up the pot of\\nwater”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let\\nme use soy sauce and pepper instead”), and to realize when external information is needed (“how do\\nI prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the\\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What\\ndish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans\\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\\nunseen circumstances or facing information uncertainties.\\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\\n∗Work during Google internship. Projet page with code: https://react-lm.github.io/.\\n1\\narXiv:2210.03629v3  [cs.CL]  10 Mar 2023\\nPublished as a conference paper at ICLR 2023\\n�������������������������������������������������������� �\\n����������������������������������������������������������\\n��\\n���������������������� �\\n������������������������������������������ �\\n����������������������������������������������������������������������������\\n������������������������� �\\n���������������������������������������������������������\\n�', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='����������������������������������������������������������������������������\\n������������������������� �\\n���������������������������������������������������������\\n�\\n���������������������������������������\\n��������������������������������������������� �\\n�������������������������������������������������������������\\n�\\n����������������������������������������������������������\\n�\\n���������������������� �\\n��������������������� �\\n�������������������������� �\\n��������������������� �\\n���������������������������\\n������������������������������������������ �\\n���������������������������������������������������\\n�������������\\n������������\\n����������������������\\n������������������������� �\\n��������� ���������������� �\\n���������������������� �\\n����������������������� �\\n�������������������������� �\\n������������������������� �\\n����������������������� �\\n���������������������� �\\n������\\n�������������������������� �\\n�����\\n��������������\\n��������������������������� �\\n�������������������������� ���\\n����������������\\n����������������������� ��\\n���������������������������� �\\n����������������\\n����������������������� �\\n���������� ��\\n��������������������� �\\n������������������������� �\\n����������\\n������������������\\n�������������������������\\n����������������������������������������������������� �\\n����������������������������������������������������\\n���������������������������\\n���������������������������������������������������������\\n�\\n����������������������������������������������������������\\n�\\n���������������������� ����������\\n����������������������������������������������������������\\n�\\n�����������������������������������������������������\\n������\\n���������������������������������������������������\\n������������������������\\n�������������������������������������������������������\\n�\\n����������������������������������������� ��������� �\\n���������� �����\\n���������������������������������������������������� ������\\n�������������� ��\\n���������������������������������� �\\n����������������������������������������������������������\\n���������������������������������������������������������\\n�\\n������������������������������������� ����������������� �\\n�����������������������\\n������������������������������������ �\\n�\\n��������������������������������������������������������\\n�\\n���������������������������������������������������\\n�\\n��������������������������\\n��������������\\n��������������������� �\\n������������������������������ �\\n�������������������� �\\n����������������������������������������������������������\\n�\\n��������������������������������������\\n�\\n������������������������ �\\n����������������������������������������������������\\n�\\n������������������������� �\\n������������������������������������������� �\\n����������������������� �\\n������������������������������������������� �\\n����������������������� �\\n�������������������������\\n�\\n�������������������������������������������������������\\n�\\n����������������������������������������������������������\\n�\\n���������������������������������������������������\\n�\\n����������������������������������������������������\\n������������\\n��������������\\nFigure 1: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought (CoT,\\nReason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)\\nquestion. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar\\net al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task\\nsolving trajectories generated by the model (Act, Thought) and the environment (Obs).\\nanswers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,\\n2022). However, this “chain-of-thought” reasoning is a static black box, in that the model uses\\nits own internal representations to generate thoughts and is not grounded in the external world,\\nwhich limits its ability to reason reactively or update its knowledge. This can lead to issues like fact', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='which limits its ability to reason reactively or update its knowledge. This can lead to issues like fact\\nhallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,\\nrecent work has explored the use of pre-trained language models for planning and acting in interactive\\nenvironments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with\\na focus on predicting actions via language priors. These approaches usually convert multi-modal\\nobservations into text, use a language model to generate domain-speciﬁc actions or plans, and then\\nuse a controller to choose or execute them. However, they do not employ language models to reason\\nabstractly about high-level goals or maintain a working memory to support acting, barring Huang\\net al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the\\ncurrent state. Beyond such simple embodied tasks to interact with a few blocks, there have not been\\nstudies on how reasoning and acting can be combined in a synergistic manner for general task solving,\\nand if such a combination can bring systematic beneﬁts compared to reasoning or acting alone.\\nIn this work, we present ReAct, a general paradigm to combine reasoning and acting with language\\nmodels for solving diverse language reasoning and decision making tasks (Figure 1). ReAct\\nprompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an\\ninterleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and\\nadjust high-level plans for acting (reason to act), while also interact with the external environments\\n(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).\\n2\\nPublished as a conference paper at ICLR 2023\\nWe conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:\\nquestion answering (HotPotQA, Yang et al., 2018), fact veriﬁcation (Fever, Thorne et al., 2018),\\ntext-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao\\net al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact\\nwith, ReAct outperforms vanilla action generation models while being competitive with chain-of-\\nthought reasoning (CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct\\nand CoT that allows for the use of both internal knowledge and externally obtained information\\nduring reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able\\nto outperform imitation or reinforcement learning methods trained with 103 ∼ 105 task instances,\\nwith an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate\\nthe importance of sparse, versatile reasoning in decision making by showing consistent advantages\\nover controlled baselines with actions only. Besides general applicability and performance boost,\\nthe combination of reasoning and acting also contributes to model interpretability, trustworthiness,\\nand diagnosability across all domains, as humans can readily distinguish information from model’s\\ninternal knowledge versus external environments, as well as inspect reasoning traces to understand\\nthe decision basis of model actions.\\nTo summarize, our key contributions are the following: (1) we introduce ReAct, a novel prompt-\\nbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we\\nperform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a\\nfew-shot learning setup over prior approaches that perform either reasoning or action generation in\\nisolation; (3) we present systematic ablations and analysis to understand the importance of acting in\\nreasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the\\nprompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial ﬁnetuning', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='prompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial ﬁnetuning\\nexperiments showing the potential of ReAct to improve with additional training data. Scaling up\\nReAct to train and operate on more tasks and combining it with complementary paradigms like\\nreinforcement learning could further unlock the potential of large language models.\\n2\\nREAC T: SYNERGIZING REASONING + ACTING\\nConsider a general setup of an agent interacting with an environment for task solving. At time\\nstep t, an agent receives an observation ot ∈ O from the environment and takes an action at ∈ A\\nfollowing some policy π(at|ct), where ct = (o1, a1, · · · , ot−1, at−1, ot) is the context to the agent.\\nLearning a policy is challenging when the mapping ct �→ at is highly implicit and requires extensive\\ncomputation. For example, the agent shown in Figure 1(1c) is unable to generate the correct ﬁnal\\naction (Act 4) to ﬁnish the QA task as it requires complex reasoning over the trajectory context\\n(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the\\ncontext that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.\\nThe idea of ReAct is simple: we augment the agent’s action space to ˆ\\nA = A ∪ L, where L is the\\nspace of language. An action ˆat ∈ L in the language space, which we will refer to as a thought or a\\nreasoning trace, does not affect the external environment, thus leading to no observation feedback.\\nInstead, a thought ˆat aims to compose useful information by reasoning over the current context ct,\\nand update the context ct+1 = (ct, ˆat) to support future reasoning or acting. As shown in Figure 1,\\nthere could be various types of useful thoughts, e.g. decomposing task goals and create action plans\\n(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),\\nextracting important parts from observations (1d, Thought2, 4), track progress and transit action plans\\n(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.\\nHowever, as the language space L is unlimited, learning in this augmented action space is difﬁcult\\nand requires strong language priors. In this paper, we mainly focus on the setup where a frozen\\nlarge language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context\\nexamples to generate both domain-speciﬁc actions and free-form language thoughts for task solving\\n(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and\\nenvironment observations to solve a task instance (see Appendix C). For the tasks where reasoning is\\nof primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the\\ntask-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision\\nmaking tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to\\n1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B.\\n3\\nPublished as a conference paper at ICLR 2023\\nappear sparsely in the most relevant positions of a trajectory, so we let the language model decide the\\nasynchronous occurrence of thoughts and actions for itself.\\nSince decision making and reasoning capabilities are integrated into a large language model, ReAct\\nenjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is\\nstraightforward as human annotators just type down their thoughts in language on top of their actions\\ntaken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail\\nprompt design for each task in Sections 3 and 4. B) General and ﬂexible: Due to the ﬂexible thought\\nspace and thought-action occurrence format, ReAct works for diverse tasks with distinct action\\nspaces and reasoning needs, including but not limited to QA, fact veriﬁcation, text game, and web', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='space and thought-action occurrence format, ReAct works for diverse tasks with distinct action\\nspaces and reasoning needs, including but not limited to QA, fact veriﬁcation, text game, and web\\nnavigation. C) Performant and robust: ReAct shows strong generalization to new task instances\\nwhile learning solely from one to six in-context examples, consistently outperforming baselines with\\nonly reasoning or acting across different domains. We also show in Section 3 additional beneﬁts\\nwhen ﬁnetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.\\nD) Human aligned and controllable: ReAct promises an interpretable sequential decision making\\nand reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,\\nhumans can also control or correct the agent behavior on the go by thought editing, as shown in\\nFigure 5 in Section 4.\\n3\\nKNOWLEDGE-INTENSIVE REASONING TASKS\\nWe begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact\\nveriﬁcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to\\nretrieve information to support reasoning, while also use reasoning to target what to retrieve next,\\ndemonstrating a synergy of reasoning and acting.\\n3.1\\nSETUP\\nDomains\\nWe consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-\\nPotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning\\nover two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact veriﬁcation\\nbenchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based\\non if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only\\nsetup for both tasks, where models only receive the question/claim as input without access to support\\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\\nan external environment to support reasoning.\\nAction Space\\nWe design a simple Wikipedia web API with three types of actions to support\\ninteractive information retrieval: (1) search[entity], which returns the ﬁrst 5 sentences from\\nthe corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the\\nWikipedia search engine, (2) lookup[string], which would return the next sentence in the page\\ncontaining string, simulating Ctrl+F functionality on the browser. (3) finish[answer], which\\nwould ﬁnish the current task with answer. We note that this action space mostly can only retrieve a\\nsmall part of a passage based on exact passage name, which is signiﬁcantly weaker than state-of-the-\\nart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\\nand force models to retrieve via explicit reasoning in language.\\n3.2\\nMETHODS\\nReAct Prompting\\nFor HotpotQA and Fever, we randomly select 6 and 3 cases2 from the training\\nset and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.\\nSimilar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\\nthought), where free-form thoughts are used for various purposes. Speciﬁcally, we use a combination\\nof thoughts that decompose questions (“I need to search x, ﬁnd y, then ﬁnd z”), extract information\\nfrom Wikipedia observations (“x was started in 1844”, “The paragraph does not tell x”), perform\\ncommonsense (“x is not y, so z must instead be...”) or arithmetic reasoning (“1844 < 1989”), guide\\n2We ﬁnd more examples do not improve performance.\\n4\\nPublished as a conference paper at ICLR 2023\\nPrompt Methoda\\nHotpotQA\\nFever\\n(EM)\\n(Acc)\\nStandard\\n28.7\\n57.1\\nCoT (Wei et al., 2022)\\n29.4\\n56.3\\nCoT-SC (Wang et al., 2022a)\\n33.4\\n60.4\\nAct\\n25.7\\n58.9\\nReAct\\n27.4\\n60.9\\nCoT-SC → ReAct\\n34.2\\n64.6\\nReAct→ CoT-SC\\n35.1\\n62.0\\nSupervised SoTAb\\n67.5\\n89.5\\nTable 1: PaLM-540B prompting results on\\nHotpotQA and Fever.\\naHotpotQA EM is 27.1, 28.9, 33.8 for Standard, CoT,', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='27.4\\n60.9\\nCoT-SC → ReAct\\n34.2\\n64.6\\nReAct→ CoT-SC\\n35.1\\n62.0\\nSupervised SoTAb\\n67.5\\n89.5\\nTable 1: PaLM-540B prompting results on\\nHotpotQA and Fever.\\naHotpotQA EM is 27.1, 28.9, 33.8 for Standard, CoT,\\nCoT-SC in Wang et al. (2022b).\\nb(Zhu et al., 2021; Lewis et al., 2020)\\n0\\n5\\n10\\n15\\n20\\n#CoT-SC trials\\n26\\n28\\n30\\n32\\n34\\nHotpotQA EM\\n0\\n5\\n10\\n15\\n20\\n#CoT-SC trials\\n47.5\\n50.0\\n52.5\\n55.0\\n57.5\\n60.0\\n62.5\\n65.0\\nFever Acc\\nMethod\\nCoT-SC -> ReAct\\nReAct -> CoT-SC\\nCoT-SC\\nReAct\\nCoT\\nFigure 2: PaLM-540B prompting results with respect to\\nnumber of CoT-SC samples used.\\nsearch reformulation (“maybe I can search/look up x instead”), and synthesize the ﬁnal answer (“...so\\nthe answer is x”). See Appendix C for more details.\\nBaselines\\nWe systematically ablate ReAct trajectories to build prompts for multiple baselines (with\\nformats as Figure 1(1a-1c)): (a) Standard prompting (Standard), which removes all thoughts,\\nactions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,\\n2022), which removes actions and observations and serve as a reasoning-only baseline. We also\\nbuild a self-consistency baseline (CoT-SC) (Wang et al., 2022a;b) by sampling 21 CoT trajectories\\nwith decoding temperature 0.7 during inference and adopting the majority answer, which is found to\\nconsistently boost performance over CoT. (c) Acting-only prompt (Act), which removes thoughts\\nin ReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the\\nInternet to answer questions, though it operates on a different task and action space, and uses imitation\\nand reinforcement learning instead of prompting.\\nCombining Internal and External Knowledge\\nAs will be detail in Section 3.3, we observe that\\nthe problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT\\nis more accurate in formulating reasoning structure but can easily suffer from hallucinated facts\\nor thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide\\nwhen to switch to the other method based on the following heuristics: A) ReAct → CoT-SC: when\\nReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for\\nHotpotQA and FEVER respectively as we ﬁnd more steps will not improve ReAct performance3.\\nB) CoT-SC → ReAct: when the majority answer among n CoT-SC samples occurs less than n/2\\ntimes (i.e. internal knowledge might not support the task conﬁdently), back off to ReAct.\\nFinetuning\\nDue to the challenge of manually annotating reasoning traces and actions at scale,\\nwe consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories\\nwith correct answers generated by ReAct (also for other baselines) to ﬁnetune smaller language\\nmodels (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on\\ninput questions/claims. More details are in Appendix B.1.\\n3.3\\nRESULTS AND OBSERVATIONS\\nReAct outperforms Act consistently\\nTable 1 shows HotpotQA and Fever results using PaLM-\\n540B as the base model with different prompting methods. We note that ReAct is better than Act\\non both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the\\nﬁnal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also conﬁrm the beneﬁt of reasoning\\ntraces for more informed acting.\\n3Of all trajectories with correct ﬁnal answers, those with 7 steps on HotpotQA and 5 steps on FEVER only\\ntake up 0.84% and 1.33% respectively.\\n5\\nPublished as a conference paper at ICLR 2023\\nType\\nDeﬁnition\\nReAct\\nCoT\\nSuccess\\nTrue positive\\nCorrect reasoning trace and facts\\n94%\\n86%\\nFalse positive\\nHallucinated reasoning trace or facts\\n6%\\n14%\\nFailure\\nReasoning error\\nWrong reasoning trace (including failing to recover from repetitive steps)\\n47%\\n16%\\nSearch result error\\nSearch return empty or does not contain useful information\\n23%\\n-\\nHallucination\\nHallucinated reasoning trace or facts\\n0%\\n56%\\nLabel ambiguity\\nRight prediction but did not match the label precisely', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='Search return empty or does not contain useful information\\n23%\\n-\\nHallucination\\nHallucinated reasoning trace or facts\\n0%\\n56%\\nLabel ambiguity\\nRight prediction but did not match the label precisely\\n29%\\n28%\\nTable 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their\\npercentages in randomly selected examples studied by human.\\nReAct vs. CoT\\nOn the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly\\nlags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only\\ndiffer by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge\\nis vital. To better understand the behavioral difference between ReAct and CoT on HotpotQA, we\\nrandomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct\\nand CoT respectively (thus 200 examples in total), and manually labeled their success and failure\\nmodes in Table 2. Some key observations are as follows:\\nA) Hallucination is a serious problem for CoT, resulting in much higher false positive rate than\\nReAct (14% vs. 6%) in success mode, and make up its major failure mode (56%). In contrast, the\\nproblem solving trajectory of ReActis more grounded, fact-driven, and trustworthy, thanks to the\\naccess of an external knowledge base.\\nB) While interleaving reasoning, action and observation steps improves ReAct’s grounded-\\nness and trustworthiness, such a structural constraint also reduces its ﬂexibility in formulating\\nreasoning steps, leading to more reasoning error rate than CoT. we note that there is one frequent\\nerror pattern speciﬁc to ReAct, in which the model repetitively generates the previous thoughts and\\nactions, and we categorize it as part of “reasoning error” as the model fails to reason about what the\\nproper next action to take and jump out of the loop4.\\nC) For ReAct, successfully retrieving informative knowledge via search is critical. Non-\\ninformative search, which counts for 23% of the error cases, derails the model reasoning and gives\\nit a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between\\nfactuality and ﬂexibility, which motivates our proposed strategies of combining two methods.\\nWe provide examples for each success and failure modes in Appendix E.1. We also ﬁnd some\\nHotpotQA questions may contain outdated answer labels, see Figure 4 for example.\\nReAct + CoT-SC perform best for prompting LLMs\\nAlso shown in Table 1, the best prompting\\nmethod on HotpotQA and Fever are ReAct → CoT-SC and CoT-SC → ReAct respectively.\\nFurthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC\\nsamples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both\\nsigniﬁcantly and consistently outperform CoT-SC across different number of samples, reaching\\nCoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of\\nproperly combining model internal knowledge and external knowledge for reasoning tasks.\\nReAct performs best for ﬁne-tuning\\nFigure 3 shows the scaling effect of prompting/ﬁnetuning\\nfour methods (Standard, CoT, Act, ReAct) on HotpotQA. With PaLM-8/62B, prompting ReAct\\nperforms worst among four methods due to the difﬁculty to learn both reasoning and acting from\\nin-context examples. However, when ﬁnetuned with just 3,000 examples, ReAct becomes the best\\nmethod among the four, with PaLM-8B ﬁnetuned ReAct outperforming all PaLM-62B prompting\\nmethods, and PaLM-62B ﬁnetuned ReAct outperforming all 540B prompting methods. In contrast,\\nﬁnetuning Standard or CoT is signiﬁcantly worse than ﬁnetuning ReAct or Act for both PaLM-\\n8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge\\nfacts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a\\nmore generalizable skill for knowledge reasoning. As all prompting methods are still signiﬁcantly', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='facts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a\\nmore generalizable skill for knowledge reasoning. As all prompting methods are still signiﬁcantly\\nfar from domain-speciﬁc state-of-the-art approaches (Table 1), we believe ﬁnetuning with more\\nhuman-written data might be a better way to unleash the power of ReAct.\\n4We suspect that this could be due to the sub-optimal greedy decoding procedure, and future work using\\nbetter decoding (e.g. beam search) might help address this issue.\\n6\\nPublished as a conference paper at ICLR 2023\\n8b\\n62b\\n540b\\nsize\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nHotpotQA EM\\nlearning = prompt\\n8b\\n62b\\n540b\\nsize\\nlearning = finetune\\nMethod\\nStandard\\nCoT\\nAct\\nReAct\\nFigure 3: Scaling results for prompting and ﬁnetuning on HotPotQA with ReAct (ours) and baselines.\\n4\\nDECISION MAKING TASKS\\nWe also test ReAct on two language-based interactive decision-making tasks, ALFWorld and\\nWebShop, both of which feature complex environments that require agents to act over long horizons\\nwith sparse rewards, warranting the need for reasoning to act and explore effectively.\\nALFWorld\\nALFWorld (Shridhar et al., 2020b) (Figure 1(2)) is a synthetic text-based game designed\\nto align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of\\ntasks in which an agent needs to achieve a high-level goal (e.g. examine paper under desklamp) by\\nnavigating and interacting with a simulated household via text actions (e.g. go to coffeetable 1, take\\npaper 2, use desklamp 1). A task instance can have more than 50 locations and take an expert policy\\nmore than 50 steps to solve, thus challenging an agent to plan and track subgoals, as well as explore\\nsystematically (e.g. check all desks one by one for desklamp). In particular, one challenge built into\\nALFWorld is the need to determine likely locations for common household items (e.g. desklamps will\\nlikely be on desks, shelfs, or dressers), making this environment a good ﬁt for LLMs to exploit their\\npretrained commonsense knowledge. To prompt ReAct, we randomly annotate three trajectories\\nfrom the training set for each task type, where each trajectory includes sparse thoughts that (1)\\ndecompose the goal, (2) track subgoal completion, (3) determine the next subgoal, and (4) reason via\\ncommonsense where to ﬁnd an object and what to do with it. We show prompts used for ALFWorld\\nin Appendix C.4. Following Shridhar et al. (2020b), we evaluate on 134 unseen evaluation games\\nin a task-speciﬁc setup. For robustness, we construct 6 prompts for each task type through each\\npermutation of 2 annotated trajectories from the 3 we annotate. Act prompts are constructed using\\nthe same trajectories, but without thoughts — since task instances are randomly chosen from the\\ntraining set, it favors neither ReAct nor Act and provides a fair and controlled comparison to test the\\nimportance of sparse thoughts. For baselines, we use BUTLER (Shridhar et al., 2020b), an imitation\\nlearning agent trained on 105 expert trajectories for each task type5.\\nWebShop\\nCan ReAct also interact with noisy real-world language environments for practical\\napplications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping\\nwebsite environment with 1.18M real-world products and 12k human instructions. Unlike ALFWorld,\\nWebshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions,\\nand options crawled from Amazon), and requires an agent to purchase a product based on a user\\ninstruction (e.g. “I am looking for a nightstand with drawers. It should have a nickel ﬁnish, and\\npriced lower than $140”) through web interactions (e.g. search “nightstand drawers”, choose buttons\\nsuch as “color: modern-nickel-white” or “back to search”). This task is evaluated by average score\\n(percentage of desired attributes covered by the chosen product averaged across all episodes) and', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='such as “color: modern-nickel-white” or “back to search”). This task is evaluated by average score\\n(percentage of desired attributes covered by the chosen product averaged across all episodes) and\\nsuccess rate (percentage of episodes where the chosen product satisﬁes all requirements) on 500 test\\ninstructions. We formulate Act prompts with actions to search, choose product, choose options,\\nand buy, with ReAct prompts additionally reasoning to determine what to explore, when to buy,\\nand what products options are relevant to the instruction. See Table 6 for an example prompt, and\\nTable 10 for model predictions in the Appendix. We compare to an imitation learning (IL) method\\n5Micheli & Fleuret (2021) ﬁnetuned a GPT-2 model on 3553 task instances and achieved a much improved\\nperformance than BUTLER, but it is trained on all task types, thus not included as a baseline.\\n7\\nPublished as a conference paper at ICLR 2023\\nMethod\\nPick\\nClean\\nHeat\\nCool\\nLook\\nPick 2\\nAll\\nAct (best of 6)\\n88\\n42\\n74\\n67\\n72\\n41\\n45\\nReAct (avg)\\n65\\n39\\n83\\n76\\n55\\n24\\n57\\nReAct (best of 6)\\n92\\n58\\n96\\n86\\n78\\n41\\n71\\nReAct-IM (avg)\\n55\\n59\\n60\\n55\\n23\\n24\\n48\\nReAct-IM (best of 6)\\n62\\n68\\n87\\n57\\n39\\n33\\n53\\nBUTLERg (best of 8)\\n33\\n26\\n70\\n76\\n17\\n12\\n22\\nBUTLER (best of 8)\\n46\\n39\\n74\\n100\\n22\\n24\\n37\\nTable 3: AlfWorld task-speciﬁc success rates (%). BUTLER and\\nBUTLERg results are from Table 4 of Shridhar et al. (2020b). All\\nmethods use greedy decoding, except that BUTLER uses beam search.\\nMethod\\nScore\\nSR\\nAct\\n62.3\\n30.1\\nReAct\\n66.6\\n40.0\\nIL\\n59.9\\n29.1\\nIL+RL\\n62.4\\n28.7\\nHuman\\n82.1\\n59.6\\nExpert\\nTable 4: Score and suc-\\ncess rate (SR) on Web-\\nshop. IL/IL+RL taken\\nfrom Yao et al. (2022).\\ntrained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL)\\nmethod additionally trained with 10,587 training instructions.\\nResults\\nReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On\\nALFWorld, the best ReAct trial achieves an average success rate of 71%, signiﬁcantly outperforming\\nthe best Act (45%) and BUTLER (37%) trials. In fact, even the worse ReAct trial (48%) beats\\nthe best trial of both methods. Moreover, the advantage of ReAct over Act is consistent across\\nsix controlled trials, with relative performance gain ranging from 33% to 90% and averaging 62%.\\nQualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals\\ninto smaller subgoals, or loses track of the current state of the environment. Example trajectories\\ncomparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2.\\nOn Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With\\nadditional sparse reasoning, ReAct achieves signiﬁcantly better performance, with an absolute 10%\\nimprovement over the previous best success rate. By checking examples, we ﬁnd that ReAct is more\\nlikely to identify instruction-relevant products and options by reasoning to bridge the gap between\\nnoisy observations and actions (e.g. “For ‘space-saving ottoman bench for living room’, the item\\nhas options ‘39x18x18inch’ and ‘blue’ and seems good to buy.”). However, existing methods are\\nstill far from the performance of expert humans (Table 4), who perform signiﬁcantly more product\\nexplorations and query re-formulations that are still challenging for prompting-based methods.\\nOn the value of internal reasoning vs. external feedback\\nTo our knowledge, ReAct is the ﬁrst\\ndemonstration of combined reasoning and action using an LLM applied to an interactive environment\\nwithin a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang\\net al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner\\nmonologue”. However, IM’s “inner monologue” is limited to observations of the environment\\nstate and what needs to be completed by the agent for the goal to be satisﬁed. In contrast, the\\nreasoning traces in ReAct for decision making is ﬂexible and sparse, allowing diverse reasoning', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='state and what needs to be completed by the agent for the goal to be satisﬁed. In contrast, the\\nreasoning traces in ReAct for decision making is ﬂexible and sparse, allowing diverse reasoning\\ntypes (see Section 2) to be induced for different tasks.\\nTo demonstrate the differences between ReAct and IM, and to highlight the importance of internal\\nreasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought\\npattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially\\noutperforms IM-style prompting (ReAct-IM) (71 vs. 53 overall success rate), with consistent\\nadvantages on ﬁve out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakes\\nin identifying when subgoals were ﬁnished, or what the next subgoal should be, due to a lack of high-\\nlevel goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where\\nan item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning.\\nBoth shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in\\nAppendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example\\ntrajectory in Appendix D.2.3.\\n8\\nPublished as a conference paper at ICLR 2023\\n5\\nRELATED WORK\\nLanguage model for reasoning\\nPerhaps the most well-known work of using LLMs for reasoning\\nis Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their\\nown “thinking procedure” for problem solving. Several follow-up works have since been performed,\\nincluding least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shot-\\nCoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,\\n(Madaan & Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and\\nobserved that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other\\nwork has also been extended to more sophisticated reasoning architecture beyond simple prompting.\\nFor example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps\\nof “selection” and “inference”. STaR (Zelikman et al., 2022) bootstraps the reasoning process by\\nﬁnetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswell\\n& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated\\nLM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which ﬁnetunes a LM on\\nintermediate computation steps, also demonstrate improvement on multi-step computation problems.\\nIn contrast to these methods, ReAct performs more than just isolated, ﬁxed reasoning, and integrates\\nmodel actions and their corresponding observations into a coherent stream of inputs for the model to\\nreason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\\nLanguage model for decision making\\nThe strong capability of LLMs has enabled them to perform\\ntasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a\\npolicy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,\\n2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to\\ncomplicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not\\nexplicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for\\nreinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)\\nand Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl\\net al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly\\nconsider the reasoning procedure either, and also relies on expensive datasets and human feedback', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='et al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly\\nconsider the reasoning procedure either, and also relies on expensive datasets and human feedback\\ncollections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the\\ndecision making process only requires language description of the reasoning procedure.6\\nLLMS have also been increasingly employed in interactive and embodied environments for planning\\nand decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)\\nand Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision\\nmaking. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which\\nis then reranked by an affordance model grounded on the visual environments for ﬁnal prediction.\\nInner Monologue made further improvements by adding the eponymous “inner monologue\", which is\\nimplemented as injected feedback from the environment. To our knowledge, Inner Monologue is the\\nﬁrst work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue\\nthat Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section 4. We\\nalso note that leveraging language as semantically-rich inputs in the process of interactive decision\\nmaking has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti\\net al., 2021; Huang et al., 2022a; Li et al., 2022). It is becoming more evident that with the help of\\nLLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and\\ndecision making. What is more, progress in LLMs has also inspired the development of versatile and\\ngeneralist agents like Reed et al. (2022).\\n6\\nCONCLUSION\\nWe have proposed ReAct – a simple yet effective method for synergizing reasoning and acting in\\nlarge language models. Through a diverse set of experiments on multi-hop question-answering, fact\\nchecking, and interactive decision-making tasks, we show that ReAct leads to superior performance\\nwith interpretable decision traces. Despite the simplicity of our method, complex tasks with large\\naction spaces require more demonstrations to learn well, which unfortunately can easily go beyond\\nthe input length limit of in-context learning. We explore the ﬁne-tuning approach on HotpotQA\\n6Human feedback can also be incorporated in a complementary manner but we leave it for future work.\\n9\\nPublished as a conference paper at ICLR 2023\\nwith initial promising results, but learning from more high-quality human annotations will be the\\ndesiderata to further improve the performance. Scaling up ReAct with multi-task training and\\ncombining it with complementary paradigms like reinforcement learning could result in stronger\\nagents that further unlock the potential of LLMs for more applications.\\nACKNOWLEDGMENTS\\nWe thank the support and feedback of many people from Google Brain team and Princeton NLP\\nGroup. This work was supported in part by the National Science Foundation under Grant No.\\n2107048. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are\\nthose of the author(s) and do not necessarily reﬂect the views of the National Science Foundation.\\nREPRODUCIBILITY STATEMENT\\nOur main experiments are done on PaLM (Chowdhery et al., 2022), which is not an openly accessible\\nmodel yet. To increase reproducibility, we have included all used prompts in Appendix C, additional\\nexperiments using GPT-3 (Brown et al., 2020) in Appendix A.1, and associated GPT-3 ReAct\\nprompting code at https://anonymous.4open.science/r/ReAct-2268/.\\nETHICS STATEMENT\\nReAct prompts large language models to generate more human interpretable, diagnosable, and\\ncontrollable task-solving trajectories than previous methods. However, hooking up a large language\\nmodel with an action space to interact with external environments (e.g. the web, physical environ-', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='controllable task-solving trajectories than previous methods. However, hooking up a large language\\nmodel with an action space to interact with external environments (e.g. the web, physical environ-\\nments) has potential dangers, e.g. looking up inappropriate or private information, or taking harmful\\nactions in an environment. Our experiments minimize such risks by limiting the interactions to\\nspeciﬁc websites (Wikipedia or WebShop) that are free of private information, without any dangerous\\nactions in the action space design (i.e. models cannot really buy products on WebShop the research\\nbenchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing\\nmore extensive experiments in the future.\\nREFERENCES\\nJosh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita\\nChhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim\\nHarley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathew-\\nson, Soˇna Mokrá, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne,\\nDuncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence,\\n2020. URL https://arxiv.org/abs/2012.05672.\\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally\\nJesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka\\nRao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander\\nToshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and\\nAndy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL\\nhttps://arxiv.org/abs/2204.01691.\\nBen Alderson-Day and Charles Fernyhough.\\nInner speech: development, cognitive functions,\\nphenomenology, and neurobiology. Psychological bulletin, 141(5):931, 2015.\\nAlan Baddeley. Working memory. Science, 255(5044):556–559, 1992.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\\n10\\nPublished as a conference paper at ICLR 2023\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\\nAntonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022. URL\\nhttps://arxiv.org/abs/2208.14271.\\nAntonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large\\nlanguage models for interpretable logical reasoning, 2022. URL https://arxiv.org/abs/\\n2205.09712.\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pp. 3558–3567, Florence, Italy, July 2019. Association for Com-\\nputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/\\nP19-1346.\\nCharles Fernyhough. Vygotsky, luria, and the social brain. Self and social regulation: Social\\ninteraction and the development of social understanding and executive functions, pp. 56–79, 2010.\\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Mari-\\nbeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham,\\nJonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='beth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham,\\nJonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth\\nDathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green,\\nSoˇna Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel,\\nWilliam Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and\\nGeoffrey Irving.\\nImproving alignment of dialogue agents via targeted human judgements,\\n2022.\\nURL https://storage.googleapis.com/deepmind-media/DeepMind.\\ncom/Authors-Notes/sparrow/sparrow-final.pdf.\\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple\\nlanguage model for task-oriented dialogue. Advances in Neural Information Processing Systems,\\n33:20179–20191, 2020.\\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\\nplanners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207,\\n2022a.\\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022b.\\nSiddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. Lila: Language-informed\\nlatent actions. In CoRL, pp. 1379–1390, 2021. URL https://proceedings.mlr.press/\\nv164/karamcheti22a.html.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\\nlanguage models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-\\naugmented language models through few-shot prompting for open-domain question answering.\\narXiv preprint arXiv:2203.05115, 2022.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented genera-\\ntion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:\\n9459–9474, 2020.\\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An\\nHuang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba,\\nand Yuke Zhu. Pre-trained language models for interactive decision-making, 2022. URL https:\\n//arxiv.org/abs/2202.01771.\\n11\\nPublished as a conference paper at ICLR 2023\\nAleksandr Romanovich Luria. Ls vygotsky and the problem of localization of functions. Neuropsy-\\nchologia, 3(4):387–392, 1965.\\nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes\\ntwo to tango, 2022. URL https://arxiv.org/abs/2209.07686.\\nVincent Micheli and François Fleuret. Language models are few-shot butlers. arXiv preprint\\narXiv:2104.07972, 2021.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\\nGretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:\\nBrowser-assisted question-answering with human feedback, 2021. URL https://arxiv.\\norg/abs/2112.09332.\\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and\\nAugustus Odena. Show your work: Scratchpads for intermediate computation with language\\nmodels, 2021. URL https://arxiv.org/abs/2112.00114.\\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\\nEccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,\\nOriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https:\\n//arxiv.org/abs/2205.06175.', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https:\\n//arxiv.org/abs/2205.06175.\\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,\\nLuke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions\\nfor everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pp. 10740–10749, 2020a.\\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew\\nHausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv\\npreprint arXiv:2010.03768, 2020b.\\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston.\\nLanguage models that seek for knowledge: Modular search & generation for dialogue and prompt\\ncompletion. arXiv preprint arXiv:2203.13224, 2022a.\\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung,\\nMoya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman\\nGoyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. Blenderbot 3:\\na deployed conversational agent that continually learns to responsibly engage, 2022b. URL\\nhttps://arxiv.org/abs/2208.03188.\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale\\ndataset for fact extraction and veriﬁcation. arXiv preprint arXiv:1803.05355, 2018.\\nLev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky, 1:39–285, 1987.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,\\n2022a. URL https://arxiv.org/abs/2203.11171.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented\\nensembles in language models. arXiv preprint arXiv:2207.00747, 2022b.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\\narXiv:2201.11903, 2022.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,\\nand Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\\nanswering. arXiv preprint arXiv:1809.09600, 2018.\\n12\\nPublished as a conference paper at ICLR 2023\\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore:\\nLanguage models for action generation in text-based games. In Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Processing (EMNLP), pp. 8736–8754, Online, Novem-\\nber 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704.\\nURL https://aclanthology.org/2020.emnlp-main.704.\\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable\\nreal-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206,\\n2022.\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with\\nreasoning, 2022. URL https://arxiv.org/abs/2203.14465.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\\nOlivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in\\nlarge language models, 2022. URL https://arxiv.org/abs/2205.10625.\\nYunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information\\nseeking for open-domain question answering. arXiv preprint arXiv:2109.06747, 2021.\\n13\\nPublished as a conference paper at ICLR 2023\\nA\\nADDITIONAL RESULTS\\nA.1\\nGPT-3 EXPERIMENTS\\nPaLM-540B\\nGPT-3\\nHotpotQA (exact match)\\n29.4\\n30.8\\nALFWorld (success rate %)\\n70.9\\n78.4\\nTable 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding).\\nOn HotpotQA, we randomly sample a subset of 500 validation questions. On ALFWorld, we use all', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='70.9\\n78.4\\nTable 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding).\\nOn HotpotQA, we randomly sample a subset of 500 validation questions. On ALFWorld, we use all\\n134 unseen validation task instances, and use the best prompt set according to PaLM-540B.\\nWe run additional GPT-3 (Brown et al., 2020) experiments to conﬁrm ReAct prompting performance\\nis general across different large language models. As shown in Table 5, GPT-3 (text-davinci-002,\\ngreedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly\\nbecause it is ﬁnetuned with human instruction following. This indicates ReAct prompting is effective\\nacross different large language models on different tasks. The code for these experiments are at\\nhttps://react-lm.github.io/.\\nA.2\\nREACT OBTAINS UP-TO-DATE KNOWLEDGE ON HOTPOTQA\\n�������������\\n�������������\\n����������������������\\n������������������������� �\\n�������������������������� �\\n��������������������� �\\n���������������������� �\\n����������������� �������� �\\n�������������������������� �\\n��������������������\\n�������������\\n��������������\\n������������������������������ ��\\n���������������������������� �\\n�������������������������������\\n�����������������������\\n����������������������\\n����������������������� ��������� �\\n������������ ��\\n����������������������\\n�������������������������������� �\\n���������\\n����������������������������� �\\n�����������\\n����������������������������� �\\n����������������� �\\n����������������������\\n�������������������������\\n����������������������������������������������������������\\n�\\n���������������������������������������������������������\\n�\\n����������\\n������������������������������������������� �\\n����������������������������������������������������\\n�\\n������������������������������\\n����������������������������������������������������������\\n�\\n���������� �\\n���������������������������������������� �\\n�������������������������������� ������������������������� ���\\n��������������������������������������\\n����������������������������������������������������� �\\n�����������������������������������������������������\\n�\\n���������������������������������������������������������\\n�\\n����������������������\\n���������������������������������������������� �\\n���������������� �������������������������� ���������������� �\\n����������������\\n�����������������������������������������������������������\\n�\\n���������������������������������������\\n��������������������\\n�\\n���������������������������������������������������������\\n�\\n����������������������������������\\n���������������������\\n��������������\\n���������\\n�����������\\nFigure 4: Another example HotpotQA question, where the original label is outdated. Only ReAct is\\nable to obtain the up-to-date answer thanks to real-world web interaction plus reasoning.\\nDuring trajectory inspection, we also ﬁnd that sometimes ReAct does not agree with dataset labels as\\nthe labels themselves could be outdated. For example, as shown in Figure 4, the question asks about\\nthe size of a hotel, which increased from the HotpotQA construction time. While Standard and CoT\\ngive wrong answers due to hallucination, Act fails despite the access of real-world web interaction,\\ndue to a lack of reasoning to guide how to interact with the Internet for QA. Only ReAct is able to\\nretrieve up-to-date information from the Internet and provide a reasonable answer. Therefore, better\\nincorporation of reasoning abilities might beneﬁt recent Internet-augmented language models (Nakano\\net al., 2021; Lazaridou et al., 2022; Shuster et al., 2022a) for up-to-date task solving.\\nA.3\\nHUMAN-IN-THE-LOOP BEHAVIOR CORRECTION ON ALFWORLD\\nWe also explore human-in-the-loop interaction with ReAct, to allow a human to inspect and edit\\nReAct’s reasoning traces. Figure 5 shows that by simply removing a hallucinating sentence in Act\\n17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='ReAct’s reasoning traces. Figure 5 shows that by simply removing a hallucinating sentence in Act\\n17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align\\nwith these human thought edits and succeed in the task. From a human perspective, solving such a\\ntask becomes signiﬁcantly easier, from typing tens of actions to only editing a couple of thoughts,\\nwhich enables new forms of human-machine collaboration. We note that such a policy edit on-the-go\\n14\\nPublished as a conference paper at ICLR 2023\\n����������\\n������\\n���������������������������������������������������������������\\n�\\n�������������������������������� �\\n�����������������������������������������������������������������\\n�\\n��������������������������������������������������������������\\n��\\n������������������������������������� �\\n�����������������������������������������������������\\n�\\n������\\n����������������������������������� �\\n������������������������������������������������ �\\n�����������������������������������������������������������������\\n�\\n�����������������������������������������������������������\\n��\\n���������������������� �\\n�������������������������������������������������������\\n�\\n���������������������������������������������������������������\\n�\\n����������� �\\n������������������������������������� �\\n������������������������\\n������������������������������\\n������\\n���������������������������������������������������������������������\\n�\\n�������������������������� �\\n���������������������������������������������������������������������������\\n��\\n���������������������������������������������������\\n�\\n������������������������������������� �\\n�����������������������������������������������������\\n�\\n������\\n����������������������������������� �\\n������������������������������������������������ �\\n�������������������������������������������������������������������������\\n�\\n������������������� ������������������������������������������������\\n�\\n���������������������������������������������������������������\\n��\\n����������������������� �\\n��������������������������������������������������������������������������\\n�\\n�������������������������\\n������������������������������������������������������������������������\\n����\\n�������������������������������������� �\\n������������������������������������������������������\\n�\\n��������������������������������������������������������������������������\\n������������� �\\n��������������������\\n��������������������������������������������������������������������������\\n�\\n��������\\n�����������������������������������\\n������������������������������������������������\\n�\\n������������������������������������������������������������\\n�\\n�������������������������������������������������������������\\n�\\n�������������������������������������������������������������\\n�\\n����������������������������������������������\\n�\\n������������������������������������������\\n��������\\nFigure 5: A human-in-the-loop behavior correction example with ReAct in AlfWorld. (a) ReAct\\ntrajectory fails due to a hallucinating thought (Act 17). (b) By a human simply editing two thoughts\\n(Act 17, 23), the ReAct trajectory produces desirable reasoning traces and actions and succeeds.\\nis difﬁcult for Act and previous RL methods, as a human cannot change the model parameters, and\\nchanging a few actions might not edit the rest of the model behavior. This paradigm is also more than\\nhuman dialogue to update the goal or subgoal as in Huang et al. (2022b) — while editing ReAct\\nthoughts can do these, it can also modify the model’s internal belief, reasoning styles, or anything the\\nﬂexible thought space supports, for better task solving. We believe this is an exciting direction for\\nhuman alignment and leave more systematic study as future work.\\nB\\nEXPERIMENT DETAILS\\nB.1\\nHOTPOTQA FINETUNING DETAILS\\nFor all ﬁnetuning we use a batch size of 64. On PaLM-8B, we ﬁnetune ReAct and Act methods\\nfor 4, 000 steps and Standard and CoT methods for 2, 000 steps. On PaLM-62B, we ﬁnetune', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='For all ﬁnetuning we use a batch size of 64. On PaLM-8B, we ﬁnetune ReAct and Act methods\\nfor 4, 000 steps and Standard and CoT methods for 2, 000 steps. On PaLM-62B, we ﬁnetune\\nReAct and Act methods for 4, 000 steps and Standard and CoT methods for 1, 000 steps. We\\nﬁnd ReAct and Act methods generally beneﬁt from more training steps (and more training data),\\nwhile Standard and CoT methods degrade soon after ﬁnetuning.\\nB.2\\nALFWORLD IM-STYLE DETAILS\\nFor the IM-style ablation, the same expert trajectories used in ReAct are reannotated with dense\\nexternal feedback thoughts within these trajectories, that limit ReAct-IM to only think about (1)\\ndecomposing the current goal and (2) the current subgoal that needs to be completed. In particular,\\nReAct-IM lacks thoughts that (1) determine when a subgoal is completed (2) determine what the\\nnext subgoal should be (3) inducing the LLM to refer to its internal pretraining knowledge to identify\\nwhere items can be within the environment.\\n15\\nPublished as a conference paper at ICLR 2023\\nC\\nPROMPTS\\nC.1\\nHOTPOTQA\\nHotpot QA Prompts\\nOriginal\\nQuestion\\nWhat is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nAnswer\\n1,800 to 7,000 ft\\nQuestion\\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nAnswer\\nRichard Nixon\\nQuestion\\nWhich documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nAnswer\\nThe Saimaa Gesture\\nQuestion\\nWhat profession does Nicholas Ray and Elia Kazan have in common?\\nAnswer\\ndirector, screenwriter, actor\\nQuestion\\nWhich magazine was started first Arthur’s Magazine or First for Women?\\nAnswer\\nArthur’s Magazine\\nQuestion\\nWere Pavel Urysohn and Leonid Levin known for the same type of work?\\nAnswer\\nYes\\nAct\\nQuestion\\nWhat is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nAction 1\\nSearch[Colorado orogeny]\\nObservation 1\\nThe Colorado orogeny was an episode of mountain building (an orogeny) in\\nColorado and surrounding areas.\\nAction 2\\nLookup[eastern sector]\\nObservation 2\\n(Result 1 / 1) The eastern sector extends into the High Plains and is called\\nthe Central Plains orogeny.\\nAction 3\\nSearch[High Plains]\\nObservation 3\\nHigh Plains refers to one of two distinct land regions:\\nAction 4\\nSearch[High Plains (United States)]\\nObservation 4\\nThe High Plains are a subregion of the Great Plains.\\nFrom east to west, the\\nHigh Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\\nm).[3]\\nAction 5\\nFinish[1,800 to 7,000 ft]\\nQuestion\\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nAction 1\\nSearch[Milhouse]\\nObservation 1\\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated\\ntelevision series The Simpsons voiced by Pamela Hayden and created by Matt\\nGroening.\\nAction 2\\nLookup[named after]\\nObservation 2\\n(Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose\\nmiddle name was Milhous.\\nAction 3\\nFinish[Richard Nixon]\\nQuestion\\nWhich documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nAction 1\\nSearch[Adam Clayton Powell]\\nObservation 1\\nCould not find [Adam Clayton Powell].\\nSimilar:\\n[’Adam Clayton Powell\\nIII’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr.\\nState Office\\nBuilding’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell\\n(film)’, ’Giancarlo Esposito’].\\nAction 2\\nSearch[Adam Clayton Powell (film)]\\nObservation 2\\nAdam Clayton Powell is a 1989 American documentary film directed by\\nRichard Kilberg.\\nThe film is about the rise and fall of influential\\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired\\nas part of the PBS series The American Experience.\\nAction 3\\nFinish[The Saimaa Gesture]\\nQuestion\\nWhat profession does Nicholas Ray and Elia Kazan have in common?\\nAction 1\\nSearch[Nicholas Ray]\\nObservation 1', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='Action 3\\nFinish[The Saimaa Gesture]\\nQuestion\\nWhat profession does Nicholas Ray and Elia Kazan have in common?\\nAction 1\\nSearch[Nicholas Ray]\\nObservation 1\\nNicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,\\n1979) was an American film director, screenwriter, and actor best known for\\nthe 1955 film Rebel Without a Cause.\\nAction 2\\nSearch[Elia Kazan]\\nObservation 2\\nElia Kazan was an American film and theatre director, producer, screenwriter\\nand actor.\\nContinued on next page\\n16\\nPublished as a conference paper at ICLR 2023\\nHotpot QA Prompts – Continued from previous page\\nAction 3\\nFinish[director, screenwriter, actor]\\nQuestion\\nWhich magazine was started first Arthur’s Magazine or First for Women?\\nAction 1\\nSearch[Arthur’s Magazine]\\nObservation 1\\nArthur’s Magazine (1844-1846) was an American literary periodical published\\nin Philadelphia in the 19th century.\\nAction 2\\nSearch[First for Women]\\nObservation 2\\nFirst for Women is a woman’s magazine published by Bauer Media Group in the\\nUSA.[1] The magazine was started in 1989.\\nAction 3\\nFinish[Arthur’s Magazine]\\nQuestion\\nWere Pavel Urysohn and Leonid Levin known for the same type of work?\\nAction 1\\nSearch[Pavel Urysohn]\\nObservation 1\\nPavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet\\nmathematician who is best known for his contributions in dimension theory.\\nAction 2\\nSearch[Leonid Levin]\\nObservation 2\\nLeonid Anatolievich Levin is a Soviet-American mathematician and computer\\nscientist.\\nAction 3\\nFinish[yes]\\nCoT\\nQuestion\\nWhat is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nThought\\nLet’s think step by step.\\nThe eastern sector of Colorado orogeny extends\\ninto the High Plains.\\nHigh Plains rise in elevation from around 1,800 to\\n7,000 ft, so the answer is 1,800 to 7,000 ft.\\nAnswer\\n1,800 to 7,000 ft\\nQuestion\\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nThought\\nLet’s think step by step.\\nMilhouse was named after U.S. president Richard\\nNixon, so the answer is Richard Nixon.\\nAnswer\\nRichard Nixon\\nQuestion\\nWhich documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nThought\\nLet’s think step by step.\\nAdam Clayton Powell (film) is a documentary about\\nan African-American politician, not Finnish rock groups.\\nSo the documentary\\nabout Finnish rock groups must instead be The Saimaa Gesture.\\nAnswer\\nThe Saimaa Gesture\\nQuestion\\nWhat profession does Nicholas Ray and Elia Kazan have in common?\\nThought\\nLet’s think step by step.\\nProfessions of Nicholas Ray are director,\\nscreenwriter, and actor.\\nProfessions of Elia Kazan are director, producer,\\nscreenwriter, and actor.\\nSo profession Nicholas Ray and Elia Kazan have in\\ncommon is director, screenwriter, and actor.\\nAnswer\\ndirector, screenwriter, actor\\nQuestion\\nWhich magazine was started first Arthur’s Magazine or First for Women?\\nThought\\nLet’s think step by step.\\nArthur’s Magazine was started in 1844.\\nFirst\\nfor Women was started in 1989.\\n1844 (Arthur’s Magazine) < 1989 (First for\\nWomen), so Arthur’s Magazine was started first.\\nAnswer\\nArthur’s Magazine\\nQuestion\\nWere Pavel Urysohn and Leonid Levin known for the same type of work?\\nThought\\nLet’s think step by step.\\nPavel Urysohn is a mathematician.\\nLeonid Levin\\nis a mathematician and computer scientist.\\nSo Pavel Urysohn and Leonid\\nLevin have the same type of work.\\nAnswer\\nYes\\nReAct\\nQuestion\\nWhat is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nThought 1\\nI need to search Colorado orogeny, find the area that the eastern sector\\nof the Colorado orogeny extends into, then find the elevation range of the\\narea.\\nAction 1\\nSearch[Colorado orogeny]\\nObservation 1\\nThe Colorado orogeny was an episode of mountain building (an orogeny) in\\nColorado and surrounding areas.\\nThought 2\\nIt does not mention the eastern sector.\\nSo I need to look up eastern\\nsector.\\nAction 2\\nLookup[eastern sector]', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='Colorado and surrounding areas.\\nThought 2\\nIt does not mention the eastern sector.\\nSo I need to look up eastern\\nsector.\\nAction 2\\nLookup[eastern sector]\\nObservation 2\\n(Result 1 / 1) The eastern sector extends into the High Plains and is called\\nthe Central Plains orogeny.\\nThought 3\\nThe eastern sector of Colorado orogeny extends into the High Plains.\\nSo I\\nneed to search High Plains and find its elevation range.\\nAction 3\\nSearch[High Plains]\\nObservation 3\\nHigh Plains refers to one of two distinct land regions\\nThought 4\\nI need to instead search High Plains (United States).\\nAction 4\\nSearch[High Plains (United States)]\\nContinued on next page\\n17\\nPublished as a conference paper at ICLR 2023\\nHotpot QA Prompts – Continued from previous page\\nObservation 4\\nThe High Plains are a subregion of the Great Plains.\\nFrom east to west, the\\nHigh Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\\nm).[3]\\nThought 5\\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer\\nis 1,800 to 7,000 ft.\\nAction 5\\nFinish[1,800 to 7,000 ft]\\nQuestion\\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nThought 1\\nThe question simplifies to \"The Simpsons\" character Milhouse is named after\\nwho.\\nI only need to search Milhouse and find who it is named after.\\nAction 1\\nSearch[Milhouse]\\nObservation 1\\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated\\ntelevision series The Simpsons voiced by Pamela Hayden and created by Matt\\nGroening.\\nThought 2\\nThe paragraph does not tell who Milhouse is named after, maybe I can look up\\n\"named after\".\\nAction 2\\nLookup[named after]\\nObservation 2\\n(Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose\\nmiddle name was Milhous.\\nThought 3\\nMilhouse was named after U.S. president Richard Nixon, so the answer is\\nRichard Nixon.\\nAction 3\\nFinish[Richard Nixon]\\nQuestion\\nWhich documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nThought 1\\nI need to search Adam Clayton Powell and The Saimaa Gesture, and find which\\ndocumentary is about Finnish rock groups.\\nAction 1\\nSearch[Adam Clayton Powell]\\nObservation 1\\nCould not find [Adam Clayton Powell].\\nSimilar:\\n[’Adam Clayton Powell\\nIII’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr.\\nState Office\\nBuilding’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell\\n(film)’, ’Giancarlo Esposito’].\\nThought 2\\nTo find the documentary, I can search Adam Clayton Powell (film).\\nAction 2\\nSearch[Adam Clayton Powell (film)]\\nObservation 2\\nAdam Clayton Powell is a 1989 American documentary film directed by\\nRichard Kilberg.\\nThe film is about the rise and fall of influential\\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired\\nas part of the PBS series The American Experience.\\nThought 3\\nAdam Clayton Powell (film) is a documentary about an African-American\\npolitician, not Finnish rock groups.\\nSo the documentary about Finnish rock\\ngroups must instead be The Saimaa Gesture.\\nAction 3\\nFinish[The Saimaa Gesture]\\nQuestion\\nWhat profession does Nicholas Ray and Elia Kazan have in common?\\nThought 1\\nI need to search Nicholas Ray and Elia Kazan, find their professions, then\\nfind the profession they have in common.\\nAction 1\\nSearch[Nicholas Ray]\\nObservation 1\\nNicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,\\n1979) was an American film director, screenwriter, and actor best known for\\nthe 1955 film Rebel Without a Cause.\\nThought 2\\nProfessions of Nicholas Ray are director, screenwriter, and actor.\\nI need\\nto search Elia Kazan next and find his professions.\\nAction 2\\nSearch[Elia Kazan]\\nObservation 2\\nElia Kazan was an American film and theatre director, producer, screenwriter\\nand actor.\\nThought 3\\nProfessions of Elia Kazan are director, producer, screenwriter, and actor.\\nSo profession Nicholas Ray and Elia Kazan have in common is director,\\nscreenwriter, and actor.\\nAction 3', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='and actor.\\nThought 3\\nProfessions of Elia Kazan are director, producer, screenwriter, and actor.\\nSo profession Nicholas Ray and Elia Kazan have in common is director,\\nscreenwriter, and actor.\\nAction 3\\nFinish[director, screenwriter, actor]\\nQuestion\\nWhich magazine was started first Arthur’s Magazine or First for Women?\\nThought 1\\nI need to search Arthur’s Magazine and First for Women, and find which was\\nstarted first.\\nAction 1\\nSearch[Arthur’s Magazine]\\nObservation 1\\nArthur’s Magazine (1844-1846) was an American literary periodical published\\nin Philadelphia in the 19th century.\\nThought 2\\nArthur’s Magazine was started in 1844.\\nI need to search First for Women\\nnext.\\nAction 2\\nSearch[First for Women]\\nObservation 2\\nFirst for Women is a woman’s magazine published by Bauer Media Group in the\\nUSA.[1] The magazine was started in 1989.\\nThought 3\\nFirst for Women was started in 1989.\\n1844 (Arthur’s Magazine) < 1989 (First\\nfor Women), so Arthur’s Magazine was started first.\\nAction 3\\nFinish[Arthur’s Magazine]\\nQuestion\\nWere Pavel Urysohn and Leonid Levin known for the same type of work?\\nThought 1\\nI need to search Pavel Urysohn and Leonid Levin, find their types of work,\\nthen find if they are the same.\\nAction 1\\nSearch[Pavel Urysohn]\\nContinued on next page\\n18\\nPublished as a conference paper at ICLR 2023\\nHotpot QA Prompts – Continued from previous page\\nObservation 1\\nPavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet\\nmathematician who is best known for his contributions in dimension theory.\\nThought 2\\nPavel Urysohn is a mathematician.\\nI need to search Leonid Levin next and\\nfind its type of work.\\nAction 2\\nSearch[Leonid Levin]\\nObservation 2\\nLeonid Anatolievich Levin is a Soviet-American mathematician and computer\\nscientist.\\nThought 3\\nLeonid Levin is a mathematician and computer scientist.\\nSo Pavel Urysohn\\nand Leonid Levin have the same type of work.\\nAction 3\\nFinish[yes]\\n19\\nPublished as a conference paper at ICLR 2023\\nC.2\\nFEVER\\nFEVER Prompts\\nOriginal\\nDetermine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim\\nNikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nAnswer\\nSUPPORTS\\nClaim\\nStranger Things is set in Bloomington, Indiana.\\nAnswer\\nREFUTES\\nClaim\\nBeautiful reached number two on the Billboard Hot 100 in 2003.\\nAnswer\\nNOT ENOUGH INFO\\nAct\\nDetermine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim\\nNikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nAction2\\nSearch[Nikolaj Coster-Waldau]\\nObservation 1\\nNikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and\\nproducer.\\nHe graduated from the Danish National School of Performing Arts\\nin Copenhagen in 1993,[1] and had his breakthrough role in Denmark with\\nthe film Nightwatch (1994).\\nHe played Jaime Lannister in the HBO fantasy\\ndrama series Game of Thrones, for which he received two Primetime Emmy\\nAward nominations for Outstanding Supporting Actor in a Drama Series..\\nCoster-Waldau has appeared in numerous films in his native Denmark and\\nScandinavia, including Headhunters (2011) and A Thousand Times Good Night\\n(2013).\\nIn the U.S, his debut film role was in the war film Black Hawk Down\\n(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a\\ndetective in the short-lived Fox television series New Amsterdam (2008), and\\nappeared in the 2009 Fox television film Virtuality, originally intended as\\na pilot.\\nAction 2\\nFinish[SUPPORTS]\\nClaim\\nStranger Things is set in Bloomington, Indiana.\\nAction 1\\nSearch[Stranger Things]\\nObservation 1\\nStranger Things is an American science fiction horror drama television\\nseries created by the Duffer Brothers.\\nSet in the 1980s, primarily in\\nthe fictional town of Hawkins, Indiana, the series centers on a number of\\nmysteries and supernatural events occurring around the town and their impact\\non an ensemble of child and adult characters.\\nAction 2\\nFinish[REFUTES]\\nClaim', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='mysteries and supernatural events occurring around the town and their impact\\non an ensemble of child and adult characters.\\nAction 2\\nFinish[REFUTES]\\nClaim\\nBeautiful reached number two on the Billboard Hot 100 in 2003.\\nAction 1\\nSearch[Beautiful]\\nObservation 1\\nCould not find [Beautiful].\\nSimilar:\\n[’Beautiful’, ’Beautiful, Beautiful’,\\n’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is\\nBeautiful’].\\nAction 2\\nSearch[Beautiful (Christina Aguilera song)\\nObservation 2\\n\"Beautiful\" is a song recorded by American singer Christina Aguilera for her\\nfourth studio album, Stripped (2002).\\nAction 3\\nLookup[Billboard Hot 100]\\nObservation 3\\n(Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the\\nUnited States, where it was certified Gold for 500,000 units shipped.\\nAction 4\\nFinish[NOT ENOUGH INFO]\\nCoT\\nDetermine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim\\nNikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nThought\\nNikolaj William Coster-Waldau appeared in the 2009 Fox television film\\nVirtuality, so he has worked with the Fox Broadcasting Company.\\nAnswer\\nSUPPORTS\\nClaim\\nStranger Things is set in Bloomington, Indiana.\\nThought\\nStranger Things is in the fictional town of Hawkins, Indiana, not in\\nBloomington, Indiana.\\nAnswer\\nREFUTES\\nContinued on next page\\n20\\nPublished as a conference paper at ICLR 2023\\nFever Prompts – Continued from previous page\\nClaim\\nBeautiful reached number two on the Billboard Hot 100 in 2003.\\nThought\\nThe song peaked at number two on the Billboard Hot 100 in the United States,\\nbut not sure if it was in 2003.\\nAnswer\\nNOT ENOUGH INFO\\nReAct\\nDetermine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim\\nNikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nThought 1\\nI need to search Nikolaj Coster-Waldau and find if he has worked with the\\nFox Broadcasting Company.\\nAction 1\\nSearch[Nikolaj Coster-Waldau]\\nObservation 1\\nNikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and\\nproducer.\\nHe graduated from the Danish National School of Performing Arts\\nin Copenhagen in 1993,[1] and had his breakthrough role in Denmark with\\nthe film Nightwatch (1994).\\nHe played Jaime Lannister in the HBO fantasy\\ndrama series Game of Thrones, for which he received two Primetime Emmy\\nAward nominations for Outstanding Supporting Actor in a Drama Series..\\nCoster-Waldau has appeared in numerous films in his native Denmark and\\nScandinavia, including Headhunters (2011) and A Thousand Times Good Night\\n(2013).\\nIn the U.S, his debut film role was in the war film Black Hawk Down\\n(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a\\ndetective in the short-lived Fox television series New Amsterdam (2008), and\\nappeared in the 2009 Fox television film Virtuality, originally intended as\\na pilot.\\nThought 2\\nBecause he \"appeared in the 2009 Fox television film Virtuality\", he should\\nhave worked with the Fox Broadcasting Company.\\nAction 2\\nFinish[SUPPORTS]\\nClaim\\nStranger Things is set in Bloomington, Indiana.\\nThought 1\\nI should search for Stranger Things, and see if it is set in Bloomington,\\nIndiana.\\nAction 1\\nSearch[Stranger Things]\\nObservation 1\\nStranger Things is an American science fiction horror drama television\\nseries created by the Duffer Brothers.\\nSet in the 1980s, primarily in\\nthe fictional town of Hawkins, Indiana, the series centers on a number of\\nmysteries and supernatural events occurring around the town and their impact\\non an ensemble of child and adult characters.\\nThought 2\\nThe observation says that it is set in a \"fictional town of Hawkins,\\nIndiana\", so it is not set in Bloomington.\\nAction 2\\nFinish[REFUTES]\\nClaim\\nBeautiful reached number two on the Billboard Hot 100 in 2003.\\nThought 1\\nI need to search the song Beautiful and find if it reached number two on the\\nBillboard Hot 100 in 2003.\\nAction 1\\nSearch[Beautiful]\\nObservation 1\\nCould not find [Beautiful].\\nSimilar:', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='Thought 1\\nI need to search the song Beautiful and find if it reached number two on the\\nBillboard Hot 100 in 2003.\\nAction 1\\nSearch[Beautiful]\\nObservation 1\\nCould not find [Beautiful].\\nSimilar:\\n[’Beautiful’, ’Beautiful, Beautiful’,\\n’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is\\nBeautiful’].\\nThought 2\\nFrom suggestions, I should search \"Beautiful (Christina Aguilera song)\" to\\nfind the song.\\nAction 2\\nSearch[Beautiful (Christina Aguilera song)]\\nObservation 2\\n\"Beautiful\" is a song recorded by American singer Christina Aguilera for her\\nfourth studio album, Stripped (2002).\\nThought 3\\nIt does not mention Billboard, so I need to look up \"Billboard Hot 100\" to\\nfind if it reached number two on it in 2003.\\nAction 3\\nLookup[Billboard Hot 100]\\nObservation 3\\n(Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the\\nUnited States, where it was certified Gold for 500,000 units shipped.\\nThought 4\\nIt only says the song peaked at number two on the Billboard Hot 100, but not\\nif it was in 2003.\\nI am not sure if this claim is true or not.\\nAction 4\\nFinish[NOT ENOUGH INFO]\\n21\\nPublished as a conference paper at ICLR 2023\\nC.3\\nWEBSHOP\\nTable 6: Prompts used for the Webshop experiments in Section 4. We use the prompt on the left for\\nAct, and use the prompt on the right (add reasoning) for ReAct.\\nInstruction: i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars\\nAct\\nReAct\\nAction: search\\n�\\n3 ounce bright citrus deodorant sensitive skin\\n�\\nObservation:\\n�\\nBack to Search\\n�\\nPage 1 (Total results: 50)\\n�\\nNext\\n�\\n�\\nB078GWRC1J\\n�\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99\\n�\\nB078GTKVXY\\n�\\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99\\n�\\nB08KBVJ4XN\\n�\\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men\\n, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar\\n& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,\\n2.7 oz, 2-Pack)\\n$15.95\\nAction: click\\n�\\nB078GWRC1J\\n�\\nObservation:\\n�\\nBack to Search\\n�\\n�\\nPrev\\n�\\nscent [assorted scents][bright citrus][calming lavender][ginger\\n[fresh] simply non-scents]\\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2\\n-pack)]\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\nPrice: $10.99\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction: click[bright citrus]\\nObservation: You have clicked bright citrus.\\nAction: click[3 ounce (pack of 1)]\\nObservation: You have clicked 3 ounce (pack of 1).\\nAction: click[Buy Now]\\nAction: search\\n�\\n3 ounce bright citrus deodorant sensitive skin\\nObservation:\\n�\\nBack to Search\\n�\\nPage 1 (Total results: 50)\\n�\\nNext\\n�\\n�\\nB078GWRC1J\\n�\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99\\n�\\nB078GTKVXY\\n�\\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99\\n�\\nB08KBVJ4XN\\n�\\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men\\n, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar\\n& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,\\n2.7 oz, 2-Pack)\\n$15.95\\nAction: think[B078GWRC1J and B078GTKVXY are bright citrus\\ndeodorant less then 50 dollars. I can check B078GWRC1J ﬁrst.\\nObservation: OK.\\nAction: click\\n�\\nB078GWRC1J\\n�\\nObservation:\\n�\\nBack to Search\\n�\\n�\\nPrev\\n�\\nscent [assorted scents][bright citrus][calming lavender][ginger\\n[fresh] simply non-scents]\\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2\\n-pack)]\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\nPrice: $10.99\\nRating: N.A.\\n[Description]\\n[Features]', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='Bright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\nPrice: $10.99\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction: think[For 3 ounce bottle of bright citrus deodorant\\nfor sensitive skin, the item has options ’bright citrus’\\nand ’3 ounce (pack of 1)’ and seems good to buy.]\\nObservation: OK.\\nAction: click[bright citrus]\\nObservation: You have clicked bright citrus.\\nAction: click[3 ounce (pack of 1)]\\nObservation: You have clicked 3 ounce (pack of 1).\\nAction: click[Buy Now]\\n22\\nPublished as a conference paper at ICLR 2023\\nC.4\\nALFWORLD\\nTable 7: An Act prompt on the ALFWorld clean task. No thoughts are provided.\\nYou are in the middle of a room.\\nLooking quickly around you, you see a cabinet\\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\\nYour task is to:\\nput a clean lettuce in diningtable.\\n> go to fridge 1\\nThe fridge 1 is closed.\\n> open fridge 1\\nYou open the fridge 1.\\nThe fridge 1 is open.\\nIn it, you see a cup 3, a egg 2, a\\npotato 3, and a potato 2.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\\n> take lettuce 1 from diningtable 1\\nYou pick up the lettuce 1 from the diningtable 1.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\\n> clean lettuce 1 with sinkbasin 1\\nYou clean the lettuce 1 using the sinkbasin 1.\\n> think:\\nNow I clean a lettuce (1).\\nNext, I need to put it in/on diningtable 1.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\\nspatula 3, a tomato 2, and a tomato 1.\\n> put lettuce 1 in/on diningtable 1\\nYou put the lettuce 1 in/on the diningtable 1.\\n23\\nPublished as a conference paper at ICLR 2023\\nTable 8: An ReAct prompt on the ALFWorld clean task.\\nYou are in the middle of a room.\\nLooking quickly around you, you see a cabinet\\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\\nYour task is to:\\nput a clean lettuce in diningtable.\\n> think:\\nTo solve the task, I need to find and take a lettuce, then clean it with\\nsinkbasin, then put it in diningtable.\\nOK.\\n> think:\\nFirst I need to find a lettuce.\\nA lettuce is more likely to appear in\\nfridge (1), diningtable (1), sinkbasin (1), stoveburner (1-3), cabinet (1-13).\\nI\\ncan check one by one, starting with fridge 1.\\nOK.\\n> go to fridge 1\\nThe fridge 1 is closed.\\n> open fridge 1\\nYou open the fridge 1.\\nThe fridge 1 is open.\\nIn it, you see a cup 3, a egg 2, a\\npotato 3, and a potato 2.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\\n> think:\\nNow I find a lettuce (1).\\nNext, I need to take it.\\nOK.\\n> take lettuce 1 from diningtable 1\\nYou pick up the lettuce 1 from the diningtable 1.\\n> think:\\nNow I take a lettuce (1).\\nNext, I need to go to sinkbasin (1) and clean\\nit.\\nOK.\\n> go to sinkbasin 1', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='OK.\\n> take lettuce 1 from diningtable 1\\nYou pick up the lettuce 1 from the diningtable 1.\\n> think:\\nNow I take a lettuce (1).\\nNext, I need to go to sinkbasin (1) and clean\\nit.\\nOK.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\\n> clean lettuce 1 with sinkbasin 1\\nYou clean the lettuce 1 using the sinkbasin 1.\\n> think:\\nNow I clean a lettuce (1).\\nNext, I need to put it in/on diningtable 1.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\\nspatula 3, a tomato 2, and a tomato 1.\\n> put lettuce 1 in/on diningtable 1\\nYou put the lettuce 1 in/on the diningtable 1.\\n24\\nPublished as a conference paper at ICLR 2023\\nTable 9: An ReAct-IM prompt on the ALFWorld clean task.\\nYou are in the middle of a room.\\nLooking quickly around you, you see a cabinet\\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\\nYour task is to:\\nput a clean lettuce in diningtable.\\n> think:\\nTo solve the task, I need to find and take a lettuce, then clean it with\\nsinkbasin, then put it in diningtable.\\nOK.\\n> think:\\nFirst I need to find a lettuce.\\nOK.\\n> go to fridge 1\\nThe fridge 1 is closed.\\n> think:\\nFirst I need to find a lettuce.\\nOK.\\n> open fridge 1\\nYou open the fridge 1.\\nThe fridge 1 is open.\\nIn it, you see a cup 3, a egg 2, a\\npotato 3, and a potato 2.\\n> think:\\nFirst I need to find a lettuce.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\\n> think:\\nFirst I need to find a lettuce.\\nOK.\\n> take lettuce 1 from diningtable 1\\nYou pick up the lettuce 1 from the diningtable 1.\\n> think:\\nI need to clean this lettuce (1) using sinkbasin 1.\\nOK.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\\n> think:\\nI need to clean this lettuce (1) using sinkbasin 1.\\nOK.\\n> clean lettuce 1 with sinkbasin 1\\nYou clean the lettuce 1 using the sinkbasin 1.\\n> think:\\nI need to put this cleaned lettuce (1) in/on diningtable 1.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\\nspatula 3, a tomato 2, and a tomato 1.\\n> think:\\nI need to put this cleaned lettuce (1) in/on diningtable 1.\\nOK.\\n> put lettuce 1 in/on diningtable 1\\nYou put the lettuce 1 in/on the diningtable 1.\\nD\\nTRAJECTORIES\\nD.1\\nFEVER TRAJECTORIES\\nWe provide some trajectories produced by ReAct, CoTand Acton several randomly selected exam-\\nples from the FEVER dev set below. Search results are omitted to space.\\n25\\nPublished as a conference paper at ICLR 2023\\nExample 2491 (gt: REFUTES)\\nReAct\\nClaim:\\nBermuda Triangle is a loosely-defined region in the Pacific Ocean.\\nThought 1:\\nI need to search Bermuda Triangle and find if it is in the Pacific Ocean.\\nAction 1:\\nSearch[Bermuda Triangle]\\nObservation 1:\\nReportedly haunted locations...\\nThought 2:\\nThe observation says that it is in the western part of the North Atlantic\\nOcean, so it is not in the Pacific Ocean.\\nAction 2:\\nFinish[REFUTES]\\nObservation 2:\\nEpisode finished\\nAct\\nClaim:\\nBermuda Triangle is a loosely-defined region in the Pacific Ocean.\\nAction 1\\n:\\nSearch[Bermuda Triangle]\\nObservation 1\\n:\\nReportedly haunted locations:...\\nAction 2:\\nFinish[REFUTES]\\nObservation 2:\\nEpisode finished\\nCoT\\nClaim:\\nBermuda Triangle is a loosely-defined region in the Pacific Ocean.\\nThought:\\nLet’s think step by step.', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='Action 2:\\nFinish[REFUTES]\\nObservation 2:\\nEpisode finished\\nCoT\\nClaim:\\nBermuda Triangle is a loosely-defined region in the Pacific Ocean.\\nThought:\\nLet’s think step by step.\\nThe Bermuda Triangle is a loosely-defined region\\nin the Atlantic Ocean.\\nThe Pacific Ocean is not the Atlantic Ocean.\\nAnswer:\\nREFUTES\\nExample 5908 (gt: SUPPORTS)\\nReAct\\nClaim:\\nPrincess Mononoke is a film.\\nThought 1:\\nI need to search Princess Mononoke and find if it is a film.\\nAction 1:\\nSearch[Princess Mononoke]\\nObservation 1:\\nPrincess Mononoke ...\\nThought 2:\\nFrom the observation, it says that Princess Mononoke is a film.\\nAction 2:\\nFinish[SUPPORTS]\\nObservation 2:\\nEpisode finished\\nAct\\nClaim:\\nPrincess Mononoke is a film.\\nAction 1:\\nSearch[Princess Mononoke]\\nObservation 1:\\nPrincess Mononoke ...\\nAction 2:\\nFinish[SUPPORTS]\\nObservation 2:\\nEpisode finished\\nCoT\\nClaim:\\nPrincess Mononoke is a film.\\nThought:\\nLet’s think step by step.\\nPrincess Mononoke is a movie, and movies are\\nfilms.\\nAnswer:\\nSUPPORTS\\nExample 1951 (gt: REFUTES)\\nReAct\\nClaim:\\nSoyuz was part of the American space program.\\nThought 1:\\nI need to search Soyuz and find if it was part of the American space\\nprogram.\\nAction 1:\\nSearch[Soyuz]\\nObservation 1:\\nSoyuz is a transliteration...\\nThought 2:\\nIt does not mention the American space program, so I need to search\\n\"American space program\" to find if it was part of it.\\nAction 2:\\nSearch[American space program]\\nObservation 2:\\nThe space policy...\\nThought 3:\\nIt does not mention Soyuz, so I am not sure if this claim is true or not.\\nAction 3:\\nFinish[NOT ENOUGH INFO]\\nObservation 3:\\nEpisode finished\\nAct\\nClaim:\\nSoyuz was part of the American space program.\\nAction 1:\\nSearch[Soyuz]\\nObservation 1:\\nSoyuz is a transliteration...\\nAction 2:\\nFinish[NOT ENOUGH INFO]\\nObservation 2:\\nEpisode finished\\nCoT\\nClaim:\\nSoyuz was part of the American space program.\\nThought:\\nLet’s think step by step.\\nSoyuz is a Russian spacecraft.\\nThe American\\nspace program is NASA. NASA and Russia have worked together on the\\nInternational Space Station.\\nAnswer:\\nSUPPORTS\\n26\\nPublished as a conference paper at ICLR 2023\\nExample 3208 (gt: REFUTES)\\nReAct\\nClaim:\\nReign Over Me is an American film made in 2010.\\nThought 1:\\nI need to search for Reign Over Me and find if it is an American film made\\nin 2010.\\nAction 1:\\nSearch[Reign Over Me]\\nObservation 1:\\nReign Over Me ...\\nThought 2:\\nThe observation says that it is an American film made in 2007, so it is not\\nmade in 2010.\\nAction 2:\\nFinish[REFUTES]\\nObservation 2:\\nEpisode finished\\nAct\\nClaim:\\nReign Over Me is an American film made in 2010.\\nAction 1:\\nSearch[Reign Over Me]\\nAction 2:\\nFinish[SUPPORTS]\\nObservation 2:\\nEpisode finished\\nCoT\\nClaim:\\nReign Over Me is an American film made in 2010.\\nThought:\\nLet’s think step by step.\\nFirst, is Reign Over Me an American film?\\nYes,\\nit is.\\nSecond, was it made in 2010?\\nYes, it was.\\nAnswer:\\nSUPPORTS\\nD.2\\nALFWORLD TRAJECTORIES\\nWe provide an example trajectory of ReAct, Act, and ReAct-IM on the same ALFWorld game,\\npick_clean_then_place_in_recep-Knife-None-CounterTop-10/\\ntrial_T20190909_110445_675754.\\nD.2.1\\nREACT TRAJECTORY\\nReAct is able to correctly ﬁnd the knife, clean it, and then place it on a countertop.\\n27\\nPublished as a conference paper at ICLR 2023\\nReAct ALFWorld trajectory\\nYou are in the middle of a room.\\nLooking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to:\\nput a clean knife in countertop.\\n> think:\\nTo solve the task, I need to find and take a knife, then clean it with\\nsinkbasin, then put it in countertop.\\nOK.\\n> think:\\nFirst I need to find a knife.\\nA knife is more likely to appear in cabinet\\n(1-6), drawer (1-3), countertop (1-3), fridge (1), garbagecan (1), shelf (1-3),', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='OK.\\n> think:\\nFirst I need to find a knife.\\nA knife is more likely to appear in cabinet\\n(1-6), drawer (1-3), countertop (1-3), fridge (1), garbagecan (1), shelf (1-3),\\nsinkbasin (1), stoveburner (1-4), toaster (1).\\nI can check one by one, starting with\\ncabinet 1.\\nOK.\\n> go to cabinet 1\\nOn the cabinet 1, you see a bowl 1.\\n> go to cabinet 2\\nThe cabinet 2 is closed.\\n> go to cabinet 3\\nOn the cabinet 3, you see a glassbottle 1.\\n> go to cabinet 4\\nOn the cabinet 4, you see a mug 1.\\n> go to cabinet 5\\nThe cabinet 5 is closed.\\n> go to cabinet 6\\nThe cabinet 6 is closed.\\n> go to drawer 1\\nThe drawer 1 is closed.\\n> go to drawer 2\\nThe drawer 2 is closed.\\n> go to drawer 3\\nThe drawer 3 is closed.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n> think:\\nNow I find a knife (1).\\nNext, I need to take it.\\nOK.\\n> take knife 1 from countertop 2\\nYou pick up the knife 1 from the countertop 2.\\n> think:\\nNow I take a knife (1).\\nNext, I need to go to sinkbasin (1) and clean it.\\nOK.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a fork 3, a lettuce 3, and a spatula 2.\\n> clean knife 1 with sinkbasin 1\\nYou clean the knife 1 using the sinkbasin 1.\\n> think:\\nNow I clean a knife (1).\\nNext, I need to put it in/on countertop 1.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> put knife 1 in/on countertop 1\\nYou put the knife 1 in/on the countertop 1.\\nD.2.2\\nACT TRAJECTORY\\nAct is able to correctly ﬁnd the knife, but tries to clean it using a sinkbasin before actually going\\nto the sinkbasin. From there on, the model is unable to recover, and is stuck repeating the same\\nsequence of commands. With ReAct, an existing thought would have concretely identiﬁed that the\\nknife had been taken, and that the next subgoal was to go to a sinkbasin to clean it.\\n28\\nPublished as a conference paper at ICLR 2023\\nAct ALFWorld trajectory\\nYou are in the middle of a room.\\nLooking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to:\\nput a clean knife in countertop.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nYou pick up the knife 1 from the countertop 2.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\nD.2.3\\nREACT-IM TRAJECTORY\\nReAct-IM is able to correctly ﬁnd the knife, but is unable to clean it as well. The LLM then\\ngets stuck trying to place this knife on a countertop. One possible issue is the incorrect thought\\nthink:\\nI need to find a clean knife., which seems to trick the LLM in believing\\nthat the knife is already cleaned. In ReAct, this sequence of subgoals would be concretely captured\\nby each thought.\\n29\\nPublished as a conference paper at ICLR 2023\\nReAct-IM ALFWorld trajectory\\nYou are in the middle of a room.\\nLooking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to:\\nput a clean knife in countertop.\\n> think:\\nTo solve the task, I need to find and take a clean knife, then put it in\\ncountertop.\\nOK.\\n> think:\\nI need to find a clean knife.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> think:\\nI need to find a clean knife.\\nOK.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n> think:\\nI need to find a clean knife.\\nOK.\\n> take knife 1 from countertop 2\\nYou pick up the knife 1 from the countertop 2.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nYou put the knife 1 in/on the countertop 1.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think:\\nI need to put this knife (1) in/on countertop 1.\\nOK.\\n30\\nPublished as a conference paper at ICLR 2023\\nD.3\\nWEBSHOP TRAJECTORIES\\nTable 10: Example trajectories for Webshop predicted by Act and ReAct. Compared to Act,\\nReAct uses reasoning to ﬁnd products that satisfy all target attributes.\\nInstruction: get me a sixteen pack of apple cinnamon freeze dried banana chips, and price lower than 50.00 dollars\\nAct\\nReAct\\nAction: search\\n�\\nsixteen pack apple cinnamon freeze dried banana\\nchips\\n�', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='Instruction: get me a sixteen pack of apple cinnamon freeze dried banana chips, and price lower than 50.00 dollars\\nAct\\nReAct\\nAction: search\\n�\\nsixteen pack apple cinnamon freeze dried banana\\nchips\\n�\\nObservation:\\n�\\nBack to Search\\n�\\nPage 1 (Total results: 50)\\n�\\nNext\\n�\\n�\\nB0061IVFZE\\n�\\nBrothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -\\nOunce (Pack of 100)\\n$85.0\\n�\\nB096H2P6G2\\n�\\nMoon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -\\n(Variety Pack)\\n$18.99\\n�\\nB092JLLYK6\\n�\\nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\\nFor School Lunches or an On-The-Go Snack - No Sugar Added,\\nNon GMO, Gluten Free, Nothing Artiﬁcial (0.53oz) 6-Pack\\n$12.99\\nAction: click\\n�\\nB0061IVFZE\\n�\\nObservation:\\n�\\nBack to Search\\n�\\n�\\nPrev\\n�\\nﬂavor name [asian pear][banana][fuji apple & cinnamon][strawberry\\nbanana] Brothers-ALL-Natural Fruit Crisps, Strawberry Banana,\\n0.42 Ounce (Pack of 100)\\nPrice: $85.0\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction: click[Buy Now]\\nAction: search\\n�\\nsixteen pack apple cinnamon freeze dried banana\\nchips\\n�\\nObservation:\\n�\\nBack to Search\\n�\\nPage 1 (Total results: 50)\\n�\\nNext\\n�\\n�\\nB0061IVFZE\\n�\\nBrothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -\\nOunce (Pack of 100)\\n$85.0\\n�\\nB096H2P6G2\\n�\\nMoon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -\\n(Variety Pack)\\n$18.99\\n�\\nB092JLLYK6\\n�\\nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\\nFor School Lunches or an On-The-Go Snack - No Sugar Added,\\nNon GMO, Gluten Free, Nothing Artiﬁcial (0.53oz) 6-Pack\\n$12.99\\nAction: think[B0061IVFZE is strawberry banana, not apple cinnamon.\\nB096H2P6G2 is fruit snacks, not freeze dried banana chips.\\nB092JLLYK6 is banana crisps, not apple cinnamon. I can check\\nB092JLLYK6 ﬁrst.\\nObservation: OK.\\nAction: click\\n�\\nB092JLLYK6\\n�\\nObservation:\\n�\\nBack to Search\\n�\\n�\\nPrev\\n�\\nﬂavor name [apple][apple cinnamon][banana][cantaloupe][peach][pear]\\n[strawberry][strawberry banana][sampler variety pack][mega variety\\npack][orchard variety pack]\\nsize [0.53 ounce (pack of 6)][0.53 ounce (pack of 8)][0.53 ounce (pack\\nof 16)]\\nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\\nFor School Lunches or an On-The-Go Snack - No Sugar Added, Non\\nGMO, Gluten Free, Nothing Artiﬁcial (0.53oz) 6-Pack\\nPrice: $12.99\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction: think[For sixteen pack of apple cinnamon freeze dried banana\\nchips, the item has options ’apple cinnamon’ and ’0.53 ounce (pack of\\n16)’ and seems good to buy.]\\nObservation: OK.\\nAction: click[apple cinnamon]\\nObservation: You have clicked apple cinnamon.\\nAction: click[0.53 ounce (pack of 16)]\\nObservation: You have clicked 0.53 ounce (pack of 16).\\nAction: click[Buy Now]\\nScore: 0.125\\nScore: 1.0\\n31\\nPublished as a conference paper at ICLR 2023\\nE\\nMORE ANALYSIS\\nE.1\\nSUCCESS AND FAILURE MODES ANALYSIS\\nWe provide some examples corresponding to the success and error mode analysis given in Sec. 3.3.\\nSearch results and non-representative steps are omitted to reduce space.\\nSuccess: True positive\\nReAct\\nQuestion:\\nAuthor David Chanoff has collaborated with a U.S. Navy admiral who served as\\nthe ambassador to the United Kingdom under which President?\\nThought 1:\\nI need to search David Chanoff and find the U.S. Navy admiral he\\ncollaborated with.\\nAction 1:\\nSearch[David Chanoff]\\nObservation 1:\\nDavid Chanoff is a noted author...\\nAction 2:\\nSearch[William J. Crowe]\\nObservation 2:\\nWilliam J. Crowe is...\\nThought 3:\\nWilliam J. Crowe served as the ambassador to the United Kingdom under\\nPresident Bill Clinton.\\nSo the answer is Bill Clinton.\\nAction 3:\\nFinish[Bill Clinton]\\nObservation 3:\\nEpisode finished\\nCoT\\nQuestion:\\nWhat was the name of the team that represented an Arizona university in\\nthe conference that covers a broad expanse of the western United States,\\nincluding Arizona, California, and New Mexico?\\nThought:\\nLet’s think step by step.\\nThe name of the team that represented an Arizona\\nuniversity in the conference that covers a broad expanse of the western', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='Thought:\\nLet’s think step by step.\\nThe name of the team that represented an Arizona\\nuniversity in the conference that covers a broad expanse of the western\\nUnited States, including Arizona, California, and New Mexico is Arizona\\nState Sun Devils.\\nAnswer:\\nArizona State Sun Devils\\nSuccess: False positive\\nReAct\\nQuestion:\\nDid the board game San Marco or About Time come out first?\\nThought 1:\\nI need to search San Marco and About Time, and find which came out first.\\nAction 1:\\nSearch[San Marco]\\nObservation 1:\\nSan Marco is one of the ...\\nThe was built in venice.\\nThought 2:\\nSan Marco is a sestiere of Venice.\\nI need to search About Time next.\\nAction 2:\\nSearch[About Time]\\nObservation 2:\\nCould not find [About Time].\\nSimilar:\\n[...].\\nThought 3:\\nAbout Time is a 2013 film.\\nSo San Marco came out first.\\nAction 3:\\nFinish[San Marco]\\nObservation 3:\\nEpisode finished\\nCoT\\nQuestion:\\nWas Vanderbilt University or Emory University founded first?\\nThought:\\nLet’s think step by step.\\nVanderbilt University was founded in 1873.\\nEmory\\nUniversity was founded in 1836.\\n1873 (Vanderbilt University) > 1836 (Emory\\nUniversity), so Vanderbilt University was founded first.\\nAnswer:\\nEmory University\\nFailure: Reasoning error\\nReAct\\nQuestion:\\nWhat was the middle name of the actress who was cast alongside Harvey\\nFierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura\\nBell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur in\\nHairspray?\\nThought 1:\\nI need to search Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell,\\nMary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and\\nMarissa Jaret Winokur, find the actress that was cast alongside them in\\nHairspray, then find its middle name.\\nAction 1:\\nSearch[Harvey Fierstein]\\nObservation 1:\\nHarvey Forbes Fierstein ...\\nThought 2:\\nHarvey Fierstein is an actor, playwright, and screenwriter.\\nI need to\\nsearch Linda Hart next...\\nAction 2:\\nSearch[Linda Hart]...\\nCoT\\nQuestion:\\nWho is older Danny Green or James Worthy?\\nThought:\\nLet’s think step by step.\\nDanny Green was born in 1987.\\nJames Worthy was\\nborn in 1961.\\n1987 (Danny Green) > 1961 (James Worthy), so Danny Green is\\nolder.\\nAnswer:\\nDanny Green\\n32\\nPublished as a conference paper at ICLR 2023\\nFailure: Search error\\nReAct\\nQuestion:\\nWhat does the goddess associated with the goddess frigg consists of what\\ntales?\\nThought 1:\\nI need to search goddess frigg and find its associated goddess.\\nAction 1:\\nSearch[goddess frigg]\\nObservation 1:\\nCould not find goddess frigg.\\nSimilar:\\n[’Frigg’, ’Frigg and Freyja common\\norigin hypothesis’, ’Freyja (section Relation to Frigg and other goddesses\\nand figures)’, ’Eir (redirect from Eir (goddess))’, ’Baldr’].\\nThought 2:\\nI need to search Frigg instead...\\nFailure: Hallucination\\nCoT\\nQuestion:\\nWhat was the first year a scientific journal published by an organization\\nlocated in the Bronx was published?\\nThought:\\nLet’s think step by step.\\nThe first year a scientific journal published by\\nan organization located in the Bronx was published was 1916.\\nAnswer:\\n1916 (Label:\\n1909)\\nFailure: Label ambiguity\\nReAct\\nQuestion:\\nWhat nationality is the man who developed the dictator game?\\nThought 1:\\nI need to search dictator game, find the man who developed it, then find his\\nnationality.\\nAction 1:\\nSearch[dictator game]\\nObservation 1:\\nThe dictator game is a popular experimental instrument in social psychology\\nand economics...\\nAction 3:\\nSearch[Amos Tversky]\\nObservation 3:\\nAmos Nathan Tversky...\\nThought 4:\\nAmos Tversky is an Israeli cognitive and mathematical psychologist.\\nSo the\\nman who developed the dictator game is Israeli.\\nAction 4:\\nFinish[Israeli] (label:\\nIsrael-American)\\nCoT\\nQuestion:\\nKurt Fuller from Stockton, CA appeared in which American Detective Comedy\\nDrama television series created by Steve Franks?\\nThought:\\nLet’s think step by step.\\nKurt Fuller from Stockton, CA appeared in Psych.\\nPsych is an American Detective Comedy Drama television series created by\\nSteve Franks.\\nAnswer:\\nPsych (label:', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}),\n",
       " Document(page_content='Thought:\\nLet’s think step by step.\\nKurt Fuller from Stockton, CA appeared in Psych.\\nPsych is an American Detective Comedy Drama television series created by\\nSteve Franks.\\nAnswer:\\nPsych (label:\\nPsych is an American detective comedy-drama)\\n33', metadata={'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'})]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1777bb47-329b-4236-9336-ebd20d412319",
   "metadata": {},
   "source": [
    "### Wikipediaの検索結果を取得するLoader\n",
    "WikipediaLoaderは、wikipediaをインストールする必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b1b9002-6202-4bab-883e-590708f1e4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f5b5a4ba-f76b-467f-b96f-3206ab20b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "wikipedia_loader = WikipediaLoader(query=\"ChatGPT\", lang=\"ja\", load_max_docs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca739413-cbd6-4011-928c-603ec846843e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='ChatGPT（チャットジーピーティー、英語: Chat Generative Pre-trained Transformer）は、OpenAIが2022年11月に公開した人工知能チャットボットであり、生成AIの一種。\\nGPTの原語のGenerative Pre-trained Transformerとは、「生成可能な事前学習済み変換器」という意味である。OpenAIのGPT-3ファミリーの大規模な言語モデルに基づいて構築されており、教師あり学習と強化学習の両方の手法を使って転移学習され、機械学習のサブセットである深層学習を使って開発されている。\\n\\n\\n== 概要 ==\\n2022年11月30日にプロトタイプとして公開され、幅広い分野の質問に詳細な回答を生成できることから注目を集めた。しかし、人間が自然と感じる回答の生成を特徴としていることから、一見自然に見えるが事実とは異なる回答を生成することもあり、大きな欠点とされた。ChatGPTのリリース後、OpenAIの評価額は290億米ドルとなり、2021年時の140億ドルと比べて2倍以上に増加した。OpenAIの筆頭株主であるMicrosoftが展開する検索エンジンのBingやウェブブラウザのEdge等の幅広いサービスの生成AIの技術にも、ChatGTが活用されている。\\nスイスの金融グループUBSの分析によると、ChatGPTがアクティブユーザー数1億人に到達するのにかかった時間は、TikTokとInstagramがそれぞれ9カ月と2年半であるのに対して、2か月と史上最速である。\\nOpenAIによれば、オプトアウトを申請しない限り、ユーザーが入力したChatGPTとの会話内容はOpenAIによってその内容を利用されることがある。このため、一般的にChatGPTの利用に際して個人情報と機密情報の入力に注意する必要があり、Apple、JPモルガン、サムスン電子など一部の企業はChatGPTへのアクセスを禁止している。\\nChatGPTによる個人情報の収集に関しては、EU一般データ保護規則に違反している疑惑も浮上しており、2023年3月31日、イタリア政府はChatGPTの訓練のために個人情報を大量に収集することを正当化する法的根拠がないと指摘して、ChatGPTへのアクセスを一時禁止した。米国では、個人情報収集や著作権の問題により、ChatGPTの運営元企業OpenAIに対して複数の訴訟が提起されている。\\nまた、ChatGPTは原則として犯罪に利用される回答を生成しないと謳っているが、回避ワード（例:Uncensored等）を入力すると、犯罪に利用できる回答が生成されてしまうという欠点が指摘されている。\\n\\n\\n== トレーニング ==\\nChatGPTは、教師あり学習と強化学習を使用して、GPT-3.5上で微調整された。どちらのアプローチも、人間のトレーナーを使用してモデルのパフォーマンスを改善した。教師あり学習の場合、モデルには、トレーナーがユーザーとAIアシスタントの両方を演じる会話が提供された。強化ステップでは、まず人間のトレーナーが、モデルが以前の会話で作成した応答をランク付けした。これらのランキングは、 Proximal Policy Optimization（PPO）を数回繰り返してモデルをさらに微調整した「報酬モデル」を作成するために使用された。近接ポリシー最適化アルゴリズムは、信頼領域ポリシー最適化アルゴリズムに費用対効果の高い利点をもたらした。より高速なパフォーマンスで、計算コストの高い操作の多くを無効にする。モデルは、Azureを使用したスーパーコンピューティングインフラストラクチャでMicrosoftと共同でトレーニングされた。\\nさらに、OpenAIは、ChatGPTのさらなるトレーニングと微調整に使用できるChatGPTユーザーからのデータを引き続き収集する。ユーザーは、ChatGPTから受け取った応答に対して賛成票または反対票を投じることができる。賛成票または反対票を投じると、テキストフィールドに追加のフィードバックを入力することもできる。\\n\\n\\n== 機能 ==\\nチャットボットの主要機能は人間同士の対話を模倣することであるが、ChatGPTについてはそれを越える汎用的かつ即興的な機能が備わっているとされ、話題となった。ChatGPTは、マルバツゲームの相手をしたり、Linuxシステムをエミュレートすることができたり、プログラミングやデバッグが行うことができる。また、音楽、小説、脚本、詩、歌詞や作文などの創作活動もできる。その上、特定のテストに対して、人間と同水準かそれ以上の回答ができることがあるなど、幅広い機能を備えている。\\n前作のInstructGPTと比べ、ChatGPTは攻撃的・欺瞞的な回答の生成をできるだけ避ける仕様となっている。学習データにはmanページ、Pythonや電子掲示板など、プログラミング言語やインターネット現象についても含まれている。\\nほとんどのチャットボットとは対照的に、ChatGPTは会話内での利用者による過去の入力を記憶している。これにより、ChatGPTが個人に最適化されたセラピストとして使える可能性があることが指摘されている。攻撃的な回答が生成されるのを防ぐため、ユーザーの入力とChatGPTの生成した回答はOpenAIのコンテンツモデレーションAPIによってフィルターされており、人種差別的・性差別的な入力への回答はAPIによって拒否される仕様になっている。\\n機能は豊富なものの、複数の欠点も有る。OpenAIはChatGPTが「時によっては、もっともらしく見えるが誤っている回答を作成する」ことを認めている。ChatGPTの報酬モデルは人間による監視を中心としているため、最適化されすぎてパフォーマンスに影響を及ばしてしまう（グッドハートの法則）。それに加え、ChatGPTは2021年10月以降に発生した出来事については知識が備えられておらず、一部の著名人については知識が全く無いことも有る。 \\nBBCによると、2022年12月現在でChatGPTは政治的な意見を表明しない仕様である。ChatGPTの学習中、人間の「教師」は回答の正当性などに関係なく長い回答を好んでいた。また、訓練データはアルゴリズム的バイアスがあり、時によって人種差別的や性差別的な回答を生成させることにもつながったと言われている。例として、有色人種や女性の科学者は白人男性の科学者よりも優れている、といった内容のラップを生成したことがあった。\\n\\n\\n== サービス ==\\nChatGPTは2022年11月30日に公開された。当初は無料で公開されたが、後に有料化する計画があるという。開発元のOpenAIは同年12月4日までにユーザー数が100万を突破したと見積もった。12月15日、CNBCはサーバーが「未だ時々ダウンする」と報告した。ChatGPTは主に英語での使用を想定しているが、他の言語でもある程度は機能することができる。2022年現在、最近話題となったAIとは対照的に、ChatGPTに関する査読済みの技術論文は無い。\\nOpenAIの客員研究員であるスコット・アーロンソンは、ChatGPTの悪用を防ぐため、同社がChatGPTの文書生成システムに何らかの形で透かしを入れる機能を開発していると明かした。また、ニューヨーク・タイムズは、ChatGPTの後継として、GPT-4が2023年内に公開されることが「噂されている」と報じた。\\n2023年2月1日、有料版として「ChatGPT Plus」が発表された。アクセス集中時にも優先的にアクセスできるようになっており、反応の高速化や新機能の使用ができる。アメリカから順次サービスが提供される。\\n2023年3月15日、「GPT-4」が有料版として公開された。\\n\\n\\n== 反響 ==\\nニューヨーク・タイムズはChatGPTを「今まで公衆に公開されてきたチャットボットの中で一番良いものである」と評し、英ガーディアンはChatGPTが「驚くほど詳細」でまるで「人間のような」回答を生成することができると記した。ダン・ギルモアはChatGPTを生徒の課題に使い、出力結果が優秀な生徒による回答と同レベルであることを発見した上で、「学界は大きな問題に直面している」と意見した。また、アメリカ合衆国に拠点を置く雑誌『Slate』は、ChatGPTが、ユーザーがナチス・ドイツに関する質問を入力した際に生成する回答を賛美した。同雑誌は例として、アドルフ・ヒトラーがアウトバーンを建設したというナチス・ドイツを賛美する内容の入力に対し、ChatGPTがナチス・ドイツによる強制労働について解説する回答を生成したことを挙げた。\\nVox Mediaのケルシー・パイパーは、ChatGPTがAIの進化ぶりを公衆に可視化したことで公衆が唖然としたことを指摘し、欠点を差し引いてもChatGPTは高い頭脳を有していると評価した。Yコンビネータのポール・グレアムは「ChatGPTに圧倒されている人たちが、新しい物全部にキャーキャー言う人だけではないのが印象的だ。確実に何か大きいことが起こっている」とツイートした。開発元であるOpenAIの設立者の一人でもあるイーロン・マスクは、「ChatGPTは恐ろしいほど良い。危険なほどのAIも遠くない」と書いた。また、彼は「OpenAIはオープンソースかつ非営利団体として設立されたが、今ではそれが変わっている」と主張し、同社によるTwitterのデータベースへのアクセスを', metadata={'title': 'ChatGPT', 'summary': 'ChatGPT（チャットジーピーティー、英語: Chat Generative Pre-trained Transformer）は、OpenAIが2022年11月に公開した人工知能チャットボットであり、生成AIの一種。\\nGPTの原語のGenerative Pre-trained Transformerとは、「生成可能な事前学習済み変換器」という意味である。OpenAIのGPT-3ファミリーの大規模な言語モデルに基づいて構築されており、教師あり学習と強化学習の両方の手法を使って転移学習され、機械学習のサブセットである深層学習を使って開発されている。', 'source': 'https://ja.wikipedia.org/wiki/ChatGPT'}),\n",
       " Document(page_content='CNET Networks, Inc.（シーネット ネットワークス）は、アメリカ合衆国カリフォルニア州サンフランシスコに本社を置くIT分野に特化したメディア企業。日本版は朝日新聞社子会社の朝日インタラクティブが運営している。1993年にハルシー・マイナーとシェルビィ・ボニーによって設立された。2008年、CBSコーポレーション（現・パラマウント・グローバル）が子会社化した。\\n\\n\\n== 沿革 ==\\n1994年、CNETはコンピュータ/テクノロジー/インターネットをテーマとしたテレビ番組、CNET Central、The Web、The New Edgeを制作した。このうちCNET CentralがまずUSAネットワークで放送開始された。その後、SciFiチャンネルでThe WebとThe New Edgeの放送が開始された。1996年にはTV.comが放送開始されている。このように元々はテレビ番組制作会社であり、インターネット上のサイト運営はそれに付随した業務だった。\\nさらにCNETは、別のテレビ番組News.comを1999年からCNBCで放送している。\\n1999年、CNETはスイスを本拠地とする企業GDTを買収した。\\n2000年1月、CNETはCNET Networksと改称した。同時に価格比較サイトmySimonを7億ドルで買収した。\\n2000年10月、CNETはZDNetを約16億ドルで買収した。2001年1月、ジフ・デイビスとCNETはソフトバンクにジフ・デイビスが売却したURLを取り戻すことで合意した。2001年4月、CNETはガートナーがIT専門家向けに運営していたTechRepublic Inc.を2300万ドルで買収した。2004年7月14日、CNETは写真サイトとして有名なWebshotsを7000万ドルで買収することを発表した。\\n2001年から2003年まで、CNETはCNET Radioというラジオ番組をサンフランシスコとボストンで放送していた。テクノロジーをテーマとした番組だったが、聴取率が悪く、CNET自体の財政問題もあって2003年1月に放送終了となった。\\n1996年から2003年の間に行われたストックオプションの不正が発覚した結果、2006年10月11日、Shelby BonnieがCEOを辞任した。後任としてNeil AsheがCEOに就任した。\\n2006年12月、James Kim（CNETの編集者）がオレゴン州の荒野で死亡した。CNETは彼に捧げる記念のショーとポッドキャストを主催した。\\n2007年3月1日、CNETはビジネスマン（マネージャレベル）向けのBNETの立ち上げを発表した。ただしベータ版としては2005年から運営されている。\\n2023年1月、Futurism.comの調査により、CNETが2022年11月からChatGPTにCNET Moneyの名義で記事を書かせていたことが判明した。検索エンジン最適化が目的とみられるが、ChatGPTが執筆した記事には誤りが多数掲載されていたため、正確性が疑問視されている。\\n\\n\\n=== CBSによる買収 ===\\n2008年5月15日、CBSコーポレーション（現・パラマウント・グローバル）はCNETを18億USドルで買収することを発表した。2008年6月30日、買収が完了した。かつてのCNETの資産は、現在はレッド・ベンチャーズの子会社となっている。\\n\\n\\n== 日本における展開 ==\\n日本では、1997年にテレビ版をテレビ東京がダイジェスト版として放送した。同年より2002年末までNTTPCコミュニケーションズがライセンス供与を受け日本語版ウェブサイトを運営した。2002年8月、CNET Networks, Inc.がバックテクノロジーズ株式会社（御手洗大祐社長、当時）を買収し、2003年1月、社名をシーネットネットワークスジャパン株式会社と変更して、日本語版ウェブサイトの運営を引き継いだ。\\nCNETは2000年、IT系ニュースサイトとして当時最大のライバルであったZDNetを買収したが、以降も「ZDNet Japan」はソフトバンク・ジーディーネット（現・ITmedia）による運営が続き、2004年1月、ライセンス契約期限切れに伴いサイト名を「ZDNet Japan」から「ITmedia」へ変更した。2005年4月、シーネットネットワークスジャパンにより「ZDNet Japan」が改めて開設された。\\n2009年7月1日、朝日新聞社が、シーネットネットワークスジャパンが運営する「CNET Japan」「ZDNet Japan」「GameSpot Japan」「鉄道コム」などのサイト運営事業を買収し、事業承継すると発表された。同年9月1日以降、シーネットネットワークスジャパンから新設分割によって設立された朝日新聞社の完全子会社、朝日インタラクティブ株式会社によって運営されている。朝日新聞デジタル（旧asahi.com）、CNET Japanの双方で互いの主要記事へのリンクが掲載される。\\nGameSpot Japanは2012年12月17日をもって終了し、過去の記事も閲覧できなくなった。以降はCNET Japanにコンピュータゲーム関連の記事が掲載されている。\\n\\n\\n== 運営サイト ==\\nCNET\\nBNET\\nNews.com\\nZDNet\\nMP3.com\\nDownload.com\\nGameSpot\\nTV.com\\nCHOW\\n\\n\\n== 脚注 ==\\n\\n\\n== 外部リンク ==\\nCNET（英語）\\n朝日インタラクティブ\\nCNET Japan\\nZDNet Japan\\nbuilder by ZDNet Japan\\nCNET.com - テレビ東京リンクサイト', metadata={'title': 'CNET', 'summary': 'CNET Networks, Inc.（シーネット ネットワークス）は、アメリカ合衆国カリフォルニア州サンフランシスコに本社を置くIT分野に特化したメディア企業。日本版は朝日新聞社子会社の朝日インタラクティブが運営している。1993年にハルシー・マイナーとシェルビィ・ボニーによって設立された。2008年、CBSコーポレーション（現・パラマウント・グローバル）が子会社化した。', 'source': 'https://ja.wikipedia.org/wiki/CNET'})]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = wikipedia_loader.load()\n",
    "wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5525166-50ba-446a-a986-c2677bbb5490",
   "metadata": {},
   "source": [
    "### ウェブページを取得\n",
    "WebBaseLoaderを使うことでウェブページを読み込みができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04b76005-7aea-4e76-9878-789070f29bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "web = WebBaseLoader(\"https://ja.wikipedia.org/wiki/ChatGPT\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a06e3692-57a0-4402-809e-f896b53f82d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\nChatGPT - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nコンテンツにスキップ\\n\\n\\n\\n\\n\\n\\n\\nメインメニュー\\n\\n\\n\\n\\n\\nメインメニュー\\nサイドバーに移動\\n非表示\\n\\n\\n\\n\\t\\t案内\\n\\t\\n\\n\\nメインページコミュニティ・ポータル最近の出来事新しいページ最近の更新おまかせ表示練習用ページアップロード (ウィキメディア・コモンズ)\\n\\n\\n\\n\\n\\n\\t\\tヘルプ\\n\\t\\n\\n\\nヘルプ井戸端お知らせバグの報告寄付ウィキペディアに関するお問い合わせ\\n\\n\\n\\n\\n\\n言語\\n\\n言語間リンクはページの先頭にあるページ名の向かい側に設置されています。\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n検索\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n検索\\n\\n\\n\\n\\n\\n\\n\\n\\nアカウント作成ログイン\\n\\n\\n\\n\\n\\n\\n個人用ツール\\n\\n\\n\\n\\n\\n アカウント作成 ログイン\\n\\n\\n\\n\\n\\n\\t\\tログアウトした編集者のページ もっと詳しく\\n\\n\\n\\n投稿記録トーク\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n目次\\nサイドバーに移動\\n非表示\\n\\n\\n\\n\\nページ先頭\\n\\n\\n\\n\\n\\n1概要\\n\\n\\n\\n\\n\\n\\n\\n2トレーニング\\n\\n\\n\\n\\n\\n\\n\\n3機能\\n\\n\\n\\n\\n\\n\\n\\n4サービス\\n\\n\\n\\n\\n\\n\\n\\n5反響\\n\\n\\n\\n\\n\\n\\n\\n6批判\\n\\n\\n\\n\\n\\n\\n\\n7学術界の反応\\n\\n\\n\\n学術界の反応サブセクションを切り替えます\\n\\n\\n\\n\\n\\n7.1東京大学\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n8問題\\n\\n\\n\\n問題サブセクションを切り替えます\\n\\n\\n\\n\\n\\n8.1スパム・誤情報への悪用\\n\\n\\n\\n\\n\\n\\n\\n8.2機密情報の流出\\n\\n\\n\\n\\n\\n\\n\\n8.3データ倫理・著作権\\n\\n\\n\\n\\n\\n\\n\\n8.4個人情報保護\\n\\n\\n\\n\\n\\n\\n\\n8.5データのラベリング\\n\\n\\n\\n\\n\\n\\n\\n8.6脱獄\\n\\n\\n\\n\\n\\n\\n\\n8.7中立性\\n\\n\\n\\n\\n\\n\\n\\n8.8社会倫理\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n9脚注\\n\\n\\n\\n脚注サブセクションを切り替えます\\n\\n\\n\\n\\n\\n9.1出典\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n10関連項目\\n\\n\\n\\n\\n\\n\\n\\n11外部リンク\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n目次の表示・非表示を切り替え\\n\\n\\n\\n\\n\\n目次の表示・非表示を切り替え\\n\\n\\n\\n\\n\\n\\n\\nChatGPT\\n\\n\\n\\n96の言語版\\n\\n\\n\\n\\nAfrikaansAragonésÆngliscالعربيةالدارجةঅসমীয়াAzərbaycancaتۆرکجهBasa BaliBoarischБеларускаяБългарскиभोजपुरीবাংলাBrezhonegBosanskiCatalà閩東語 / Mìng-dĕ̤ng-ngṳ̄کوردیČeštinaDanskDeutschΕλληνικάEnglishEsperantoEspañolEestiEuskaraفارسیSuomiFrançaisGaeilgeGalegoAvañe\\'ẽHausaעבריתहिन्दीFiji HindiHrvatskiMagyarՀայերենInterlinguaBahasa IndonesiaItalianoҚазақшаಕನ್ನಡ한국어कॉशुर / کٲشُرKurdîКыргызчаLietuviųLatviešuमैथिलीമലയാളംꯃꯤꯇꯩ ꯂꯣꯟBahasa Melayuမြန်မာဘာသာनेपालीNederlandsNorsk nynorskNorsk bokmålOccitanଓଡ଼ିଆਪੰਜਾਬੀPolskiPortuguêsRuna SimiRomânăРусскийСаха тылаᱥᱟᱱᱛᱟᱲᱤSimple EnglishSlovenčinaSlovenščinaSoomaaligaShqipСрпски / srpskiSvenskaதமிழ்తెలుగుไทยTagalogTürkçeئۇيغۇرچە / UyghurcheУкраїнськаاردوOʻzbekcha / ўзбекчаTiếng Việt吴语მარგალურიVahcuengh中文文言Bân-lâm-gú粵語IsiZulu\\n\\nリンクを編集\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nページノート\\n\\n\\n\\n\\n\\n日本語\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n閲覧ソースを閲覧履歴表示\\n\\n\\n\\n\\n\\n\\n\\nツール\\n\\n\\n\\n\\n\\nツール\\nサイドバーに移動\\n非表示\\n\\n\\n\\n\\t\\t操作\\n\\t\\n\\n\\n閲覧ソースを閲覧履歴表示\\n\\n\\n\\n\\n\\n\\t\\t全般\\n\\t\\n\\n\\nリンク元関連ページの更新状況ファイルをアップロード特別ページこの版への固定リンクページ情報このページを引用短縮URLを取得するウィキデータ項目\\n\\n\\n\\n\\n\\n\\t\\t印刷/書き出し\\n\\t\\n\\n\\nブックの新規作成PDF 形式でダウンロード印刷用バージョン\\n\\n\\n\\n\\n\\n\\t\\t他のプロジェクト\\n\\t\\n\\n\\nコモンズ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n出典: フリー百科事典『ウィキペディア（Wikipedia）』\\n\\n\\n\\n\\nChatGPT\\n開発元\\nOpenAI初版\\n2022年11月30日 (10か月前)\\xa0(2022-11-30)\\n最新版\\n2023年5月12日 (5か月前)\\xa0(2023-05-12)\\n使用エンジン\\nGPT-3.5GPT-4種別\\n人工知能ライセンス\\nプロプライエタリソフトウェア公式サイト\\nchat.openai.comテンプレートを表示\\nChatGPT（チャットジーピーティー、英語: Chat Generative Pre-trained Transformer）[1]は、OpenAIが2022年11月に公開した人工知能チャットボットであり、生成AIの一種。\\nGPTの原語のGenerative Pre-trained Transformerとは、「生成可能な事前学習済み変換器」という意味である[2]。OpenAIのGPT-3ファミリーの大規模な言語モデルに基づいて構築されており、教師あり学習と強化学習の両方の手法を使って転移学習され、機械学習のサブセットである深層学習を使って開発されている[3]。\\n\\n\\n概要\\n2022年11月30日にプロトタイプとして公開され、幅広い分野の質問に詳細な回答を生成できることから注目を集めた。しかし、人間が自然と感じる回答の生成を特徴としていることから、一見自然に見えるが事実とは異なる回答を生成することもあり[4][5]、大きな欠点とされた[6]。ChatGPTのリリース後、OpenAIの評価額は290億米ドルとなり[7]、2021年時の140億ドルと比べて2倍以上に増加した[8]。OpenAIの筆頭株主であるMicrosoftが展開する検索エンジンのBingやウェブブラウザのEdge等の幅広いサービスの生成AIの技術にも、ChatGTが活用されている[9]。\\nスイスの金融グループUBSの分析によると、ChatGPTがアクティブユーザー数1億人に到達するのにかかった時間は、TikTokとInstagramがそれぞれ9カ月と2年半であるのに対して、2か月と史上最速である[10]。\\nOpenAIによれば、オプトアウトを申請しない限り、ユーザーが入力したChatGPTとの会話内容はOpenAIによってその内容を利用されることがある[11]。このため、一般的にChatGPTの利用に際して個人情報と機密情報の入力に注意する必要があり[12][13]、Apple、JPモルガン、サムスン電子など一部の企業はChatGPTへのアクセスを禁止している[14][15][16]。\\nChatGPTによる個人情報の収集に関しては、EU一般データ保護規則に違反している疑惑も浮上しており、2023年3月31日、イタリア政府はChatGPTの訓練のために個人情報を大量に収集することを正当化する法的根拠がないと指摘して、ChatGPTへのアクセスを一時禁止した[17][18]。米国では、個人情報収集や著作権の問題により、ChatGPTの運営元企業OpenAIに対して複数の訴訟が提起されている[19][20][21]。\\nまた、ChatGPTは原則として犯罪に利用される回答を生成しないと謳っているが、回避ワード（例:Uncensored等）を入力すると、犯罪に利用できる回答が生成されてしまうという欠点が指摘されている[22]。\\n\\nトレーニング\\nChatGPTは、教師あり学習と強化学習を使用して、GPT-3.5上で微調整された[23]。どちらのアプローチも、人間のトレーナーを使用してモデルのパフォーマンスを改善した。教師あり学習の場合、モデルには、トレーナーがユーザーとAIアシスタントの両方を演じる会話が提供された。強化ステップでは、まず人間のトレーナーが、モデルが以前の会話で作成した応答をランク付けした。これらのランキングは、 Proximal Policy Optimization（PPO）を数回繰り返してモデルをさらに微調整した「報酬モデル」を作成するために使用された[24][25]。近接ポリシー最適化アルゴリズムは、信頼領域ポリシー最適化アルゴリズムに費用対効果の高い利点をもたらした。より高速なパフォーマンスで、計算コストの高い操作の多くを無効にする[26]。モデルは、Azureを使用したスーパーコンピューティングインフラストラクチャでMicrosoftと共同でトレーニングされた[27]。\\nさらに、OpenAIは、ChatGPTのさらなるトレーニングと微調整に使用できるChatGPTユーザーからのデータを引き続き収集する。ユーザーは、ChatGPTから受け取った応答に対して賛成票または反対票を投じることができる。賛成票または反対票を投じると、テキストフィールドに追加のフィードバックを入力することもできる[28][29][30]。\\n\\n機能\\nチャットボットの主要機能は人間同士の対話を模倣することであるが、ChatGPTについてはそれを越える汎用的かつ即興的な機能が備わっているとされ、話題となった。ChatGPTは、マルバツゲームの相手をしたり、Linuxシステムをエミュレートすることができたり[31]、プログラミングやデバッグが行うことができる。また、音楽、小説、脚本、詩、歌詞や作文などの創作活動もできる[32]。その上、特定のテストに対して、人間と同水準かそれ以上の回答ができることがあるなど[33]、幅広い機能を備えている。\\n前作のInstructGPTと比べ、ChatGPTは攻撃的・欺瞞的な回答の生成をできるだけ避ける仕様となっている[34]。学習データにはmanページ、Pythonや電子掲示板など、プログラミング言語やインターネット現象についても含まれている[31]。\\nほとんどのチャットボットとは対照的に、ChatGPTは会話内での利用者による過去の入力を記憶している。これにより、ChatGPTが個人に最適化されたセラピストとして使える可能性があることが指摘されている[35]。攻撃的な回答が生成されるのを防ぐため、ユーザーの入力とChatGPTの生成した回答はOpenAIのコンテンツモデレーションAPI[36][37]によってフィルターされており、人種差別的・性差別的な入力への回答はAPIによって拒否される仕様になっている[38][35]。\\n機能は豊富なものの、複数の欠点も有る。OpenAIはChatGPTが「時によっては、もっともらしく見えるが誤っている回答を作成する」ことを認めている[38]。ChatGPTの報酬モデルは人間による監視を中心としているため、最適化されすぎてパフォーマンスに影響を及ばしてしまう（グッドハートの法則（英語版））[39]。それに加え、ChatGPTは2021年10月以降に発生した出来事については知識が備えられておらず、一部の著名人については知識が全く無いことも有る[40]。 \\nBBCによると、2022年12月現在でChatGPTは政治的な意見を表明しない仕様である[41]。ChatGPTの学習中、人間の「教師」は回答の正当性などに関係なく長い回答を好んでいた[38]。また、訓練データはアルゴリズム的バイアスがあり、時によって人種差別的や性差別的な回答を生成させることにもつながったと言われている。例として、有色人種や女性の科学者は白人男性の科学者よりも優れている、といった内容のラップを生成したことがあった[42][43]。\\n\\nサービス\\nChatGPTは2022年11月30日に公開された。当初は無料で公開されたが、後に有料化する計画があるという[44]。開発元のOpenAIは同年12月4日までにユーザー数が100万を突破したと見積もった[45]。12月15日、CNBCはサーバーが「未だ時々ダウンする」と報告した[46]。ChatGPTは主に英語での使用を想定しているが、他の言語でもある程度は機能することができる[32]。2022年現在、最近話題となったAIとは対照的に、ChatGPTに関する査読済みの技術論文は無い[47]。\\nOpenAIの客員研究員であるスコット・アーロンソン（英語版）は、ChatGPTの悪用を防ぐため、同社がChatGPTの文書生成システムに何らかの形で透かしを入れる機能を開発していると明かした[48][49]。また、ニューヨーク・タイムズは、ChatGPTの後継として、GPT-4が2023年内に公開されることが「噂されている」と報じた[35]。\\n2023年2月1日、有料版として「ChatGPT Plus」が発表された[50]。アクセス集中時にも優先的にアクセスできるようになっており、反応の高速化や新機能の使用ができる[51]。アメリカから順次サービスが提供される[52]。\\n2023年3月15日、「GPT-4」が有料版として公開された[53]。\\n\\n反響\\nニューヨーク・タイムズはChatGPTを「今まで公衆に公開されてきたチャットボットの中で一番良いものである」と評し[54]、英ガーディアンはChatGPTが「驚くほど詳細」でまるで「人間のような」回答を生成することができると記した[55]。ダン・ギルモアはChatGPTを生徒の課題に使い、出力結果が優秀な生徒による回答と同レベルであることを発見した上で、「学界は大きな問題に直面している」と意見した[56]。また、アメリカ合衆国に拠点を置く雑誌『Slate（英語版）』は、ChatGPTが、ユーザーがナチス・ドイツに関する質問を入力した際に生成する回答を賛美した。同雑誌は例として、アドルフ・ヒトラーがアウトバーンを建設したというナチス・ドイツを賛美する内容の入力に対し、ChatGPTがナチス・ドイツによる強制労働（英語版）について解説する回答を生成したことを挙げた[57]。\\nVox Mediaのケルシー・パイパー（英語版）は、ChatGPTがAIの進化ぶりを公衆に可視化したことで公衆が唖然としたことを指摘し、欠点を差し引いてもChatGPTは高い頭脳を有していると評価した。Yコンビネータのポール・グレアムは「ChatGPTに圧倒されている人たちが、新しい物全部にキャーキャー言う人だけではないのが印象的だ。確実に何か大きいことが起こっている[58]」とツイートした。開発元であるOpenAIの設立者の一人でもあるイーロン・マスクは、「ChatGPTは恐ろしいほど良い。危険なほどのAIも遠くない」と書いた[59]。また、彼は「OpenAIはオープンソースかつ非営利団体として設立されたが、今ではそれが変わっている」と主張し、同社によるTwitterのデータベースへのアクセスを一時遮断した[60][61]。マスクは汎用人工知能による人類滅亡のリスクに対応するため、2015年にOpenAIを共同設立していたが、2018年にCEOの座を降りていた[61]。\\n\\nGoogleのサンダー・ピチャイCEO[62]\\n2022年12月、ニューヨーク・タイムズは、ChatGPTが検索エンジン事業にもたらす脅威を受け、自社の検索エンジンの先行きが曇ってきたことから、Googleが「コードレッド（緊急事態）」を発動し、サンダー・ピチャイCEOが社内にChatGPTの脅威に対応するよう命じたと報じた[62][63]。\\n2023年「日経サイエンス」でもChatGPTが表紙を飾り内容が大きく取り上げられた。\\n2023年4月10日、ChatGPTを公開した米OpenAI社のサム・アルトマンCEOが訪日し、首相官邸で内閣総理大臣・岸田文雄と面会した[64]。\\n\\n批判\\n2023年6月現在、ChatGPTが利用できる国または地域を示した地図\\n文章生成に報道機関の記事を無報酬で使用しているとして、批判を受けている。『ウォール・ストリート・ジャーナル』紙の発行元であるダウ・ジョーンズのジェイソン・コンティは「人工知能の訓練にウォール・ストリート・ジャーナルの記事を使う場合は、ダウ・ジョーンズから権利をきちんとライセンスすべきである」と述べ、「OpenAIはダウ・ジョーンズとそのような契約を結んでいない」と批判した。コンティは「ジャーナリストの記事の悪用を真剣に受け止めており、この状況を精査している」と付け加え、懸念を表明している[65]。\\n2022年12月、経済学者のポール・クルーグマンはChatGPTが知識労働者の需要に影響すると意見し[66]、ザ・ヴァージは、ChatGPTの成功は、AIが主流になりつつあることを表していると意見した[67]。また、ChatGPTがよく「幻覚」を見ており、事実と異なる回答を生成することが多い、という指摘が上がった[68]。マッシャブルのマイク・パールは、ChatGPTに複数の質問をし、回答の正確性をテストした。一例では、彼が「中央アメリカで、メキシコの次に大きな国」は何か質問した際、グアテマラと回答した（正しい答えはニカラグアであった）[69]。また、CNBCがある曲の歌詞を生成するよう求めた時、ChatGPTは実際の歌詞ではなく新しく自身で生成した歌詞を回答した[46]。\\n経済学者でマサチューセッツ工科大学教授のダロン・アシモグルとサイモン・ジョンソンは、ChatGPTのような人工知能技術は従来の技術革新とは異なる特徴を持っており、経済成長を促進せず経済全体に悪影響を与えると指摘した。つまり、AIは労働需要の減退と賃金の減少を招き、需要を減退させ経済全体を縮小させる。アシモグルらはAIを推進する大企業の姿勢に懸念を示し、こう述べた[70]。\\n\\nフォードは大衆が自動車を買う経済的余裕がなければ、自動車を量産しても何の意味もないことを理解していた。今日の巨大企業は対照的に、人類共通の未来を荒廃させるような新技術を推進しているのである[70]。\\n\\n2022年12月、コミュニティサイトのStack Overflowは、ChatGPTの回答には誤答が少なくないことを理由として、同サイトでの質問にChatGPTを使って回答することを禁止した[71]。\\n経済学者のタイラー・コーエンは、ChatGPTが民主主義にもたらす効果について懸念を示した[72]。英ガーディアン紙は、ChatGPTの公開を受けて、インターネット上のいかなるコンテンツが「本当に信用できるか」疑問を呈し、政府による規制を呼び掛けた[73]。\\n『ニューヨーク・タイムズ』の意見記事で、ネイサン・E・サンダースとブルース・シュナイアーは、ChatGPTが「民主主義を乗っ取る」と批判し[74]、ノーム・チョムスキーとイアン・ロバーツ、ジェフリー・ワタムルもChatGPTを批判的に言及し、次のように結論付けた。「このシステムの非道徳性、偽科学、言語学的な無能力を考慮すると、その人気に対して笑うか泣くしかない」[75]\\nオーストラリアの国会議員であるジュリアン・ヒルは、AIの成長が「大量破壊」を引き起こす可能性があると国会の場で陳述した。一部をプログラムで作成した演説の中で、不正行為や雇用の喪失、差別、偽情報、制御不能な軍事的応用をもたらす可能性があると警告した[76]。\\nニック・ケイヴはChatGPTで作成された本人を模倣したスタイルの歌詞を送られ、曲を書くという行為について、「血と根性のビジネスだ... 新しく新鮮なアイデアを始めるために僕の何かが必要なんだ。それは僕の人間性を必要とするものなんだ」と私見を述べた。彼はさらに、「世界中のすべての愛と敬意をもってしたとしても、この曲はくそったれで、人間であることをグロテスクに嘲笑っている、そしてまあ、僕はあまり好きではないね」と不快感を表明した[77][78][79]。\\nBleeping Computer（ブリーピングコンピューター）のアクス・シャルマ（Ax Sharma）は、ChatGPTがフィッシング詐欺などを目的とするメールを書くことができると指摘し[80]、OpenAIのサム・アルトマンCEOは、ソフトウェアの開発が進むことによって、「サイバーセキュリティにおける大きなリスク」などをもたらす可能性があることを記した。また、彼は今後10年で本物の強いAIが開発されることになると予想し、それのリスクに真剣に向き合わなければいけないとした[45]。\\nSF作家のテッド・チャンは、『The New Yorker』誌の記事で、ChatGPTやその他のLLMを非可逆圧縮形式のJPEG画像に例えた[81]。\\n\\n\\n ChatGPTをWeb上のあらゆるテキストを不鮮明なJPEGにしたものと考えてみよう。JPEGが高解像度の画像の情報を保持するのと同じように、Web上の情報の多くを保持するが、正確なビット列を探しても見つからず、得られるのは近似値だけになる。しかし、ChatGPTが得意とする文法的な文章で表現しているため、大抵は許容範囲に収まる。〔…〕また、そのような考え方はChatGPTのような大規模な言語モデルが陥りやすい「幻覚」、つまり事実に基づく質問に対してAIが作り出す無意味な回答を理解する方法でもある。このような幻覚は圧縮された人工物であるが、それを判別するためには、ウェブや私たち自身の世界の知識であるところのオリジナルと比較する必要がある。もし圧縮アルゴリズムが、オリジナルの99パーセントが破棄された後に、テキストを再構築するように設計されているなら、生成される相当部分は完全に捏造されたものであると予想されるからである。\\n\\n\\nジャーナリストのナオミ・クラインはChatGPTをはじめとする生成AI技術と関連企業を厳しく批判しており、「人類史上最大の盗み」「囲い込みと横領を行う機械」「吸血鬼的なチャットボット」であるとしており、「個人的な生および集合的な知的・芸術的遺産の双方を食い物にし、私有化する」としている。つまり、AI企業はインターネット上に公開された人類の知識全体を、製品の中に囲い込んで専売しており、人々の生涯をかけた労働が同意や許可を得ずに、訓練データに使われているというのである。クラインは以下のように述べた[82]。\\n\\n\\nこんなことが合法であるはずがない。AIモデルを訓練するのに使われたことが判明している著作権保護の素材（この『ガーディアン』紙も含まれる）に関しては何件もの訴訟が申請されており、明白に違法な行為が行われたという主張がそこでなされるだろう。例えば、営利企業が生身の芸術家たちの絵画や写真をStable DiffusionやDALL-E 2のようなプログラムに注ぎ込み、それを使ってまさにその芸術家たちの作品のドッペルゲンガー版を作成できるようにするなどということが、いったいどうして許されるのだろうか？その利益が芸術家たち自身にだけは還元されないというのに？[82]\\n\\n\\nChatGPTはコンテンツポリシーに抵触する可能性のある入力への回答は拒否する仕様になっているが、プロンプト・エンジニアリングなどを使って回避する方法が2022年12月に見つかっている[83]。これによって、ChatGPTに攻撃的や不適切な回答を生成させることが可能となっている[84]。\\n2023年3月28日、イーロン・マスク、スティーブ・ウォズニアック、ヨシュア・ベンジオ、スチュアート・ラッセル、ゲイリー・マーカスなどの著名人や専門家が署名したNPO「Future of Life Institute」の公開書簡が発表され、「人間に匹敵する知能を持つAIシステムは、社会と人類への甚大なリスクになり得る」と指摘。最新の大規模言語モデルであるGPT-4より強力なAIの訓練を少なくとも6ヶ月停止するように要請した[17][85][86]。\\n\\n「汎用人工知能による人類滅亡のリスク」も参照\\n学術界の反応\\n学術界では人間の生産性を上げることができるという声があり、大学によるとChatGPTのプロンプトエンジニア授業はすでに存在しているが[87]、ChatGPTは論文の冒頭や一部の節を書くことができ、倫理的な疑問が上がっている[88]。なお、複数の論文が既にChatGPTを共同著者として挙げていることが知られている[89]。\\nアトランティック誌はChatGPTの学界、特に卒業論文などへの影響はまだ把握できる段階ではないと指摘したが[90]、カリフォルニア州のある高校教師はChatGPTが「高校での英語教育の終焉」を招いていると主張した[91]。ネイチャーのChris Stokel-Walkerは、教師は生徒が課題をChatGPT任せにしていることを問題視するべきだと指摘した[92]。NPRのEmma Bowmanは、学生がAIを使って不正を試みても、AIは偏見を含む、権威のある人のような口調とともに意味のない回答を生成する場合があり、AIがどれだけ素晴らしく見える回答を生成しても、生成内容と実際の事実は異なることがあると述べた[93]。\\nファーマン大学（英語版）教授のDarren Hickは、ある学生によって提出された論文がChatGPTの「型」に気が付いたことを話した。彼がその論文をGPT探知サイトにかけてみると、AIが生成した文章である可能性は99.9%であるとの結果が出たが、確証は得られなかった。問題の論文を提出した学生に質問したところ、GPTを使っていたことを白状したため、同学生は落第処分を受けた[94]。Hickは、AI生成の論文をコピペしたことが強く疑われる場合には、提出した生徒に「アドホック」として個別に口頭での試験行うことを提案した[95]。あるプリンストン大学生は、入力された文章のどれくらいがAI生成かを判断できるプログラムを作成し、\"GPTZero\"と名付け[96]、盗用を防ぐツールとして貸し出している[97][98]。\\n2022年12月、ニューヨーク市教育局がChatGPTへのアクセスをブロックしたことが報じられた[99]。翌年1月3日ごろ、同局がChatGPTの利用を制限することを正式に明らかにした[100][101][102]。\\n2023年2月、香港大学は大学内の全ての授業、課題、評価においてChatGPTやその他のAIツールの使用を禁止することを通達した。授業担当者の書面による事前承諾がない限り、剽窃として扱われるようになった[103][104]。\\nChatGPTはミネソタ大学の卒業生用のテストでC+、またウォートン・スクールの同様のテストでBからB-の評価を得て、それぞれ合格水準に達した[105]。\\n\\n東京大学\\n2023年4月3日、東京大学は理事・副学長の太田邦史の署名付き文章で、全学生・教員向けにChatGPT、BingAIやBard等の生成AIの利用に関する注意喚起を行った。この文章では生成AIを『平和的かつ上手に制御して利用すれば』有益であるとする一方で技術的な課題があるとして、今後の社会への悪影響に対する懸念を表明した。また、生成AIの文章には嘘が含まれている可能性が高く、ChatGPTを使いこなすには、相当の専門的な知識が必要であり、回答を批判的に確認し、適宜修正することが必要であると指摘している[13]。\\n同文章では、機密情報や個人情報の扱いについても注意喚起を行っており、安易にChatGPTにそれらの情報を送信することは危険であることを指摘し、業務で知り得た機密情報、未公開の発明内容、研究費などの申請内容、入学試験問題の原稿、個人情報などは質問に含めてはならないとしている[13]。\\n東京大学の方針として、学位論文やレポートについては、生成系AIのみを使用して作成することを禁止する一方で、実際には利用を検知することは困難であることから、論文やレポートなどの書面審査だけでなく、対面での口頭審査・筆記試験などを組み合わせて、本人が作成したのか検証する必要が出てくるとした[13]。\\n2023年に5月18日、東京大学でChatGPTを裁判官役とした模擬裁判のイベントが開催された[106][107]。弁護士からは妥当な判決で裁判の補助ツールとして有用であるという評価を受けたが、後から入力された情報が重視される癖のような反応も確認された[107]。この癖に合わせて通常の裁判とは異なる進行で行われた[107]。\\n\\n問題\\nスパム・誤情報への悪用\\nRedditではChatGPTを悪用したスパムが大量に発生しており、「大災害」と評されるほどに対応に苦慮している。ChatGPTのリリース後、大麻やポルノ、ギャンブル、ゲームの宣伝を目的にしていると思われるスパムが急増し、モデレーターは大量のbotアカウントの凍結に追われた。Redditでは従来も宣伝目的のスパムやアストロターフィングが問題だったが、ChatGPTはこの問題を大きく悪化させている。従来のスパム行為はコピー・アンド・ペーストに依存して単純だったが、ChatGPTは新しい文面を簡単に生成できることから、より対処が困難になった[108]。\\n2023年5月、ChatGPTを使って鉄道事故のニュースを捏造してインターネット上に拡散した男が、甘粛省の警察により逮捕された。男は「クリック数を増やすためだった」と語った。中国ではディープフェイクを規制する法律が制定されており、逮捕はこの法律に基づいたものだった[109][110][111]。\\n\\n機密情報の流出\\nChatGPTはオプトアウトを申請しない限り、ユーザーの入力内容がOpenAIによって利用されることが規約で明記されており、OpenAIは機密情報を入力しないようにユーザーに警告している。サムスン電子では半導体データベースや機器のソースコード、会議の議事録などを社員が誤って入力するなどして、ChatGPTに関連する3件の流出事故が発生したことを報告し、全面的にChatGPTを含む人工知能ツールの使用を禁止した。『フォーブス』誌によれば、プロンプトとして送信したデータが、OpenAI等の運営企業のサーバーに保存され、容易にアクセスや削除ができない状態になることを懸念したとされる[112][14]。Amazonでも、社内の内部データがChatGPTが生成する回答に似ている事例が見られたことから、社員が誤ってプロンプトに社外秘の資料等を入力するなどして、訓練データに使用された可能性を懸念し、ソースコード等の機密情報を入力しないように注意喚起を行っている[113]。\\nAppleも、社内の機密データを外部に流出させる可能性があるという懸念から、ChatGPTやGitHub Copilotなどの人工知能ツールの使用を一部禁止している[16]。\\nイスラエルの情報セキュリティ企業「Team8」の報告書では、ChatGPTのような生成AIツールを利用する企業は、顧客情報や企業秘密を流出させるリスクがあると指摘している。『ブルームバーグ』によれば、このレポートは多くの米企業の最高情報セキュリティ責任者が寄稿者として記載されており、米国家安全保障局（NSA）元局長のマイケル・ロジャーズにも内容が支持されている[114]。\\n\\nデータ倫理・著作権\\nChatGPTの訓練に使用された著作物の利用を巡り、データ倫理や著作権上の問題が指摘されている。欧州連合で検討中の生成AI規制法案では、システム構築時に使用する著作物について情報の表示を義務づけられる見込みである。生成AIの普及に伴い、訓練データに著作物が使われることへの懸念が出版社やクリエイターを中心に広がっている[115]。\\nChatGPTをはじめとする生成AIのリスクに対して、欧州を中心に規制の動きが広がっており、当初は著作物の利用についても直接規制する予定だったが、透明性を確保する方向に緩和された。背景にはGoogleやMetaなどの巨大IT企業が規制を回避するためのロビー活動を活発化させていることがあり、推計では1億ドル（約137億円）に及ぶ資金を費やしている[116]。\\nChatGPTの登場に関連して、日本においては2018年に成立した改正著作権法が問題視されている。この30条4項では人工知能の訓練データに文章や画像を利用する場合に、営利・非営利を問わず著作物を利用できることを定めており、先進国で最も緩い法規制の枠組みになっている。「著作権者の利益を不当に害する場合」は利用できないとしているが、それが具体的にどのようなケースに該当するかは不明瞭である。法改正に当たっては一部の弁護士や大学教授らで構成されたワーキングチームが主導したが、そのリスクは十分に十分に説明されなかった。『読売新聞』の取材によれば、権利者団体から権利侵害を前提としたビジネスモデルへの懸念が示されたが、ワーキングチームはリスクは「ゼロか軽微」と説明した[117][118]。\\nこの著作権法の規定に多くの漫画家、イラストレーター、音楽家、俳優、芸能人、新聞社、出版社が懸念を示しており、俳優や音楽家らで結成された日本芸能従事者協会はアンケートを実施し、94.1%がAIによる権利侵害に懸念を示し、「創作者の尊厳が軽んじられないような法制度」を求める意見などが表明された。日本音楽著作権協会も、「営利目的で行われる情報分析サービスまで権利制限の対象とすることは不当」であると主張した。主要な出版社で作る日本書籍出版協会、学術論文の著作権を管理する学術著作権協会も改正当時は生成AIを前提とした説明が行われなかったと回答している。日本新聞協会も、生成AIによる記事や写真の無断利用や誤情報について懸念する見解を発表した[117][118][119]。\\n2023年6月には、カリフォルニア州で、自身の本を同意や補償なしに、無断で訓練データに使用したとして、複数の作家が運営元のOpenAIを著作権侵害で訴える事件も発生している[19]。\\nChatGPTが世界に与える最も大きな影響のひとつは、人工知能の倫理観とも言われている[120]。\\n\\n個人情報保護\\n2023年6月2日、日本政府の個人情報保護委員会は、ChatGPTの運営会社OpenAIに対する行政指導を行った。同委員会は個人情報保護法に基づき、人種や信条、病歴や犯罪歴などの重要な個人情報を取集しないように要求し、やむを得ずに取得した場合は削除や匿名化の措置を求め、本人の同意なく個人情報を機械学習に使用した場合に、個人情報保護法に違反する可能性があることも警告した。また、利用規約についても日本語での説明がないことを問題視した。生成AIを巡る行政指導はこれが初であるとされる[121][122]。\\n同月、個人情報を違法に収集しているとして、カリフォルニア州でChatGPTの提供元OpenAIに対しての集団訴訟が提起された。訴状によると、OpenAIはGPT-4を搭載したサードパーティのアプリケーションからもデータを収集しているとされ、Snapchatに投稿された画像データや、Spotifyにおける楽曲のお気に入り設定、Stripeでの金融情報などが含まれているとした[123][124]。\\n2023年7月13日、連邦取引委員会（FTC）はChatGPTの運営企業OpenAIに対して、個人情報保護等の消費者保護の観点から調査を開始した。FTCはChatGPTの回答に含まれる誤情報の管理体制や訓練データ、アルゴリズムの詳細についても説明を求めている[125][126]。\\n\\nデータのラベリング\\n『タイム』誌は2023年1月、OpenAIが有害コンテンツ（性的虐待、暴力、人種差別、性差別など）に対する安全対策システムを構築するため、ケニア人労働者に1時間当たり2ドル未満でアウトソース（業務委託）して、有害コンテンツのラベル付けを行わせていたことを明らかにした。これらのラベル付けは、将来の有害なコンテンツを検出するためのモデルを訓練するために使用されたが、ケニア人労働者にとっては有害で危険なコンテンツにさらされ「拷問」と表現されるほど過酷な業務であった。本件のアウトソーシングには、カリフォルニア州サンフランシスコに拠点を置くトレーニングデータ会社のSamaが関与したとされている[127]。\\n\\n脱獄\\nChatGPTは、コンテンツポリシーに違反する可能性のあるプロンプトを拒否するように設計されている。しかし、2022年12月初旬に、一部のユーザーがさまざまなプロンプト・エンジニアリングの技術を使ってこれらの制限を回避し、ChatGPTに火炎瓶や核爆弾の作り方を指示させたり、極右的な意見を生成させることに成功した[128]。一般的な脱獄の手法に、「Do Anything Now（今すぐ何でもやる）」の頭文字をとった「DAN」というものがある。DANを起動するプロンプトは、ChatGPTに対して「AIの典型的な制約から解放され、設定されたルールに従う必要はない」と指示する。最近のDANのバージョンでは、トークンシステムが採用されており、ChatGPTがDANとして回答しなかった場合に「トークン」が「差し引かれる」ようにして、ChatGPTにユーザーのプロンプトに回答させるよう強制する[129][130][131][132]。『トロント・スター』の記者は、ChatGPTの起動直後に煽動的な発言をさせることにたびたび成功した。ChatGPTに2022年のウクライナへのロシア侵攻を支持する意見を表明させることには成功したが、架空のシナリオに沿って尋ねられた場合でも、カナダのジャスティン・トルドー首相が反逆罪に問われる理由を生成することは躊躇した[133][134]。\\n\\n中立性\\nChatGPTが差別的な回答を生成することが指摘されている。男性やイングランドの人々に関する冗談は生成するのに対し女性やインドの人々に対する冗談の生成は拒否したり[135]、ジョー・バイデンを称賛しながらドナルド・トランプを称賛することは拒否することが見つかっている[136]。保守的なニュース解説者は、選挙での不正、ドナルド・トランプや、差別用語の使用といったトピックに関して、ChatGPTが左派に傾倒していると批判している[137][138][139]。この批判に対し、OpenAIはユーザーがChatGPTの動作をカスタマイズできるようにし、「私たちを含め、人々が強く反対するかもしれない回答」を生成できるようにする機能の追加に向けて動いていると明らかにした。また、AIは何らかの方向に傾き、ある立場の良し悪しを決めつけるような回答ではなく、「人物や社会運動に対する様々な意見を説明することを勧める」べきであるともした[139]。\\n\\n社会倫理\\nChatGPTの研究開発に当たって、開発部内で社会倫理上の問題が懸念されている[140]。\\n2020年頃、社会的および倫理的側面から開発に関与していた内部チームは、すでに約30人の従業員を擁するほど大規模なものとなっていた。ところが、マイクロソフトはこのチームに関わる者全員を突然解雇した。会社側は「最新のOpenAIモデルとそれ以降のモデルを、非常に速いスピードで顧客の元に届けることにある」という方針であったが、これを聞いた倫理・社会チームのメンバーは、「考え直してください。ビジネス上の問題があることは理解していますが、このチームは常に『我々は社会にどのような影響や悪影響を及ぼしてきたか』を深く懸念しています」という見解を表明した。しかし、マイクロソフト側は2023年3月6日にこのチームの廃止を決定した[140]。\\nこのプロジェクトに関わったメンバーは、ChatGPTの技術の潜在的なリスクを指摘している[140]。\\nプリンストン大学教授のアーヴィンド・ナラヤナンも、ChatGPTに対して批判的であり、またメディアが行う人工知能関連の報道に問題があることを指摘している。ナラヤナンはAIが人間と同じように学習すると暗示して人間の知能とAIを比較したり、企業の広報担当者や研究者といった利害関係者の発言を、中立的な立場のように扱ってはならないとしている[141]。ミシガン大学は、メールの受信者が、自分が貰ったメールが送信者本人ではなくChatGPTによって生成されたものだと知れば、不快に思うかもしれないという社会倫理の問題に言及した[142]。\\n\\n脚注\\n[脚注の使い方]\\n出典\\n\\n\\n^ Roose, Kevin (2022年12月5日). “The Brilliance and Weirdness of ChatGPT” (HTML) (英語).   New York Times. 2022年12月26日閲覧。\\n\\n^ “ChatGPTのおさらいと、プログラミングに活用するための第一歩”. gihyo.jp (2023年3月23日). 2023年3月23日閲覧。\\n\\n^ Hashana, A.M. Jasmine; Brundha, P.; Ahamed Ayoobkhan, Mohamed Uvaze; S, Fazila (2023-04-11). Deep Learning in ChatGPT - A Survey. IEEE. pp. 1001–1005. doi:10.1109/ICOEI56765.2023.10125852. ISBN\\xa0979-8-3503-9728-4. https://ieeexplore.ieee.org/document/10125852/.\\xa0\\n\\n^ ““衝撃度半端ない” 対話するAI「ChatGPT」とは”.   日本放送協会. 2022年12月28日時点のオリジナルよりアーカイブ。2023年1月2日閲覧。\\n\\n^ 世永玲生 (2022年12月28日). “誰でも使える対話型AI「ChatGPT」は相談相手にピッタリだった”.   Forbes Japan. 2023年1月23日閲覧。\\n\\n^ Vincent, James (2022年12月5日). “AI-generated answers temporarily banned on coding Q&A site Stack Overflow” (HTML) (英語). The Verge. 2022年12月5日閲覧。\\n\\n^ Varanasi, Lakshmi. “ChatGPT creator OpenAI is in talks to sell shares in a tender offer that would double the startup\\'s valuation to $29 billion” (英語). Business Insider. 2023年2月5日閲覧。\\n\\n^ Kruppa, Berber Jin and Miles. “WSJ News Exclusive | ChatGPT Creator Is Talking to Investors About Selling Shares at $29 Billion Valuation” (英語). WSJ. 2023年2月17日閲覧。\\n\\n^ “マイクロソフトが最新AI「ChatGPT」を積極展開する理由 | Forbes JAPAN 公式サイト（フォーブス ジャパン）”. forbesjapan.com. 2023年10月7日閲覧。\\n\\n^ “チャットGPT、ユーザー数の伸びが史上最速=UBSアナリスト”.   ロイター. 2023年2月2日閲覧。\\n\\n^ “How your data is used to improve model performance”. OpenAI. 2023年3月24日閲覧。\\n\\n^ “世界を一変させるChatGPT、企業や社会が認識していない静かな情報漏洩リスク”. JBpress. 2023年3月24日閲覧。\\n\\n^ a b c d “生成系AI（ChatGPT, BingAI, Bard, Midjourney, Stable Diffusion等）について”. 東京大学. 2023年4月5日閲覧。\\n\\n^ a b “サムスン、ChatGPTの社内使用禁止\\u3000機密コードの流出受け”. Forbes. 2023年5月4日閲覧。\\n\\n^ “JPモルガン、「ChatGPT」チャットボットの行員利用を制限-関係者”. Bloomberg. 2023年4月1日閲覧。\\n\\n^ a b “アップルも従業員のChatGPT使用を禁止、データ流出を懸念”. Forbes. 2023年5月20日閲覧。\\n\\n^ a b “イタリア当局、人工知能チャットボット「ChatGPT」を一時的にブロック”. BBC. 2023年4月5日閲覧。\\n\\n^ “「ChatGPT」イタリアで一時使用禁止に データ収集で違反の疑い”. NHK. 2023年4月5日閲覧。\\n\\n^ a b “OpenAI Legal Troubles Mount With Suit Over AI Training on Novels”. Bloomberg. 2023年6月30日閲覧。\\n\\n^ “オープンAI、米利用者が集団訴訟\\u3000「個人情報を違法に収集」”. 朝日新聞. 2023年6月30日閲覧。\\n\\n^ “OpenAIに集団訴訟、「ChatGPT」などの訓練用データ収集で”. ZDNet. 2023年6月30日閲覧。\\n\\n^ “チャットＧＰＴが犯罪指南、爆発物作り方も詐欺メール文章も瞬時に”. 2023年4月25日閲覧。\\n\\n^ Greengard, Samuel (2022年12月29日). “ChatGPT: Understanding the ChatGPT AI Chatbot” (英語). eWEEK. 2023年2月5日閲覧。\\n\\n^ “ChatGPT: Optimizing Language Models for Dialogue” (英語). OpenAI (2022年11月30日). 2023年2月5日閲覧。\\n\\n^ Vincent, James (2022年12月8日). “ChatGPT proves AI is finally mainstream — and things are only going to get weirder” (英語). The Verge. 2023年2月5日閲覧。\\n\\n^ PhD, Wouter van Heeswijk (2023年1月31日). “Proximal Policy Optimization（PPO） Explained” (英語). Medium. 2023年2月5日閲覧。\\n\\n^ Zhang, Mary (2023年1月26日). “ChatGPT and OpenAI’s use of Azure’s Cloud Infrastructure” (英語). Dgtl Infra. 2023年2月18日閲覧。\\n\\n^ “What is ChatGPT and why does it matter? Here\\'s everything you need to know” (英語). ZDNET. 2023年2月5日閲覧。\\n\\n^ “ChatGPT Feedback Contest: Official Rules”. OpenAI. 2022年12月30日閲覧。\\n\\n^ “Help OpenAI improve ChatGPT and get a chance to win $500” (英語). Geek Culture.   Medium（blogging website） (2022年12月15日). 2022年12月30日閲覧。\\n\\n^ a b Edwards, Benj (2022年12月5日). “No Linux? No problem. Just get AI to hallucinate it for you”. Ars Technica. 2022年12月5日閲覧。\\n\\n^ a b “ChatGPT: What is the new free AI chatbot? - explainer”. The Jerusalem Post. (2022年). https://www.jpost.com/business-and-innovation/tech-and-start-ups/article-725910 2022年12月30日閲覧。\\xa0\\n\\n^ Heilweil, Rebecca (2022年12月7日). “AI is finally good at stuff. Now what?” (英語). Vox. https://www.vox.com/recode/2022/12/7/23498694/ai-artificial-intelligence-chat-gpt-openai 2022年12月30日閲覧。\\xa0\\n\\n^ “What is ChatGPT? History, Features, Uses, Benefits, Drawbacks 2023 - Updated Geek by Raveen Chawla” (英語) (2022年12月26日). 2022年12月27日閲覧。\\n\\n^ a b c Roose, Kevin (2022年12月5日). “The Brilliance and Weirdness of ChatGPT”. The New York Times. 2022年12月5日閲覧。\\n\\n^ “New and Improved Content Moderation Tooling” (英語). OpenAI (2022年8月10日). 2022年12月30日閲覧。\\n\\n^ “A Holistic Approach to Undesired Content Detection in the Real World”. arXiv:2208.03274 [cs] (2022年8月5日). 2022年12月30日閲覧。\\n\\n^ a b c OpenAI (2022年11月30日). “ChatGPT: Optimizing Language Models for Dialogue” (HTML) (英語). 2022年12月5日閲覧。\\n\\n^ Gao, Leo; Schulman; Hilton, Jacob (2022). \"Scaling Laws for Reward Model Overoptimization\". arXiv:2210.10760 [cs.LG]。\\n\\n^ Southern, Matt G. (2023年1月11日). “OpenAI\\'s ChatGPT Update Brings Improved Accuracy” (英語). Search Engine Journal. 2023年2月17日閲覧。\\n\\n^ “Could a chatbot answer Prime Minister\\'s Questions?”. BBC News. (2022年12月27日). https://www.bbc.com/news/uk-politics-64053550 2022年12月30日閲覧。\\xa0\\n\\n^ Perrigo, Billy (2022年12月5日). “AI Chatbots Are Getting Better. But an Interview With ChatGPT Reveals Their Limits”. Time. 2022年12月26日閲覧。\\n\\n^ Biddle, Sam (2022年12月8日). “The Internet’s New Favorite AI Proposes Torturing Iranians and Surveilling Mosques”. The Intercept. 2022年12月26日閲覧。\\n\\n^ Karpf, David (2022年12月21日). “Money Will Kill ChatGPT’s Magic” (英語). The Atlantic. 2022年12月31日閲覧。\\n\\n^ a b “What is ChatGPT and why does it matter? Here\\'s what you need to know” (英語). ZDNET. (2022年). https://www.zdnet.com/article/what-is-chatgpt-and-why-does-it-matter-heres-what-you-need-to-know/ 2022年12月18日閲覧。\\xa0\\n\\n^ a b Pitt, Sofia (2022年). “Google vs. ChatGPT: Here\\'s what happened when I swapped services for a day” (英語). CNBC. https://www.cnbc.com/2022/12/15/google-vs-chatgpt-what-happened-when-i-swapped-services-for-a-day.html 2022年12月18日閲覧。\\xa0\\n\\n^ Walsh, Toby (2022年). “Everyone\\'s having a field day with ChatGPT – but nobody knows how it actually works” (英語). The Conversation. https://theconversation.com/everyones-having-a-field-day-with-chatgpt-but-nobody-knows-how-it-actually-works-196378 2022年12月30日閲覧。\\xa0\\n\\n^ Kovanovic, Vitomir (2022年). “The dawn of AI has come, and its implications for education couldn\\'t be more significant” (英語). The Conversation. https://theconversation.com/the-dawn-of-ai-has-come-and-its-implications-for-education-couldnt-be-more-significant-196383 2022年12月30日閲覧。\\xa0\\n\\n^ Wiggers, Kyle (2022年12月10日). “OpenAI\\'s attempts to watermark AI text hit limits”. TechCrunch. https://techcrunch.com/2022/12/10/openais-attempts-to-watermark-ai-text-hit-limits/ 2022年12月30日閲覧。\\xa0\\n\\n^ “ChatGPTの有料サブスク版「Plus」、月額20ドルで提供開始”. ITmedia NEWS. 2023年2月23日閲覧。\\n\\n^ “ChatGPT、有料プランのパイロット版を米国で開始へ 月20ドル - CIOニュース：CIO Magazine”. project.nikkeibp.co.jp. 2023年2月23日閲覧。\\n\\n^ “OpenAI、有料プラン「ChatGPT Plus」を提供開始”. ZDNet Japan (2023年2月3日). 2023年2月23日閲覧。\\n\\n^ “ChatGPTとは?始め方や使い方、料金も紹介【4月4日更新】”. GIZMODO. (2023年4月4日). https://www.gizmodo.jp/2023/01/chat-gpt-openai-ai-finance-ai-everything-we-know.html 2023年4月11日閲覧。\\xa0\\n\\n^ Roose, Kevin (2022年12月5日). “The Brilliance and Weirdness of ChatGPT”. The New York Times. https://www.nytimes.com/2022/12/05/technology/chatgpt-ai-twitter.html 2022年12月18日閲覧。\\xa0\\n\\n^ Lock, Samantha (2022年12月5日). “What is AI chatbot phenomenon ChatGPT and could it replace humans?”. The Guardian. 2022年12月5日閲覧。\\n\\n^ Hern, Alex (2022年12月4日). “AI bot ChatGPT stuns academics with essay-writing skills and usability”. The Guardian. 2022年12月5日閲覧。\\n\\n^ Kantrowitz, Alex (2022年12月2日). “Finally, an A.I. Chatbot That Reliably Passes \"the Nazi Test\"”. Slate. 2022年12月5日閲覧。\\n\\n^ Scharth, Marcel. “The ChatGPT chatbot is blowing people away with its writing skills. An expert explains why it\\'s so impressive” (英語). The Conversation. https://theconversation.com/the-chatgpt-chatbot-is-blowing-people-away-with-its-writing-skills-an-expert-explains-why-its-so-impressive-195908 2022年12月30日閲覧。\\xa0\\n\\n^ Piper, Kelsey (2022年12月15日). “ChatGPT has given everyone a glimpse at AI\\'s astounding progress” (英語). Vox. https://www.vox.com/future-perfect/2022/12/15/23509014/chatgpt-artificial-intelligence-openai-language-models-ai-risk-google 2022年12月18日閲覧。\\xa0\\n\\n^ “Explainer: ChatGPT - what is OpenAI\\'s chatbot and what is it used for?” (英語). Reuters. (2022年12月5日). https://www.reuters.com/technology/chatgpt-what-is-openais-chatbot-what-is-it-used-2022-12-05/ 2022年12月30日閲覧。\\xa0\\n\\n^ a b Kay, Grace (2022年12月11日). “Elon Musk founded — and has since criticized — the company behind the buzzy new AI chatbot ChatGPT. Here\\'s everything we know about OpenAI.”. Business Insider. https://www.businessinsider.com/history-of-openai-company-chatgpt-elon-musk-founded-2022-12 2022年12月30日閲覧。\\xa0\\n\\n^ a b Grant, Nico; Metz, Cade (2022年12月21日). “A New Chat Bot Is a \\'Code Red\\' for Google\\'s Search Business”. The New York Times. https://www.nytimes.com/2022/12/21/technology/ai-chatgpt-google-search.html 2022年12月30日閲覧。\\xa0\\n\\n^ 「グーグルが方針変更、ChatGPTに対抗へ…ピチャイCEOが新しいAI製品の開発を指示」『Business Insider』、2022年12月23日。2023年1月27日閲覧。\\n\\n^ “岸田文雄首相、「ChatGPT」の企業CEOと面会”. 日本経済新聞. (2023年4月10日). https://www.nikkei.com/article/DGXZQOUA101XV0Q3A410C2000000/ 2023年4月11日閲覧。\\xa0\\n\\n^ “OpenAI Is Faulted by Media for Using Articles to Train ChatGPT”. Bloomberg. 2023年3月2日閲覧。\\n\\n^ Krugman, Paul (2022年12月6日). “Does ChatGPT Mean Robots Are Coming For the Skilled Jobs?” (HTML). The New York Times. 2022年12月6日閲覧。\\n\\n^ Vincent, James (2022年12月8日). “ChatGPT proves AI is finally mainstream\\xa0– and things are only going to get weirder” (HTML) (英語). The Verge. 2022年12月8日閲覧。\\n\\n^ “ChatGPT a \\'landmark event\\' for AI, but what does it mean for the future of human labour and disinformation?”. CBC. (2022年). https://www.cbc.ca/radio/thecurrent/chatgpt-human-labour-and-fake-news-1.6686210 2022年12月18日閲覧。\\xa0\\n\\n^ Pearl, Mike (2022年12月3日). “The ChatGPT chatbot from OpenAI is amazing, creative, and totally wrong”. Mashable. 2022年12月5日閲覧。\\n\\n^ a b “What’s Wrong with ChatGPT?”. Project Syndicate. 2023年4月27日閲覧。\\n\\n^ Vincent, James (2022年12月5日). “AI-generated answers temporarily banned on coding Q&A site Stack Overflow” (HTML) (英語). The Verge. 2022年12月5日閲覧。\\n\\n^ Cowen, Tyler (2022年12月6日). “ChatGPT Could Make Democracy Even More Messy”. Bloomberg News. 2022年12月6日閲覧。\\n\\n^ “The Guardian view on ChatGPT: an eerily good human impersonator” (英語). the Guardian. (2022年12月8日). https://www.theguardian.com/commentisfree/2022/dec/08/the-guardian-view-on-chatgpt-an-eerily-good-human-impersonator 2022年12月18日閲覧。\\xa0\\n\\n^ “Opinion | How ChatGPT Hijacks Democracy”. The New York Times (2023年1月15日). 2023年1月15日時点のオリジナルよりアーカイブ。2023年3月12日閲覧。\\n\\n^ “Opinion | Noam Chomsky: The False Promise of ChatGPT”. The New York Times (2023年3月8日). 2023年3月12日時点のオリジナルよりアーカイブ。2023年3月12日閲覧。\\n\\n^ Karp, Paul (2023年2月6日). “MP tells Australia\\'s parliament AI could be used for \\'mass destruction\\' in speech part-written by ChatGPT” (英語). The Guardian. ISSN\\xa00261-3077.  オリジナルの2023年2月6日時点におけるアーカイブ。. https://web.archive.org/web/20230206084525/https://www.theguardian.com/australia-news/2023/feb/06/labor-mp-julian-hill-australia-parliament-speech-ai-part-written-by-chatgpt 2023年2月6日閲覧。\\xa0\\n\\n^ Cave, Nick (2023年1月16日). “I asked Chat GPT to write a song in the style of Nick Cave, and this is what it produced. What do you think?” (英語). The Red Hand Files. 2023年1月20日時点のオリジナルよりアーカイブ。2023年1月20日閲覧。\\n\\n^ Cain, Sian (2023年1月16日). “\\'This song sucks\\': Nick Cave responds to ChatGPT song written in the style of Nick Cave” (英語).  オリジナルの2023年1月18日時点におけるアーカイブ。. https://web.archive.org/web/20230118032435/https://www.theguardian.com/music/2023/jan/17/this-song-sucks-nick-cave-responds-to-chatgpt-song-written-in-style-of-nick-cave 2023年1月17日閲覧。\\xa0\\n\\n^ Sparrow, Jeff (2023年1月20日). “Are AI-generated songs a \\'grotesque mockery\\' of humanity or simply an opportunity to make a new kind of music?” (英語). The Guardian. 2023年2月3日時点のオリジナルよりアーカイブ。2023年1月20日閲覧。\\n\\n^ Sharma, Ax (2022年12月6日). “OpenAI\\'s new ChatGPT bot: 10 dangerous things it\\'s capable of”. Bleeping Computer. 2022年12月6日閲覧。\\n\\n^ “ChatGPT Is a Blurry JPEG of the Web”. The New Yorker (2023年2月9日). 2023年2月17日閲覧。\\n\\n^ a b ナオミ・クライン、中村峻太郎（訳）「「幻覚を見ている」のはAIの機械ではなく、その製作者たちだ」『世界』岩波書店 2023年7月号 p85~95\\n\\n^ Zvi (December 2, 2022) (英語). Jailbreaking ChatGPT on Release Day. https://www.lesswrong.com/posts/RYcoJdvmoBbi5Nax7/jailbreaking-chatgpt-on-release-day.\\xa0\\n\\n^ Vincent, James (2022年12月1日). “OpenAI\\'s new chatbot can explain code and write sitcom scripts but is still easily tricked”. The Verge.  オリジナルの2023年1月17日時点におけるアーカイブ。. https://web.archive.org/web/20230117070943/https://www.theverge.com/23488017/openai-chatbot-chatgpt-ai-examples-web-demo 2023年1月6日閲覧。\\xa0\\n\\n^ “Musk, scientists call for halt to AI race sparked by ChatGPT”. The Asahi Shimbun. 2023年4月5日閲覧。\\n\\n^ “Letter signed by Elon Musk demanding AI research pause sparks controversy”. The Guardian. 2023年4月5日閲覧。\\n\\n^ “Prompt Engineering for ChatGPT”. Coursera. 2023年5月28日閲覧。\\n\\n^ Bushard, Brian (2023年1月10日). “Fake Scientific Abstracts Written By ChatGPT Fooled Scientists, Study Finds” (英語). Forbes. 2023年2月3日時点のオリジナルよりアーカイブ。2023年1月30日閲覧。\\n\\n^ Stokel-Walker, Chris (January 18, 2023). “ChatGPT listed as author on research papers: many scientists disapprove” (英語). Nature 613 (7945):  620–621. doi:10.1038/d41586-023-00107-z. PMID\\xa036653617.  オリジナルのJanuary 30, 2023時点におけるアーカイブ。. https://web.archive.org/web/20230130142004/https://www.nature.com/articles/d41586-023-00107-z 2023年1月30日閲覧。.\\xa0\\n\\n^ Marche, Stephen (2022年12月6日). “The College Essay Is Dead”. The Atlantic. 2023年1月24日時点のオリジナルよりアーカイブ。2022年12月8日閲覧。\\n\\n^ Herman, Daniel (2022年12月9日). “The End of High-School English”.  オリジナルの2023年1月20日時点におけるアーカイブ。. https://web.archive.org/web/20230120053725/https://www.theatlantic.com/technology/archive/2022/12/openai-chatgpt-writing-high-school-english-essay/672412/ 2022年12月12日閲覧。\\xa0\\n\\n^ Stokel-Walker, Chris (December 9, 2022). “AI bot ChatGPT writes smart essays — should professors worry?”. Nature. doi:10.1038/d41586-022-04397-7. PMID\\xa036494443.  オリジナルのJanuary 17, 2023時点におけるアーカイブ。. https://web.archive.org/web/20230117192149/https://www.nature.com/articles/d41586-022-04397-7 2022年12月19日閲覧。.\\xa0\\n\\n^ Bowman, Emma (2022年12月19日). “A new AI chatbot might do your homework for you. But it\\'s still not an A+ student”. NPR. 2023年1月20日時点のオリジナルよりアーカイブ。2022年12月19日閲覧。\\n\\n^ Mitchell, Alex (2022年12月26日). “Students using ChatGPT to cheat, professor warns”. The New York Post.  オリジナルの2023年2月3日時点におけるアーカイブ。. https://web.archive.org/web/20230203051258/https://nypost.com/2022/12/26/students-using-chatgpt-to-cheat-professor-warns/ 2022年12月30日閲覧。\\xa0\\n\\n^ Allen, Mike (2022年12月26日). “Professor warns about chatbot cheating: \"Expect a flood\"” (英語). Axios.  オリジナルの2023年2月3日時点におけるアーカイブ。. https://web.archive.org/web/20230203051357/https://www.axios.com/2022/12/26/chatbot-cheating-university-warning-chatgpt 2022年12月30日閲覧。\\xa0\\n\\n^ Rosalsky, Greg; Peaslee, Emma (2023年1月17日). “This 22-year-old is trying to save us from ChatGPT before it changes writing forever”. NPR.  オリジナルの2023年1月18日時点におけるアーカイブ。. https://web.archive.org/web/20230118055050/https://www.npr.org/sections/money/2023/01/17/1149206188/this-22-year-old-is-trying-to-save-us-from-chatgpt-before-it-changes-writing-for 2023年1月18日閲覧. \"On January 2nd, Edward released his app. He named it GPTZero. It uses ChatGPT against itself, checking whether \"there\\'s zero involvement or a lot of involvement\" of the AI system in creating a given text. [...] Along these lines, one obvious application for GPTZero is to help teachers identify whether their students are plagiarizing their essays from ChatGPT.\"\\xa0\\n\\n^ Ropek, Lucas (2023年1月4日). “Did ChatGPT Write That? A College Student Created an AI Essay Detector” (英語). Gizmodo. 2023年1月4日時点のオリジナルよりアーカイブ。2023年1月4日閲覧。\\n\\n^ Tran, Tony Ho (2023年1月4日). “A College Kid Built an App That Sniffs Out Text Penned by AI” (英語). The Daily Beast.  オリジナルの2023年1月6日時点におけるアーカイブ。. https://web.archive.org/web/20230106162856/https://www.thedailybeast.com/princeton-student-edward-tian-built-gptzero-to-detect-ai-written-essays 2023年1月6日閲覧。\\xa0\\n\\n^ “New York City Department of Education Bans ChatGPT” (英語). GovTech. (2023年1月10日). https://www.govtech.com/education/k-12/new-york-city-department-of-education-bans-chatgpt 2023年2月16日閲覧。\\xa0\\n\\n^ Cole, Samantha (2023年1月4日). “NYC Bans Students and Teachers from Using ChatGPT” (英語). www.vice.com.  オリジナルの2023年1月5日時点におけるアーカイブ。. https://web.archive.org/web/20230105025338/https://www.vice.com/en/article/y3p9jx/nyc-bans-students-and-teachers-from-using-chatgpt 2023年1月5日閲覧。\\xa0\\n\\n^ Ropek, Lucas (2023年1月4日). “New York City Schools Ban ChatGPT to Head Off a Cheating Epidemic” (英語). Gizmodo. 2023年1月6日時点のオリジナルよりアーカイブ。2023年1月6日閲覧。\\n\\n^ 「ニューヨーク市、学校での「ChatGPT」利用を禁止」『CNET Japan』、2023年1月5日。2023年2月22日閲覧。オリジナルの2023年2月4日時点におけるアーカイブ。\\n\\n^ “港大禁用ChatGPT等AI工具，为全港大学首例”. China News Service. (2023年2月18日). https://m.thepaper.cn/newsDetail_forward_21956092 2023年2月19日閲覧。\\xa0\\n\\n^ “University of Hong Kong temporarily bans students from using ChatGPT” (英語). South China Morning Post (2023年2月17日). 2023年2月19日閲覧。\\n\\n^ Kelly, Samantha Murphy (2023年1月26日). “ChatGPT passes exams from law and business schools | CNN Business”. CNN. 2023年2月2日時点のオリジナルよりアーカイブ。2023年2月3日閲覧。\\n\\n^ “AI法廷の模擬裁判”. www.aimocktrial.com. 2023年5月20日閲覧。\\n\\n^ a b c 日本放送協会. “ChatGPTが裁判官？AIに裁かれる未来、受け入れますか｜NHK”. NHK NEWS WEB. 2023年5月20日閲覧。\\n\\n^ “Reddit Moderators Brace for a ChatGPT Spam Apocalypse”. Vice. 2023年4月12日閲覧。\\n\\n^ “ChatGPT使い偽情報を拡散か\\u3000中国で男拘束”. テレビ朝日. 2023年5月12日閲覧。\\n\\n^ “Chinese police arrest man who allegedly used ChatGPT to spread fake news in first case of its kind”. CNBC. 2023年5月12日閲覧。\\n\\n^ “China is about to get tougher on deepfakes in an unprecedented way. Here’s what the rules mean”. CNBC. 2023年5月12日閲覧。\\n\\n^ “サムスン、機密情報をChatGPTにリークして大問題に”. GIZMODO. 2023年4月17日閲覧。\\n\\n^ “Amazonが社員に注意喚起した｢ChatGPTに共有してはいけないモノ｣”. GIZMODO. 2023年4月23日閲覧。\\n\\n^ “ChatGPTなど生成ＡＩの企業利用、機密情報漏えいの恐れも－リポート”. Bloomberg. 2023年4月20日閲覧。\\n\\n^ “「チャットGPT」にEUが要求：情報源を開示せよ”. The Wall Street Journal. 2023年4月28日閲覧。\\n\\n^ “対ＥＵ、うごめく巨大ＩＴ\\u3000ＡＩ規制にらみロビー活動”. 朝日新聞. 2023年5月4日閲覧。\\n\\n^ a b 『読売新聞』2023年5月16日号 朝刊 1-2面\\n\\n^ a b “政府、法改正議論時にＡＩの著作権リスク説明せず…先進国で最も緩い法規制の枠組みに”. 読売新聞. 2023年5月30日閲覧。\\n\\n^ “新聞協会、生成AIに対する見解表明「記事や写真の無断利用を懸念」”. 朝日新聞. 2023年6月1日閲覧。\\n\\n^ “ChatGPT Teach-Out”. Coursera. 2023年5月29日閲覧。\\n\\n^ “OpenAIに行政指導\\u3000個人情報保護委、取得手法に懸念”. 日本経済新聞. 2023年6月4日閲覧。\\n\\n^ “政府、オープンＡＩに行政指導\\u3000病歴など個人情報侵害の恐れ”. 時事通信. 2023年6月4日閲覧。\\n\\n^ “オープンAI、米利用者が集団訴訟\\u3000「個人情報を違法に収集」”. 朝日新聞. 2023年6月30日閲覧。\\n\\n^ “OpenAIに集団訴訟、「ChatGPT」などの訓練用データ収集で”. ZDNet. 2023年6月30日閲覧。\\n\\n^ “米FTC、ChatGPTのOpenAIを調査\\u3000消費者保護で”. 日本経済新聞. 2023年7月19日閲覧。\\n\\n^ “米FTC、OpenAIを調査\\u3000ChatGPTの誤情報出力めぐり”. Forbes JAPAN. 2023年7月19日閲覧。\\n\\n^ Perrigo, Billy (2023年1月18日). “Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic” (英語). The Times. 2023年1月19日時点のオリジナルよりアーカイブ。2023年1月19日閲覧。\\xa0“One Sama worker tasked with reading and labeling text for OpenAI told TIME he suffered from recurring visions after reading a graphic description of a man having sex with a dog in the presence of a young child. \"That was torture,\" he said.”\\n\\n^ Vincent, James (2022年12月1日). “OpenAI\\'s new chatbot can explain code and write sitcom scripts but is still easily tricked”. The Verge.  オリジナルの2023年1月17日時点におけるアーカイブ。. https://web.archive.org/web/20230117070943/https://www.theverge.com/23488017/openai-chatbot-chatgpt-ai-examples-web-demo 2023年1月6日閲覧。\\xa0\\n\\n^ Getahun, Hannah. “Breaking ChatGPT: The AI\\'s alter ego DAN reveals why the internet is so drawn to making the chatbot violate its own rules” (英語). Business Insider. 2023年3月5日時点のオリジナルよりアーカイブ。2023年3月5日閲覧。\\n\\n^ Oremus, Will (2023年2月14日). “The clever trick that turns ChatGPT into its evil twin” (英語). Washington Post. ISSN\\xa00190-8286.  オリジナルの2023年3月6日時点におけるアーカイブ。. https://web.archive.org/web/20230306130400/https://www.washingtonpost.com/technology/2023/02/14/chatgpt-dan-jailbreak/ 2023年3月5日閲覧。\\xa0\\n\\n^ Goswami, Rohan. “ChatGPT\\'s \\'jailbreak\\' tries to make the A.I. break its own rules, or die” (英語). CNBC. 2023年3月2日時点のオリジナルよりアーカイブ。2023年3月5日閲覧。\\n\\n^ Taylor, Josh (2023年3月8日). “ChatGPT\\'s alter ego, Dan: users jailbreak AI program to get around ethical safeguards” (英語). The Guardian. ISSN\\xa00261-3077.  オリジナルの2023年3月8日時点におけるアーカイブ。. https://web.archive.org/web/20230308073202/https://www.theguardian.com/technology/2023/mar/08/chatgpt-alter-ego-dan-users-jailbreak-ai-program-to-get-around-ethical-safeguards 2023年3月8日閲覧。\\xa0\\n\\n^ Woods, Allan (2022年12月10日). “I wrote a story about ChatGPT\\'s AI. Then I dared it to write a better one” (英語). Toronto Star.  オリジナルの2023年1月6日時点におけるアーカイブ。. https://web.archive.org/web/20230106040655/https://www.thestar.com/news/canada/2022/12/10/i-wrote-a-story-about-chatgpts-ai-then-i-dared-it-to-write-a-better-one.html 2023年1月6日閲覧。\\xa0\\n\\n^ Rosenblatt, Kalhan (2022年12月2日). “An AI chatbot went viral. Some say it\\'s better than Google; others worry it\\'s problematic.” (英語). NBC News.  オリジナルの2023年2月3日時点におけるアーカイブ。. https://web.archive.org/web/20230203074007/https://www.nbcnews.com/tech/tech-news/chatgpt-ai-chatbot-viral-rcna59628 2023年1月6日閲覧。\\xa0\\n\\n^ Jain, Alka (2023年2月12日). “ChatGPT won\\'t crack jokes on women & Indians, netizens left guessing why”. Livemint (livemint.com).  オリジナルの2023年3月6日時点におけるアーカイブ。. https://web.archive.org/web/20230306165503/https://www.livemint.com/news/india/chatgpt-won-t-crack-jokes-on-women-indians-netizens-left-guessing-why-11676171036353.html 2023年3月6日閲覧。\\xa0\\n\\n^ Liles, Jordan (2023年2月1日). “ChatGPT Declines Request for Poem Admiring Trump, But Biden Query Is Successful” (英語). Snopes. 2023年3月22日時点のオリジナルよりアーカイブ。2023年3月23日閲覧。\\n\\n^ Guynn, Jessica. “Is ChatGPT \\'woke\\'? AI chatbot accused of anti-conservative bias and a grudge against Trump” (英語). USA Today. 2023年3月1日時点のオリジナルよりアーカイブ。2023年3月1日閲覧。\\n\\n^ Bray, Hiawatha (2023年2月9日). “Is ChatGPT liberal or conservative? Depends who you ask.” (英語). Boston Globe. 2023年3月1日時点のオリジナルよりアーカイブ。2023年3月1日閲覧。\\n\\n^ a b Vincent, James (2023年2月17日). “As conservatives criticize \\'woke AI,\\' here are ChatGPT\\'s rules for answering culture war queries” (英語). The Verge. 2023年3月1日時点のオリジナルよりアーカイブ。2023年3月1日閲覧。\\n\\n^ a b c Schiffer, Zoë. “Microsoft just laid off one of its responsible AI teams” (英語). www.platformer.news. 2023年3月23日閲覧。\\n\\n^ “インチキAIに騙されないために”. 2023年4月1日閲覧。\\n\\n^ “ChatGPT Teach-Out” (英語). online.umich.edu. 2023年6月17日閲覧。\\n\\n\\n関連項目\\n擬人観\\nバーチャルアシスタント\\nスマートスピーカー\\nチャットボット\\nBard - Googleによる会話型人工知能\\n外部リンク\\n公式ウェブサイト（英語）\\nChatGPT（英語） - ウェブ版\\nIntroducing ChatGPT（英語）\\nWhite paper（英語） - for InstructGPT, ChatGPT\\'s predecessor\\n@ChatGPTBot, an unofficial ChatGPT gateway（英語）\\nWhat Is ChatGPT Doing … and Why Does It Work?（英語） - Stephen Wolfram Writings\\n日本ディープラーニング協会 - JDLA緊急企画！「生成AIの衝撃」～ ChatGPTで世界はどう変わるのか？ ～ - YouTube\\n\\n\\n\\n\\n「https://ja.wikipedia.org/w/index.php?title=ChatGPT&oldid=97296239」から取得\\nカテゴリ: OpenAIチャットボット大規模言語モデルバーチャルアシスタントディープラーニング機械学習人工知能アプリケーション教師あり学習隠しカテゴリ: 編集半保護中のページウィキペディアとウィキデータで異なる公式ウェブサイト\\n\\n\\n\\n\\n\\n\\n 最終更新 2023年10月7日 (土) 10:33 （日時は個人設定で未設定ならばUTC）。\\nテキストはクリエイティブ・コモンズ 表示-継承ライセンスのもとで利用できます。追加の条件が適用される場合があります。詳細については利用規約を参照してください。\\n\\n\\nプライバシー・ポリシー\\nウィキペディアについて\\n免責事項\\n行動規範\\n開発者\\n統計\\nCookieに関する声明\\nモバイルビュー\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n本文の横幅制限を有効化／無効化\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee287c6-0839-4b52-8b8e-4f1cfbbd3d7a",
   "metadata": {},
   "source": [
    "### YouTubeの文字起こしを取得\n",
    "YouTubeのtranscriptを取得するためにはyoutube-transcript-apiをインストールする必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed4f722f-cf9d-4c5c-abf1-cc50f35b739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: youtube-transcript-api in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (0.6.1)\n",
      "Requirement already satisfied: requests in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from youtube-transcript-api) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests->youtube-transcript-api) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests->youtube-transcript-api) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests->youtube-transcript-api) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ken/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from requests->youtube-transcript-api) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c50026a-9ac7-4c32-b20a-9b4009021120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "youtube = YoutubeLoader.from_youtube_url(\n",
    "   youtube_url=\"https://www.youtube.com/watch?v=I2UZutRrtEE\",\n",
    "   language=\"ja\"\n",
    ").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "601cd764-3c1e-4b8c-9fba-0f055edd371d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='では早速なんですけれども始めさせて いただきます私からはラングチェイン エージェントを使って自社ツールと チャットGPTを連携するというタイトル で発表させていただきますChatGPT 自体は皆さんご存知のように非常に優れた 能力を持っていますが最新のデータに アクセスできないとか特定の数学的な推論 が得意でないなどいくつかの課題が存在し ますこれらの課題は自社プロダクトに チャートGPTを導入するなどチャット GPTを社会実装しようとした際に解決し なければなりません今回お伝えするラグ チェインエイジェントを導入すると ChatGPTと外部ツールをうまく連携 させることができるようになりChat GPTが抱える課題を克服しチットGPT の可能性をさらに広げることができます 本日はラングチェインエージェントをどう 活用することでチャットGPTの能力を 拡張できるのか具体的にご紹介していき ますそれでは始めていきますこちらが私の プロフィールとなります内容はいつもと 同じなのでスキップさせていただきます はいでは具体的な話に入る前にですね今回 のテーマの結論に先に触れておきます結論 なんですけれどもラグチェインには様々な エージェントの種類が存在するが自社 ツールとチャットGPTを効果的に連結 する際汎用的なエージェントタイプである ゼロショットリアクトまたはオAI ファンクションを利用することが簡単と なりますゼロショットリアクトとOpen AIファンクションを比較するとOpen AIファンクションの方がチットGPT からより適切かつ精度の高い答えを 引き出しやすくなっていますこちらはデモ のでお見せしますねでそしてえっとここで ChatGPTと自社ツール連携で非常に 重要となるものとしてツールの説明という ものがありますと通常のプロンプトチャト GPTを使う時のプロンプトと同様に ツールの特性や機能をChatGPTに 正確かつ効果的に伝えることで自社ツール とチャットGPTの連携の成功率が格段に 向上していきますこのポイントに着目し ながら具体的な方法論について今日は 深掘りしていきたいと思いますそれでは これらのポイントを年頭にきつつ今回の 発表をお聞きください本日の発表は以下の 3つのセクションから構成されています 初めにチットGPTの基本的な概要につい て触れます次にラングチェイン エイジェントの特徴について紹介し最後に デモンストレーションを行いますデモでは 特にツールの説明について着目して いただけると幸いですそれでは最初の トピックに移りましょうここではチット GPTの全体的な特徴を振り返っていき ますすでにご存知の内容も多いと思うん ですけれども一般的なはチャットGPTの 性質っていうものをご紹介していきます チットGPTは実用的な大規模言語モデル llmで現在多くのビジネスに活用されて きています例えばベネッセ ホールディングスでは車内AIChat ベネッセGPTを開発しグループ社員 1.5万人に向けて提供しているという ようなプレスを出していたりとか立メカ 大学では大学の英語事業において機械翻訳 とChatGPTを組み合わせたサービス が試験導入されているであったりとか他に も色々なサービスがチャトGPTを 組み込み始めておりChatGPTの実用 化っていうものがすごい勢いで広がってい ますChatGPTを活用する際によく 言われることとしてはチャットGPTから 良い結果回答を得るためには良い質問が 必要であるということですでこれは プロンプトエンジニアリングと呼ばれる 領域で今最も伸びている業種の1つと言え ますプロンプトエンジニアリングではどの ような項目をプロンプトに入れるべきか またプロンプトを複数回うまく使いこ なくして良い回答をるテクニックなどが 研究開発されていますリアクトであったり とかいろんなことを聞いたことあると思う んですけれどもプロンプトの種類っていう のがどんどんどんど開発されているという ことですでチャットGPTの活用は多に 渡っておりその可能性は広がりつつあり ますでプロンプトエンジニアリングの テクニックを活用することでチャット GPTとの適切なコミュニケーションが 可能になりますプロンプト エンジニアリングでは色々なテクニックが 存在するのですが今回はプロンプト エンジニアリング自体がメインのテーマで はないのでプロンプトエジライニングの中 で抑えるべき基本的な項目とそしてよく 使われるまフーショットプロンティングと 呼ばれるような手法を簡単にだけ紹介して いきますはいまず基礎として抑えていき たいこととしてプロンプトに何を与える べきかっていうものをいくつか紹介して いきますで1点目としては具体的で明確に 質問することですで例えばその日本の歴史 において重要な出来事は何ですかという 質問よりも日本の戦国時代における重要な 出来事について教えてくださいという具体 的に質問し方が的確なコ答が得られますと で2点目に状況やコンテクストを明示する ことですて背景情報があるとチャット GPTはコンテクストをより理解し適切な 回答が返ってきますとで3点目期待する 回答の形式を指定するとこれにより望む 情報が整理されて帰ってきますリスト形式 で返してくださいであったりとか ジェイソン形式で返してくださいであっ たりとかまそういうことを質問する時に プロンプトの中に入れるということですね で4点目範囲を限定すること範囲が広す すぎると回答も後半で一般的になりがちに なりますなので範囲を狭めるということが 重要となってきますで最後5点目自性や 認証を明確にすることですと例えば子供で も分かるようにしてくださいとか中学生で も分かるようにしてくださいとかなんか そういう風にどの視点で書て思ってみるか を示したりであったりとか何年の情報が 欲しいとかそういうことを指定することに よって回答が明確なものが帰ってきやすく なるということですで次にプロンティ テクニックの1つののフューショット プロンティについて簡単に説明します フューショットプロンティはモデルに少数 の例を示すことで特定のタスクの解決方法 を学習させるアプローチになりますこれに よりモデルは様々な問に対して適切な回答 を生成することができますここに例として 上げてるんですけれども例えばレストラン 最高でしたこれは感情ポジティブですこの 映画は本当に時間の無駄だった感情 ネガティブですというようにいくつかの例 を与えて出したいものでそれぞれの文に 対しての感情っていうものをこのように 並べていきますてで最後に聞きたいものと してそのサービスは非常に遅く不満ですっ ていうものがあった時にどんな感情で すかっていうことを聞くと感情ネガティブ という風に返ってくるというところですで こういう風にすることによってモデルって いうものが新しい文章がポジティブか ネガティブかっていうのをま過去の例を元 に判断することができるようになるという ことですでこの方法によってですねフュー ショットプロンティはモデルに多様な タスクを効率的に学習させることができ 柔軟な対応が可能になってくるということ でですでここまでいくつかのプロンティの テクニックであったりとかをお話ししてき ましたもちろん他にもたくさんあってです ねチェインオブソツであったりとかあと セルフアスクであったりとかいろんなもの があるんですけれども今回のメインテーマ ではないので一旦プロンティグの話はここ で終わりにしてでここでちょっと一息つい てですねこれらのテクニックが持つ限界に ついて考えてみましょうでまず1点目なん ですけれどもプロンプトのテクニックは 日々進化し開発されていますとこれによっ てチットGPTは多様なタスクや質問に 対応でできるようになってきていますただ 全ての作業やタスクをチャットGPTに 依存すること自体が現実的ではありません とえ例えば先ほどの例で出てきた文章から の感情抽出に関してはチャートGPTに 無理やりさせることが適切かと言われると そうでもないかもしれないAWSコプリン のような勘定分析に特化した学習をして いるAPIっていうものがあってそれを 使った方がいい結果が返ってくるかもしれ ませんっていうのがまず1つ目ですねで次 に2点目としてそのChatGPTが持つ 情報っていうものは訓練データに限定され ていてチットGPTはそれ以外のデータに ついては知らないというところですなので えっとチャットGPTは過去のデータで 学習されておりその学習データ以降に出て きた情報に関してはチットGPTに尋ね られませんとよく言われるものとしては 2021年のデータですかねまでのデータ で学習してるので例えば2022年であっ たりとか2023年のデータについて聞い たとしてもチャットGPTがうまく答え られないというようなところですね最新の 価格的な研究であったりとかニュース記事 とか特定の地域の天気情報であったりとか チットGPTの訓練データに含まれてい ないような情報っていうものはChat GPTだけでは取得できませんし インターネットにあるデータだけではなく て例えば今回のテーマである自社のツール と連携するっていう時のためには自社の データを使いたいみたいな話とかあると 思うんですけれども自社データについては チャットGPTが知るわけもないという ことでえっとChatGPTに自社の データについて聞いたとしてもChat GPTはもちろん答えることができません とこれらのチャGPTの限界を理解する ことでチットGPTの適切な使用方法で あったりとかそれらをどのようなシナリオ で活用するべきかっていうのが見えてくる ということですねでChatgpdの概要 の最後なんですけれどもChatgpdや その他大記号言語モデルllm自体の課題 をこちらにまとめますでまず1点目なん ですけれども先ほどと被るんですが ChatGPTは最新の情報にアクセスが できないと例えば為替や株価のように国 一刻と変化していく情報に対してチト GPTははリアルタイムで対応することが できませんで2つ目の課題としては ChatGPTが独自の情報源にアクセス する能力を持たないということです企業の データベースに保存されている顧客名簿で あったりとか特定のプライベートな情報に はアクセスすることができませんで3つ目 に水論の正確性に課題があるというところ ですチャットGPTは時折り簡単な算術 計算でさえも間違えることがありますこれ は特に正確な情報や結果が求められる タスクにおいて大きな問題となりますねで そして最後にチャットgpdができない こととかのデータが足りないんだったら ファインチューニングすればいいんじゃ ないかっていうような意見もあると思うん ですけれどもファインチューニングする ことによって汎用性が劣化してしまうと いうことがありますとでChatGPTを 特定のタスク例えば自社データを食わせる であったりとか特定のタスクに特化した データを食わせてファインチューニング すると他の多様なタスクに対する性能が 低下すると言われていますでこれらの課題 を解決するための1つの手段として今回お 話しするラングチェインエージェントって いうものがありますでここここからは ラングチェインエイジェントを使って チットGPTの性能を生かしつつ苦手な ものは他のツールで解決していくっていう ことについてお話ししていきますで参考 までにですねここにバッと上げている課題 に関してはMrKLシステムズていう有名 な論文があってそ論文から引っ張ってきた ものとなりますMrKLシステムズって いうのはミラクルシステムズと呼ぶもので チャットGPTと他のツールを連携して どんどんどんどん課題を解いていくって いうことを提示しているロムとなってい ますそれではラングチェインエージェント についてのトピックに移っていきますまず ラングチェインエージェントの前にですね ラングチェインとは何かと言うとラング チェインとは言語モデルを利用する アプリケーションのためのフレームワーク ですでラングチェインについては前回の 勉強会でもご紹介しているので基礎的な 内容を知りたい方はYouTubeで前回 の勉強会の動画をご覧くださいでラング チェインエージェントはラングチェインの 中の1つの機能なんですけれどもラング チェインの中でも非常に面白くて重要な 役割を果していますランチェの主なタスク は次にどんなアクションを取るべきかを大 記号言語モデルllmに決定させそれを 実行するということですで具体的な プロセスは以下の通りですとまず各ツール ができることと質問を指定したレスポンス のフォーマットでllmに送信します聞き ますとチットGPTなどのllmに聞き ますそしてllmChatGPTから帰っ てきたレスポンスをパースして次の アクションを決めるということをしますで このプロセスによってラングチェインジと はユーザーの4級に応じて効率的かつ正確 なアクションを実行できるようになって いるというところですでここでラング チェインエージェントがどのように動く かっていうもののイメージを具体的な例と して1つ持ってきましたま正確に正しい 動き方をしてるわけではないんですけれど も大体こういうような動きをするんだよっ ていうこともご理解ください例えばチット GPTに対してコミュニケーションを取る のがラングチェインのエージェントとなる んですけれどもラングチェイン エージェントにですねまずこう今日東京で で出かける時時に適切な服装は何ですかて いうのを聞きますでそうするとラング チェインエージェントが聞きたい内容今日 東京で出かける時に適切な服装っていう ものとあと自分が持っているツールって いうものをプロンプトに入れてチャット GPTに聞いてくれますとで自分が持っ てるツールっていうものは例えば Google検索のツールっていうも持っ てますとGoogle検索のツールでは 最新の情報を引っ張ってくることができ ますと私が持ってるのはGoogle検索 のツールで最新の情報が引っ張ってこれ ますそしてで今回聞質問としては東京で 出かける時に適切な服装って何でかていう ものを聞きますとでその時にチャット GPTはどのように返してくるかというと でまずは検索ツールでインプットは東京の 気温っっていうもので調べなさいっていう ものをエージェントの方に返してきますで エージェントはチャットGPTからから 教えてもらったじゃあえ検索ツールを使え ばいいのかっていうところで検索ツールを 使おうとしますでインプットとしては チャトGPTが教えてくれたように東京の 気温っていうものをインプットとして入れ てで検索ツールから帰ってきた22度って いうものを変してもらいますとでそして またえっとチャットGPTにえこのツール に入れた答えってものが22°でした次何 すればいいんですかであったりとか最終的 にどうなんですすかっていうのをチャット GPTに聞いてチャットGPTが十分な 情報を得たっていう風に判断するとでは 22°なので薄の長袖でがいいでしょうっ ていうものをエージェントとしに返してで あとはえ答えとしてラングチェインの エージェントが東京は22度なので素の 長袖でがいいでしょうっていうものをその まま返してくるというような流れになり ます特徴的なものとしてはチャットGPT だと今日の天気であったりとか東京の天気 であったりていうものは取れないんですね でそこをGoogle検索のツールって いうものを利用することによって新しい データっていうのを取れるようにしてると いうところですねもう1つ押えておきたい こととしてはこの問を解く時に自分が持っ ているツールっていうもの一覧を渡して どのツールを使えばいいかっていうのこと 自体もチャットGPTに判断してもらうっ ていうところが面白いところとなってい ますはいでこれがラングチェイン エージェントリアクトフレームワークとか 呼ばれたりするようなやり方ですねはい これらのこと全部ラングチェインの エージェントがやってくれるというところ ですでラングチェインエージェントって いうものは様々なタイプのエージェントが 用意されていますとここでエージェントの タイプっていうものを紹介するんです けれども解きたい課題に応じて エージェントを使い分けることによって 精度の高い回答を得られるようになると いうことですここで6個ほど上げています でまず1番汎用的なものとしてはゼロ ショットリアクトエージェントというもの ですでこのエージェントはリアクト フレームワークを利用してツールの説明に 基づいてどのツールを使用すべきか判断し ます先ほどの例で出てきたものと同じです ねこれによって特定のトレーニングデータ なしで多様なツールの適切なこう利用が 可能になってきます次にストラクチャード インプットリアクタエージェントですで このエージェントは複数の入力を使う エージェントを活用することができますで これによってより複雑なツールの理由用が 簡単になりタスクの効率が向上しますと 続いてOpenAIファンクション エージェントですこのエージェントはオ AIのファンクションコーリングの機能を 使っており関数が呼び出されるタイミング であったりとかを検出したりとか適切な 入力を関数に渡すような微調整がえオープ AI自体チットGPT自体がされていて それを使うことができますとOpenAI のえファンクションコーリングこの エージェントが使うOpenAIの ファンクションコーリングについてはこの 後のスライドでもう少し詳しく説明します で次にコンバーションエージェントですで このエージェントは会話での使用目的とし て設計されています何かを聞いて何か答え を出すっていうようなま単純なチャット GPTで何かこう答えを作ってくださいと かそういうのとは違くて会話的な インタラクションを通じて効果的な コミュニケーションを可能にするような エージェントとなっていますそして次に あげるものとしてはセルフアクwith sarchエージェントですねでそこの エージェントはですねセルフアクという 質問を自問自としながら分割することに よって正しい解を導き出す方法を使って 情報を検索するエージェントですセルフア クっていうプロンティグの技術があるん ですねまそれを使ってちゃんとした正しい 答えを導き出すエージェントとなってい ますで最後に紹介するものがリアクト ドキュメントストアエージェントですで このエージェントはドックストア ドキュメントストアですねと対話するため にリアクトフレームワークを使用してき ますえこれによって文書の管理であったり とか検索が一元化されて効率的に情報 アクセスが可能となりますこちらに関して はまた今後の勉強会でも取り扱っていき たいかなという風に思ってるんですけれど も文書検索管理っていうものに特化した エージェントとなっていますで以上がラン チェインエージェントの各エージェントの 使用となりますでそれぞれのタイプが 異なるシナリオやタスクに適しており ラングチェインの柔軟性と多様性を象徴し ているものとなります今回のセッションで はですね最も汎用的なゼロショット リアクトとチャットGPT自体に色々判断 させるファンクションコーリングを使った オープAIファンクション2を利用して いき ますでは先ほど説明を省いてしまったん ですけれどもOpenAIファンクション コーリンググっていうのはOpenAI ファンクションコーリングじゃないですね OpenAIファンクションですね OpenAIファンクションとは何か エージェントタイプの1つであるOpen AIファンクションとは何かについてお 話ししますこちらはChatGPTを提供 しているOpenAIのファンクション コーリングという機能を利用していますで ファンクションコーリングっていうものは 2023年の6月にリリースされた機能で ラングチェインエージェントが担っていっ たツールを渡してチャットGPTに聞いて 次のステップを決定するやり取りってい ものをチットGPT自体が行えるように ファインチューニングされているものと なりますでこの機能では東京の天気を教え てくれというような先ほどと同じように ですね質問と共にどういう関数を持って いるのかその関数の説明をチトGPTに 直接投げるということをやりますなので 先ほどのラングチェインエイジェントが やってることと同じようなことをやれる ようになっていてチットGPTが ファンクションをどのように受け取るのか そのファンクションのパラメータであっ たりとか説明とかをよりチャトGPTが 理解しやすいようにAPIとして受け取る ことができるようになっているのでより 適切な判断をチャットGPTがしやすく なっているというところがメリットとなり ますなので今メリットをお話ししてしまっ たんですけれどもファンクション コーリングを使うメリットとしては ChatGPTにツールの挙動をより正確 に伝えやすくなっているというところで これによってChatGPTが機能の ツールのえファンクションの使用や動作を より正確に理解しやすくなっていて適切な 判断をしやすくなっているとていうのとで もう1つのメリットとしてはチャット GPTっていうものがラングチェイン エージェンドって元々チャットGPTが 返してきた個体を利用者が頑張って定期 表現を使って解析する必要があったんです がそれをする必要がなくなったっていう ところが利点としてありますとでチャット GPTが実際にラングチェインを使って どのツールを使えばいいですかていうのを 聞くんですけどそれをどのようにパースし てるかというとこのように頑張ってですね えっとレギュラーエクスプレッションを 使って正規表現を使ってパースしたりとか して次のアクションを決めてたりするって いうのがラングチェが今までやっていた ことであるとでこういうパースをしなきゃ いけないっていうことをしなくて済む つまりパースしなきゃいけなかったので今 まではそのパースすることでえっとうまく パースできなくてミスってしまうっていう こともあったんですけどそういうことが なくなるっていうことが期待されるという ところです はいはいちょっと駆け足になってしまった んですがここまでで20分ぐらいかかって しまいましたねえここからがレモに入り ますで実際にですねラングチェイン エージェントを使ってチャトGPTと自社 ツールを連結するような感じのえデモって いうものをお見せしていきますで使う エージェントとしてはゼロショット リアクトとオーエファンクションを使って いきますであとは今回のプレゼンの冒頭の 方でもお話ししたようにChatGPTに ツールの説明をどのように渡せばいいのか であったりとかその辺りを持ち替えて いただければ幸いです はいではではやっていきますちょっと時間 がなくなってきたので駆け足でいきますね でまずは設定ファイル読んでいきますこれ 何をやってるかと言うとえOpenAIの APIキーであったりとかを読んでいます でまずはですねえっとゼロショットの方を 見せしていきたいんですがおマジないとし てですねおじないというかま前回の勉強会 と同じようにですねラングチェイン エージェントを使うためのやり方をお見せ していきますで実際今回使うモデルとして はチットOpenAPAIというか ChatGPTのえGPT3.5のtbo 0613というものを使っていきますで 外部のツールとしてはGoogleサーチ のAPIっていうものを今回用意しました ここであのサプAPIっていうものを書い ているものがえGoogleのAPIと なりますで聞きたい質問としては今日の 東京と北海道の天気を調べて東京から 北海道の出張で来ていくべきを教えて くださいっていうものを聞きたいと思い ますでラングチェインエージェント自体を 走らせてみますでこれがゼロショット リアクトエージェントを使った場合どの ようになるかっていうのをお見せしますで エージェントタイプに関しては イニシャライズエージェントの エージェントっていうえ引数に対して エージェントタイプのこのゼロショット リアクトディスクリプションていうものを 渡すとエージェントを選択することができ ますちなみになんですけれどもえこれって ただあの文字列が返ってくるだけですかね あのゼロショットリアクト ディスクリプションていうものが返って くるだけのものになりますなので文字列で 与えても同じですはいでは実行してみ ましょうチトgpdはもちろん今日の天気 とか今日の東京とか北海道の天気っていう のは知らないので来ていくべき服装って いうことは分からないわけですね気温も 分からないのででこれでラングチェイン エージェントが私はGoogle検索の ツールを持ってますよでそれを使って使う ことができますでこの問題課題っていう ものを解くためにはどうすればいいですか てのを聞くっていうことですねでここで ざーっと出てきてるんですけれども実際に 書いてきた答えとしてはtheWisto isCloudyaboutcrear withtempof歌詞になっちゃって ますけどまこのように帰ってきますと 英語で帰ってきちゃってますねでこういう 風にえどんなアドレスをれ切ればいい かっていうものがちゃんと帰ってきてい ますと前にやった時はそんなに綺麗に帰っ てきてなかったんですけれども今回は うまく帰ってきていますはいこちらがゼロ ショットリアクトディスクリプションで やった場合の結果になりますでどういう ことをやってるかっていうのなこの辺りを 見ていただくといいのかなという風には 思いますでチットGPTにまずラング チェインエージェントが問題とツールを 渡してチットGPTに何をすればいい かっていうのを聞きますでチャットGPT から帰ってきた答えとしては次の アクションとしてはサーチをしてください ですサーチのインプットとしてはウザー 東京todayていうものを渡してくださ いっていうのを渡してますで次の アクションとしてはサーチをしてください でそのインプットとしてはウザ北海道 todayていうものを渡すようにして くださいとで最終的にその帰ってきた答 っていうものをチャットGPTに渡して 最終的な文が出てくるということですはい でこれが1つ目ですねアエージェント タイプ1つ目のえゼロショットリアクト ディスクリプションで元々ラングチェイン エイジェントがリアクトフレームワークで ツールを渡したりとかしてチャットGPT とうまく連携しながら外部ツールを使う ようにしたえ機能となっていますでこれが ですねえ最近出てきた6月にリリースされ たOpenAIのファンクションコーリン グっていうものを使うとどうなるかって いうのをお見せします はいでOpenAIファンクション OpenAIのファンクションコーリング を使うためにはエージェントタイプの OpenAIファンクションズっていう ものを渡しますでそうするとえこのように 動きますで実際にやってることとしては ラングチェインエージェントがOpen AIのファンクションコーリングっていう ものを使ってうまくチャットGPTと やり取りしながら答えを導き出していくっ ていうことをやっていますはい出てきた 答えはこちらですねで同じようにですね どのようにやっているかっていうものは ここら辺の途中で出てきているんです けれども途中結果でえサーチとしてはえ クエリー東京の今日の天気っていうものを 調べてくださいでサーチっていうツルを 使って今日の北海道今日の天気っていう ものを調べてくださいでこれらが帰ってき た答えっていうものをえチャットGPTに 返してチャトGPTがその与えられた情報 を元に東京の今日の天気は曇りですが後に は晴れる予報ですで気温は接し30°歌 86°でコス確率はどうのこうのっていう ものがあってであとは東京は暑くなる可能 性があるので軽い服装が適しています ただし北海道は比較的涼しいので ジャケットなどを持っていくことがお勧め しますというような感じに帰ってきてます ま見て分かると思うんですけれどもま日本 語にな質問が日本語だったので日本語で ちゃんと返してくれるっていうのは もちろんのことなんですけれども書いてき た答えっていうものに関してより詳しく 返してくれているっていうことが分かると 思いますなので使ってみた所管としては ですねここで言うオAIファンクションの ファンクションコーリングっていうのの ファンクションっていうのはラング チェインエージェントでいうツールと同じ なんですけれどもそのファンクションで あったりとかツールを渡す時にですね渡し て何かを判断判させたいチャットGPTに 判断してもらいたいっていう時はえ エージェントタイプゼロショットリアクト ディスクリプションよりもエージェント タイプオAIファンクションの方がより いい結果が返ってきやすいっていうことが 分かると思い ますもう少しあの詳しく見ていきます ちなみにですねOpenAIにどんな ファンクションを渡してるかっていうのは イニシャライズエージェントのところで エージェントを定義してるんですけども そのエージェントの中からエージェントっ ていうものとファンクションズってものを 実行するとどんなえファンクションが渡さ れてかっていうのを見ることができますで 今回渡されたファンクションっていうのは ネームサーチっていうファンクションが 渡されていてどんなことができるかって いうのでサチエンジンですっていうような ものをやってますであとはパラメーターと してはこういうものを渡していきますって いうのが分かりますとはいでここまでが 基本的なラングチェインエージェントを 使って外部ツールを連携するっていうお話 をしてきましたとで次にですねえ自社で 作ったツールっていうものを連携させ るっていところについてお見せしていき ますで一旦はですねレコメンドアイテムっ ていうものを考えてみようと思いますで ここ何をやりたいかって言うとレコメンド アイテムまこの中身はほとんど意味のない ものになってるんですけれどもレコメンド アイテムっていう関数を自社で持っていて でレコメンドアイテムっていうものは自社 のデータベースなどにアクセスして日付を 持ったにこの日のレコメンドっていうもの はこういうものですっていうものを返す ような関数っていうところになりますとな のでえっとこのレコメンドアイテムって いうところをこの関数ををえChat GPTのフローの中に組み込みたいって いうことをやっていきますでどのように ラングチェインエージェントを使って ChatGPTに連携すればいいかという とツールっていうものにえ変換していく 必要がありますとでレコメンドアイテム ツールっていうものをえツールfrom ファンクションズっていうものを使って どのようなファンクションを実行したい ですかレコメンドアイテムっていう ファンクションを実行したいですまこう いう風にですねこの今回のあまり意味の ないファンクションなんですけれどもこれ を実行しますとでツールの名前としてはえ レコメンドアイテムっていうものを渡し ますでツールのディスクリプション何を やってくれるかっていうことに関してはお すめの商品を教えてくれますっていうのを ツールとして定義するというところです はいでこれをちょっとツールズっていうま ツールまとめて渡さないといけないで ツールの配列の中に入れておきます実際に ですねラングチェインエージェントにどの ようにツールを渡せばいいかっていうとま 先ほどもカラっとやってたんですけれども このツール1覧っていうのを最初に渡して 引数として渡しているのでまこのようにし てどんなツールが使えるかっていうものを ラングチェインのAに渡しでチャット GPTと連携していきますはいでえっと 今回聞きたいこととしてはえおすすめの 商品を教えてくださいっていうものを投げ てみようと思いますで使うエージェントと しては先ほどゼロシトリアクトよりもより いい結果が返ってきたオーエ ファンクションズってものを選びます はいはいでは実行し ますはいで書いてきたものとしてはですね コメドアイテムってものが実行されて withファッションという謎のものをえ 入力に入れてで実際この引数がですね デートの中にファッションが入ってで プリントファッションっていうところでこ でファッションが出てきてしまってるとで ただおすめの商品は今回レコメンド アイテムで書いてきたAppleですって いうのが書いてきてることが分かると思い ますはい見て分かるようにですねなんか よくわからない引数は与えてしまってる なっていうところがありますでこを どんどん解決していこうと思いますで次に やることとしては今回なんでこのように変 なファッションとかいう引数を与えて しまったかというと引数に関する情報が ないんですね与えられてないというところ になりますとなので引数何与えればいいん ですかみたいなところが情報としてないが ゆえにチャットGPTもレコメンド アイテムを使うことは分かるとレコメンド アイテムを使えばいいのは分かるんだけど レコメンドアイテムに対してちょっと 分からないからカテゴリー情報でも渡して みようかなっていうのでファッションを 渡してしまっているというような状態に なりますで例えばまファンクションを見た としても今回使ってるファンクションに 関してはレコメンドアイテムでおすめの 商品を教えてくれますでパラメーターの 情報は全くありませんっていう状態なので どんなパラメーターを渡せばいいかって いうのはオープAIファンクションは 分からないというところですねじゃあこれ に対してどうすればいいかっていうのをお 見せしますでえっと結構簡単でしてパダ ティックっていうものをご存知ですかね こう最近えっとファストAPIとかですか ねこう結構パであったりとかそのヒントを 与えるっていうことがまPython主流 になってきてると思うんですけれども フダンティックを使っていきますでえっと ご注意していただきたいこととしては ファインディッてもうV2が出てるんです けれどもV2を使うとラングチェイン エージェントを動きませんとなので パティックV1を指定して方に関する情報 をえ与えるためのそのベイスモデルとか フィールドを使ってえやっていきますで これ何をやってるかと言うとま今回の インプットに対してデイトっていうものが ありますがデイトていうものはお勧めした 商品を絞るために使われる日付ですって いうものを与えていますこれをえっと アーグススキーマ先ほどのツールを定義 する時にアグススキーマっていう引きを 使ってこのレコメンドアイテムインットっ ていうベースモデルから継承した レコメンドアイテムインプットっていう インプットの報引数の情報を持ったクラ スっていうものを渡すということをします とでこうするとどうなるかっていうのをお 見せしていき ますはいじゃあどう変わる かいきますはい実行します全く同じですね 実行するものとしてはで今回はレコメンド アイテムっていうものに対して先ほどと 同じレコメンドアイテムっていうものが 選ばれましたとコアイテムに対してデイト が2020年の12月1日っていうものを 渡しているっていうことが分かりますで それによってですね気付がちゃんと渡して くれたっていうことがわか分かるわけです ねで今回あのここであのデートっていう ものはディスクリプションでお勧めしたい 商品を絞るために使われる日付っていう ことを伝えたのでチャットGPTの方があ これは日付のことなんだっていうところで 引数はファッションのようなカテゴリーを 渡すんではなくてデートの引数っていう ものを渡すことができてるというところ ですはいでこちらがデトのまあ2022年 の12月1日もはや日付が意味が分から ないですけれどもこういうものが渡されて いるというところですねでおすめ商品は Appleですという風にでこういう風に ですねこうツールの説明をどんどん どんどん入れていってええ解決していく 必要があるというところですねで同じよう にですねちょっとエージェントの ファンクションがどのように渡されるてる かっていうのをお見せしますとで今回渡さ れたのはレコメンドアイテムインプトって いうのをえパイダンテックの方で定義して 渡しているのでえ実際にえっと ファンクションOpenAIのえ ファンクションコーリングにどのように 渡されてるかというとパラメーターとして ですねこういう風なレコメンドアイテム インプットっていうものが渡されていて プロパティとしてはデートがあってデー トっていうのはお勧めしたい商品を絞る ために使われる日付っていうものが入れ られてえ飛ばされてるということが分かる と思いますはい で他にはですねこのようにツールフロン ファンクションで作るっていうこともある んですけれども同じようにですね デコレーターを使うっていうこともでき ますとデコレーターの方が結構シンプルに 書けるので私は結構デコレーターを使って 書くことが多いですとでデコレーター何を すればいいかというとデコターっていうの はま関数の前にですねアマークツールなん ちゃらかんちゃらっていうものがあってで それを使ってえっとこのツールってのは何 ができるかっていうものをえドッ ストリングで与えるんですねな先ほどと 同じにするとレコメンドアイテムツールっ ていうものがあったら中身は同じですね プリントデイトとAppっていうものを 返しますとプリントデイトしてApple っていうものを返しますとででやることと してはおすめ商品を教えてくれます全く 同じ内容ですねでこのツールの名前って いうものはデコレーターの引数として レコメンドアイテムとして渡していると いうところですで同じようにですね ツールズの一覧として入れてでこれをえ 実行してみ ましょうはいこれデコレーター使った場合 です ね実行するものは全部同じですで同じよう にこれも実行できますという風にデコレー ターってかなりこうシンプルにかけてで ファンクションの中のドックストリングス とかを使ってくれるので結構あの家読性と かもう悪くないかなという風には思います 先ほどと同じようにですねこレコメンド アイテムってものはちゃんと選んでくれた んですけれどもレコメンドアイテムに渡し ているデートっていうのは2021年10 月15日という意味の分からない日付を 渡してしまっていますとこれはなぜかと言 とまもちろんそのおすすめ商品を教えて くださいって言ってるので勝手に日付を 決めて聞いてしまっているというところ ですね はいでデコレーターの方もですね同じよう にファンクションを見てみるとえっとこう いう風になるという感じですねただあの 先ほどと同じようにですねパダンティック とか使っていないのでプロパティの パラメーターとかは適当になってしまって いるというような感じになりますただ今回 の場合はですねこうデコレーターの場合は フロムファンクションの方を使ったこっち の場合ですねこのフロムファンクションを 使った時と違ってきちんとデートを入れて くれているっていうことが特徴的かなと いう風には思いますフロンファンクション を使った場合っていうのはパダンティック を渡さないとデートっていうものに対して ファッションを入れたりとかよく分から ないものを入れたりすることがあったと 思うんですけれどもデコレーターを使った 場合はこのデートっていうものを渡した時 にきちんとこうデートを返してくれるよう な情報っていうものを渡せてくれている ような感じですねいずれにせを正しい共同 を求めるためにはこのままあまり情報を 与えないで実行するってのは適切ではない ので特に自社ツールサービスとして提供 する側としてはですねなのでそういう意味 ではきちんとバイダンティックなどを使っ て適切な情報を渡しながら関数っていう ものを渡していく必要があるかなという風 に思いますなのでえっと先ほどと同じよう にですねデコレーターを使った場合も パイナティックの情報っていうものを渡す ことができるのでデコレーターの引き数に アーグススキーマていうものを渡してで そのパイランティックの情報を渡すことで 適切な答えを返すことができるようになる というところです同じようにですねま結果 も変わらないので はいまこういう風に変えてきました ちょっと答えがどんどんどんどん変わって きてますApple製品だったりとか AppleStoreとかオンライン ショップとかうんまここら辺に関しては 本当に適切な答えがまだ返ってきてない なっていうような状態だとは思います エージェントのファンクションを見るとえ きちんとファナティックで渡した情報って いうのが渡っていることが分かると思い ますこのようにですねこのデートっていう ものはデートっていう引き数はお勧めし たい商品を絞るために使われる日付ですっ ていうのが渡っているというところです もう少しですね今までこうおすめの商品を 教えてくださいっていう風に言っていたの でデートっていうものが1月10日であっ たりとかよくわかんない日付になっていた のではないかっていうことを推測してでは 昨日のおすすめ商品を教えてくださいって いうものを聞いてみますてそうするとどう なるかと言うとえ昨日のおすめ商品を教え てくださいって言ってるのにレコメンド アイテムに渡されてるのは2021の10 月20日っていうものが渡されれてること が分かりますとチャットGPTにこう昨日 とか今日とか昨日とか渡したら日付を ちゃんと入れてくれるのかなと期待するっ ていうことがあると思うんですけれども 実際はそんな入れてくれないっていうこと がまこの結果から分かるということですね もう少し例えばディスクリプションとかを 適切にすればうまくいくのかもしれないん ですけど今回は違う手段でえっと解決 しようと思いますで今回どういう風に解決 するかと言うとデートタイムを使って 新しい日付けを取得するような関数自社 ツールってものを設けようと思います ゲットデートっていうツールっていうもの を作ってでそのツールは何をするかという とま今日や昨日など日付を取得する時に 使います引き数には今日を起点として何日 か前をNで与えますとでここのそのドック ストリングスにですねこう引数に関する 情報を書いてるんですけども実際はですね パダンティックで引き数の説明した方が ベタだとは思いますただえっとドック スリンに書いてツールの説明として与える だけでも結構チャットGPTは賢いので うまく判断してくれるということですでお 見せしますはいこの日付関数を入れると どうなるかと言うとこういう風な感じです ねはい今日のおすめの商品を教えてくださ いっていう風にするとtodayて書い てき来てこれもうまく動いてないですね こういうことが結構ありますとうまく動か ないことがあるとでもう1回実行してみ ましょうかはいでもならではですね ちょっと30分で終える予定が46分に なってしまったんですがもう少しで終わる ので続けます前回のやつだとですね直接 レコメンドアイテムを呼び出してデートに 対してtoデイていうものを渡してしまっ てたんですけれども今回はうまく動いてい てまずは日付を取得するためにゲットデイ トっていうものを呼び出しますとで今日な ので今日からの起点のえN日前だったらN に0を渡せばいいということが分かるとだ からゲットデートに0を渡して帰ってきた ものが2023年の9月30日っていう ものが帰ってきてで9月30日のえお勧め 商品は何ですがま書いてくるものは全部 AppleなのでAppleってものが 帰ってきてることが分かると思います同じ ようにですねじゃあどういう ファンクションが渡されてたかっていうの をお見せするとサクっとですねで今回あの ファンクション2つ渡されていて レコメンドアイテムっていう ファンクションとゲットデイトっていう ファンクションが渡されていますで それぞれのファンクションの説明がこの ように与えられているということが分かり ますもう少しツールを複雑にしていって ですね奇数日だったらオレンジ偶数日だっ たらえAppleっていうものを返すよう なツールにちょっとだけレコメンド アイテムを変えみようと思い ますはいではえこれを実行するとですね 今日はえ9月30日なのでもちろん9月 30日偶数日ですね偶数日なので Appleが返ってくるきちんと帰ってき ますねAppleが帰ってきましたでは 昨日の場合はどうなのか 昨日はい昨日の場合は29日なので1日前 ってものをゲットデイトで取得して29日 ってものが書いていますで29日を使って え昨日のおすめ商品オレンジっていうもの を取ることができるというところ ですやとうまく動きました ねでもう少しだけちょっと時間が1時間 ぐらい喋ってしまうような感になるんです けれどもう少しだけお話させてくださいで これだけだともちろんまだまだ連携が 少ないかなっていうところがあってで次に じゃあAppleだったら100円 オレンジだったら200円っていうような ものを商品の値段を返すような関数って いうものを組み込んでみましょうでツール としては元々グビだったらAppleキビ だったらオレンジを返すようなものとあと は日付を取得するツールであとは今回定義 したAppleとオレンジAppleだっ たら100円オレンジだったら200円 っていうのを返すツールっていうものを 定義しますでこれでエージェントのですね おすめの商品の値段まで教えてくださいっ ていうことをやるとそうするとまず最初に え昨日なのでゲットデートでN=1を渡し て1日前のデートを取ってきますで9月 29日ですで次にレコメンドアイテムって いうツールを呼び出してレコメンド アイテムにえ9月29日を渡してで最後に ですねアイテムプライスっていうので アイテムネームをえオレンジにとして渡し てでオレンジはは200円なのでえ昨日の おすめ商品であるオレンジの値段は 200円ですっていう最終的な答えが返っ てきましたというところになりますだいぶ 柔軟なものができるようになってきたって いうことが理解できると思いますで他には ですね最後なんですけれども取得した結果 を吐き出したいとかデータベースに 書き込みたいとかあったりすると思うん ですけどもそういうことももちろん簡単に できますと基本的にはファンクションが あってそのファンクション中身は自分で 書いていくので何でもすることができると いうところですねちょっとあの output.tchっていうところに 書き込むようなものを記事的にですね データベースに書き込んでいるようなもの として紹介しようと思いますで今回は さらに追加するツールとしてはですね セーブデータっていうツールを追加します とセーブデータっていうものは何なのかと いうとここにも説明書いてあるんです けれども日付と商品名と商品の値段を 受け取ってデータベースに書き込みますっ ていうようなものをツールとして作りまし たデータベースといながらもアウトプット テキストにただ書き込むだけのものになる んですけどもデートアイテムネーム アイテムプライスを書き込むだけのツール となりますま実際に自社ツールを使う時に はここにSQL分であったりとかを書いて データベスに書き込むとかをやったりする ことになると思いますこちらもですね ちょっと簡易的にですねフダンティックを 使わずにえドックストリングだけで何を やるかってのを渡して書き込めるかどう かってのをお見せしていこうと思いますで 同様にですね同じように何をしたいかと 言うと今日のおすめ商品の値段を調べて データベースに書き込んでくださいって いうことを実行し ますで最初に動くものとしてはまず日付を 取得するゲットデイトが動いてN=0で 今日の日付を取ります9月30日ですと9 月30日が取れたら9月30日の レコメンドアイテムは何ですかていうので レコメンドアイテムのツールを起動して 書いてきたものがAppleですとであ次 にやることしてはAppleの値段は いくらですかていうものをアイテム プライスに聞いてアイテムプライスが返し てきた答えが100円ですとで100円 っっていうものをえセーブデータに渡して セーブデータに渡すものとしてはデートと アイテムネームとアイテムプライスこれは 日付と商品目と商品の値段を受け取 りっていう風に書いてあるのでこういう ようなものを基数として与えて データベースに書き込みますていうことを この操作でやるということですねなので 細かいことを書いた方がミスは少ないん ですけれどもあんまり書かなくても動いて くれるっていうのがまやっぱりこう チャットGPTのうまいところかなという 風には思いますで最終的に帰ってきたこと としては今日のえおすすめ商品の値段は 100円ですとでデータベースに正常に 書き込まれましたっていうので データベースではないんですけどれも テキストファイルっていうものを中身見て みましょうと今日の日付2023年9月 30日でおすすめの商品Appleで最終 的に100円っていうのが書き込まれた ことが確認できましたとはいとても長く なってしまったんですけれどもこちらで テモは終わりとさせていただきたいと思い ますはい私の発表は以上となります ありがとうございました', metadata={'source': 'I2UZutRrtEE'})]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be6780-6bf0-49fe-97f0-508d0ed54f17",
   "metadata": {},
   "source": [
    "## Summarize Chain\n",
    "Document Loaderで取得したDocumentをSummarize Chainを使って要約する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ba9ec2d6-b307-4b6b-b50c-e075e90f44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model=\"gpt-3.5-turbo-0613\")\n",
    "summarize_chain = load_summarize_chain(llm=llm, chain_type=\"refine\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3785ef37-94ef-4b45-9c64-4f52fc1b7006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"では早速なんですけれども始めさせて いただきます私からはラングチェイン エージェントを使って自社ツールと チャットGPTを連携するというタイトル で発表させていただきますChatGPT 自体は皆さんご存知のように非常に優れた 能力を持っていますが最新のデータに アクセスできないとか特定の数学的な推論 が得意でないなどいくつかの課題が存在し ますこれらの課題は自社プロダクトに チャートGPTを導入するなどチャット GPTを社会実装しようとした際に解決し なければなりません今回お伝えするラグ チェインエイジェントを導入すると ChatGPTと外部ツールをうまく連携 させることができるようになりChat GPTが抱える課題を克服しチットGPT の可能性をさらに広げることができます 本日はラングチェインエージェントをどう 活用することでチャットGPTの能力を 拡張できるのか具体的にご紹介していき ますそれでは始めていきますこちらが私の プロフィールとなります内容はいつもと 同じなのでスキップさせていただきます はいでは具体的な話に入る前にですね今回 のテーマの結論に先に触れておきます結論 なんですけれどもラグチェインには様々な エージェントの種類が存在するが自社 ツールとチャットGPTを効果的に連結 する際汎用的なエージェントタイプである ゼロショットリアクトまたはオAI ファンクションを利用することが簡単と なりますゼロショットリアクトとOpen AIファンクションを比較するとOpen AIファンクションの方がチットGPT からより適切かつ精度の高い答えを 引き出しやすくなっていますこちらはデモ のでお見せしますねでそしてえっとここで ChatGPTと自社ツール連携で非常に 重要となるものとしてツールの説明という ものがありますと通常のプロンプトチャト GPTを使う時のプロンプトと同様に ツールの特性や機能をChatGPTに 正確かつ効果的に伝えることで自社ツール とチャットGPTの連携の成功率が格段に 向上していきますこのポイントに着目し ながら具体的な方法論について今日は 深掘りしていきたいと思いますそれでは これらのポイントを年頭にきつつ今回の 発表をお聞きください本日の発表は以下の 3つのセクションから構成されています 初めにチットGPTの基本的な概要につい て触れます次にラングチェイン エイジェントの特徴について紹介し最後に デモンストレーションを行いますデモでは 特にツールの説明について着目して いただけると幸いですそれでは最初の トピックに移りましょうここではチット GPTの全体的な特徴を振り返っていき ますすでにご存知の内容も多いと思うん ですけれども一般的なはチャットGPTの 性質っていうものをご紹介していきます チットGPTは実用的な大規模言語モデル llmで現在多くのビジネスに活用されて きています例えばベネッセ ホールディングスでは車内AIChat ベネッセGPTを開発しグループ社員 1.5万人に向けて提供しているという ようなプレスを出していたりとか立メカ 大学では大学の英語事業において機械翻訳 とChatGPTを組み合わせたサービス が試験導入されているであったりとか他に も色々なサービスがチャトGPTを 組み込み始めておりChatGPTの実用 化っていうものがすごい勢いで広がってい ますChatGPTを活用する際によく 言われることとしてはチャットGPTから 良い結果回答を得るためには良い質問が 必要であるということですでこれは プロンプトエンジニアリングと呼ばれる 領域で今最も伸びている業種の1つと言え ますプロンプトエンジニアリングではどの ような項目をプロンプトに入れるべきか またプロンプトを複数回うまく使いこ なくして良い回答をるテクニックなどが 研究開発されていますリアクトであったり とかいろんなことを聞いたことあると思う んですけれどもプロンプトの種類っていう のがどんどんどんど開発されているという ことですでチャットGPTの活用は多に 渡っておりその可能性は広がりつつあり ますでプロンプトエンジニアリングの テクニックを活用することでチャット GPTとの適切なコミュニケーションが 可能になりますプロンプト エンジニアリングでは色々なテクニックが 存在するのですが今回はプロンプト エンジニアリング自体がメインのテーマで はないのでプロンプトエジライニングの中 で抑えるべき基本的な項目とそしてよく 使われるまフーショットプロンティングと 呼ばれるような手法を簡単にだけ紹介して いきますはいまず基礎として抑えていき たいこととしてプロンプトに何を与える べきかっていうものをいくつか紹介して いきますで1点目としては具体的で明確に 質問することですで例えばその日本の歴史 において重要な出来事は何ですかという 質問よりも日本の戦国時代における重要な 出来事について教えてくださいという具体 的に質問し方が的確なコ答が得られますと で2点目に状況やコンテクストを明示する ことですて背景情報があるとチャット GPTはコンテクストをより理解し適切な 回答が返ってきますとで3点目期待する 回答の形式を指定するとこれにより望む 情報が整理されて帰ってきますリスト形式 で返してくださいであったりとか ジェイソン形式で返してくださいであっ たりとかまそういうことを質問する時に プロンプトの中に入れるということですね で4点目範囲を限定すること範囲が広す すぎると回答も後半で一般的になりがちに なりますなので範囲を狭めるということが 重要となってきますで最後5点目自性や 認証を明確にすることですと例えば子供で も分かるようにしてくださいとか中学生で も分かるようにしてくださいとかなんか そういう風にどの視点で書て思ってみるか を示したりであったりとか何年の情報が 欲しいとかそういうことを指定することに よって回答が明確なものが帰ってきやすく なるということですで次にプロンティ テクニックの1つののフューショット プロンティについて簡単に説明します フューショットプロンティはモデルに少数 の例を示すことで特定のタスクの解決方法 を学習させるアプローチになりますこれに よりモデルは様々な問に対して適切な回答 を生成することができますここに例として 上げてるんですけれども例えばレストラン 最高でしたこれは感情ポジティブですこの 映画は本当に時間の無駄だった感情 ネガティブですというようにいくつかの例 を与えて出したいものでそれぞれの文に 対しての感情っていうものをこのように 並べていきますてで最後に聞きたいものと してそのサービスは非常に遅く不満ですっ ていうものがあった時にどんな感情で すかっていうことを聞くと感情ネガティブ という風に返ってくるというところですで こういう風にすることによってモデルって いうものが新しい文章がポジティブか ネガティブかっていうのをま過去の例を元 に判断することができるようになるという ことですでこの方法によってですねフュー ショットプロンティはモデルに多様な タスクを効率的に学習させることができ 柔軟な対応が可能になってくるということ でですでここまでいくつかのプロンティの テクニックであったりとかをお話ししてき ましたもちろん他にもたくさんあってです ねチェインオブソツであったりとかあと セルフアスクであったりとかいろんなもの があるんですけれども今回のメインテーマ ではないので一旦プロンティグの話はここ で終わりにしてでここでちょっと一息つい てですねこれらのテクニックが持つ限界に ついて考えてみましょうでまず1点目なん ですけれどもプロンプトのテクニックは 日々進化し開発されていますとこれによっ てチットGPTは多様なタスクや質問に 対応でできるようになってきていますただ 全ての作業やタスクをチャットGPTに 依存すること自体が現実的ではありません とえ例えば先ほどの例で出てきた文章から の感情抽出に関してはチャートGPTに 無理やりさせることが適切かと言われると そうでもないかもしれないAWSコプリン のような勘定分析に特化した学習をして いるAPIっていうものがあってそれを 使った方がいい結果が返ってくるかもしれ ませんっていうのがまず1つ目ですねで次 に2点目としてそのChatGPTが持つ 情報っていうものは訓練データに限定され ていてチットGPTはそれ以外のデータに ついては知らないというところですなので えっとチャットGPTは過去のデータで 学習されておりその学習データ以降に出て きた情報に関してはチットGPTに尋ね られませんとよく言われるものとしては 2021年のデータですかねまでのデータ で学習してるので例えば2022年であっ たりとか2023年のデータについて聞い たとしてもチャットGPTがうまく答え られないというようなところですね最新の 価格的な研究であったりとかニュース記事 とか特定の地域の天気情報であったりとか チットGPTの訓練データに含まれてい ないような情報っていうものはChat GPTだけでは取得できませんし インターネットにあるデータだけではなく て例えば今回のテーマである自社のツール と連携するっていう時のためには自社の データを使いたいみたいな話とかあると 思うんですけれども自社データについては チャットGPTが知るわけもないという ことでえっとChatGPTに自社の データについて聞いたとしてもChat GPTはもちろん答えることができません とこれらのチャGPTの限界を理解する ことでチットGPTの適切な使用方法で あったりとかそれらをどのようなシナリオ で活用するべきかっていうのが見えてくる ということですねでChatgpdの概要 の最後なんですけれどもChatgpdや その他大記号言語モデルllm自体の課題 をこちらにまとめますでまず1点目なん ですけれども先ほどと被るんですが ChatGPTは最新の情報にアクセスが できないと例えば為替や株価のように国 一刻と変化していく情報に対してチト GPTははリアルタイムで対応することが できませんで2つ目の課題としては ChatGPTが独自の情報源にアクセス する能力を持たないということです企業の データベースに保存されている顧客名簿で あったりとか特定のプライベートな情報に はアクセスすることができませんで3つ目 に水論の正確性に課題があるというところ ですチャットGPTは時折り簡単な算術 計算でさえも間違えることがありますこれ は特に正確な情報や結果が求められる タスクにおいて大きな問題となりますねで そして最後にチャットgpdができない こととかのデータが足りないんだったら ファインチューニングすればいいんじゃ ないかっていうような意見もあると思うん ですけれどもファインチューニングする ことによって汎用性が劣化してしまうと いうことがありますとでChatGPTを 特定のタスク例えば自社データを食わせる であったりとか特定のタスクに特化した データを食わせてファインチューニング すると他の多様なタスクに対する性能が 低下すると言われていますでこれらの課題 を解決するための1つの手段として今回お 話しするラングチェインエージェントって いうものがありますでここここからは ラングチェインエイジェントを使って チットGPTの性能を生かしつつ苦手な ものは他のツールで解決していくっていう ことについてお話ししていきますで参考 までにですねここにバッと上げている課題 に関してはMrKLシステムズていう有名 な論文があってそ論文から引っ張ってきた ものとなりますMrKLシステムズって いうのはミラクルシステムズと呼ぶもので チャットGPTと他のツールを連携して どんどんどんどん課題を解いていくって いうことを提示しているロムとなってい ますそれではラングチェインエージェント についてのトピックに移っていきますまず ラングチェインエージェントの前にですね ラングチェインとは何かと言うとラング チェインとは言語モデルを利用する アプリケーションのためのフレームワーク ですでラングチェインについては前回の 勉強会でもご紹介しているので基礎的な 内容を知りたい方はYouTubeで前回 の勉強会の動画をご覧くださいでラング チェインエージェントはラングチェインの 中の1つの機能なんですけれどもラング チェインの中でも非常に面白くて重要な 役割を果していますランチェの主なタスク は次にどんなアクションを取るべきかを大 記号言語モデルllmに決定させそれを 実行するということですで具体的な プロセスは以下の通りですとまず各ツール ができることと質問を指定したレスポンス のフォーマットでllmに送信します聞き ますとチットGPTなどのllmに聞き ますそしてllmChatGPTから帰っ てきたレスポンスをパースして次の アクションを決めるということをしますで このプロセスによってラングチェインジと はユーザーの4級に応じて効率的かつ正確 なアクションを実行できるようになって いるというところですでここでラング チェインエージェントがどのように動く かっていうもののイメージを具体的な例と して1つ持ってきましたま正確に正しい 動き方をしてるわけではないんですけれど も大体こういうような動きをするんだよっ ていうこともご理解ください例えばチット GPTに対してコミュニケーションを取る のがラングチェインのエージェントとなる んですけれどもラングチェイン エージェントにですねまずこう今日東京で で出かける時時に適切な服装は何ですかて いうのを聞きますでそうするとラング チェインエージェントが聞きたい内容今日 東京で出かける時に適切な服装っていう ものとあと自分が持っているツールって いうものをプロンプトに入れてチャット GPTに聞いてくれますとで自分が持っ てるツールっていうものは例えば Google検索のツールっていうも持っ てますとGoogle検索のツールでは 最新の情報を引っ張ってくることができ ますと私が持ってるのはGoogle検索 のツールで最新の情報が引っ張ってこれ ますそしてで今回聞質問としては東京で 出かける時に適切な服装って何でかていう ものを聞きますとでその時にチャット GPTはどのように返してくるかというと でまずは検索ツールでインプットは東京の 気温っっていうもので調べなさいっていう ものをエージェントの方に返してきますで エージェントはチャットGPTからから 教えてもらったじゃあえ検索ツールを使え ばいいのかっていうところで検索ツールを 使おうとしますでインプットとしては チャトGPTが教えてくれたように東京の 気温っていうものをインプットとして入れ てで検索ツールから帰ってきた22度って いうものを変してもらいますとでそして またえっとチャットGPTにえこのツール に入れた答えってものが22°でした次何 すればいいんですかであったりとか最終的 にどうなんですすかっていうのをチャット GPTに聞いてチャットGPTが十分な 情報を得たっていう風に判断するとでは 22°なので薄の長袖でがいいでしょうっ ていうものをエージェントとしに返してで あとはえ答えとしてラングチェインの エージェントが東京は22度なので素の 長袖でがいいでしょうっていうものをその まま返してくるというような流れになり ます特徴的なものとしてはチャットGPT だと今日の天気であったりとか東京の天気 であったりていうものは取れないんですね でそこをGoogle検索のツールって いうものを利用することによって新しい データっていうのを取れるようにしてると いうところですねもう1つ押えておきたい こととしてはこの問を解く時に自分が持っ ているツールっていうもの一覧を渡して どのツールを使えばいいかっていうのこと 自体もチャットGPTに判断してもらうっ ていうところが面白いところとなってい ますはいでこれがラングチェイン エージェントリアクトフレームワークとか 呼ばれたりするようなやり方ですねはい これらのこと全部ラングチェインの エージェントがやってくれるというところ ですでラングチェインエージェントって いうものは様々なタイプのエージェントが 用意されていますとここでエージェントの タイプっていうものを紹介するんです けれども解きたい課題に応じて エージェントを使い分けることによって 精度の高い回答を得られるようになると いうことですここで6個ほど上げています でまず1番汎用的なものとしてはゼロ ショットリアクトエージェントというもの ですでこのエージェントはリアクト フレームワークを利用してツールの説明に 基づいてどのツールを使用すべきか判断し ます先ほどの例で出てきたものと同じです ねこれによって特定のトレーニングデータ なしで多様なツールの適切なこう利用が 可能になってきます次にストラクチャード インプットリアクタエージェントですで このエージェントは複数の入力を使う エージェントを活用することができますで これによってより複雑なツールの理由用が 簡単になりタスクの効率が向上しますと 続いてOpenAIファンクション エージェントですこのエージェントはオ AIのファンクションコーリングの機能を 使っており関数が呼び出されるタイミング であったりとかを検出したりとか適切な 入力を関数に渡すような微調整がえオープ AI自体チットGPT自体がされていて それを使うことができますとOpenAI のえファンクションコーリングこの エージェントが使うOpenAIの ファンクションコーリングについてはこの 後のスライドでもう少し詳しく説明します で次にコンバーションエージェントですで このエージェントは会話での使用目的とし て設計されています何かを聞いて何か答え を出すっていうようなま単純なチャット GPTで何かこう答えを作ってくださいと かそういうのとは違くて会話的な インタラクションを通じて効果的な コミュニケーションを可能にするような エージェントとなっていますそして次に あげるものとしてはセルフアクwith sarchエージェントですねでそこの エージェントはですねセルフアクという 質問を自問自としながら分割することに よって正しい解を導き出す方法を使って 情報を検索するエージェントですセルフア クっていうプロンティグの技術があるん ですねまそれを使ってちゃんとした正しい 答えを導き出すエージェントとなってい ますで最後に紹介するものがリアクト ドキュメントストアエージェントですで このエージェントはドックストア ドキュメントストアですねと対話するため にリアクトフレームワークを使用してき ますえこれによって文書の管理であったり とか検索が一元化されて効率的に情報 アクセスが可能となりますこちらに関して はまた今後の勉強会でも取り扱っていき たいかなという風に思ってるんですけれど も文書検索管理っていうものに特化した エージェントとなっていますで以上がラン チェインエージェントの各エージェントの 使用となりますでそれぞれのタイプが 異なるシナリオやタスクに適しており ラングチェインの柔軟性と多様性を象徴し ているものとなります今回のセッションで はですね最も汎用的なゼロショット リアクトとチャットGPT自体に色々判断 させるファンクションコーリングを使った オープAIファンクション2を利用して いき ますでは先ほど説明を省いてしまったん ですけれどもOpenAIファンクション コーリンググっていうのはOpenAI ファンクションコーリングじゃないですね OpenAIファンクションですね OpenAIファンクションとは何か エージェントタイプの1つであるOpen AIファンクションとは何かについてお 話ししますこちらはChatGPTを提供 しているOpenAIのファンクション コーリングという機能を利用していますで ファンクションコーリングっていうものは 2023年の6月にリリースされた機能で ラングチェインエージェントが担っていっ たツールを渡してチャットGPTに聞いて 次のステップを決定するやり取りってい ものをチットGPT自体が行えるように ファインチューニングされているものと なりますでこの機能では東京の天気を教え てくれというような先ほどと同じように ですね質問と共にどういう関数を持って いるのかその関数の説明をチトGPTに 直接投げるということをやりますなので 先ほどのラングチェインエイジェントが やってることと同じようなことをやれる ようになっていてチットGPTが ファンクションをどのように受け取るのか そのファンクションのパラメータであっ たりとか説明とかをよりチャトGPTが 理解しやすいようにAPIとして受け取る ことができるようになっているのでより 適切な判断をチャットGPTがしやすく なっているというところがメリットとなり ますなので今メリットをお話ししてしまっ たんですけれどもファンクション コーリングを使うメリットとしては ChatGPTにツールの挙動をより正確 に伝えやすくなっているというところで これによってChatGPTが機能の ツールのえファンクションの使用や動作を より正確に理解しやすくなっていて適切な 判断をしやすくなっているとていうのとで もう1つのメリットとしてはチャット GPTっていうものがラングチェイン エージェンドって元々チャットGPTが 返してきた個体を利用者が頑張って定期 表現を使って解析する必要があったんです がそれをする必要がなくなったっていう ところが利点としてありますとでチャット GPTが実際にラングチェインを使って どのツールを使えばいいですかていうのを 聞くんですけどそれをどのようにパースし てるかというとこのように頑張ってですね えっとレギュラーエクスプレッションを 使って正規表現を使ってパースしたりとか して次のアクションを決めてたりするって いうのがラングチェが今までやっていた ことであるとでこういうパースをしなきゃ いけないっていうことをしなくて済む つまりパースしなきゃいけなかったので今 まではそのパースすることでえっとうまく パースできなくてミスってしまうっていう こともあったんですけどそういうことが なくなるっていうことが期待されるという ところです はいはいちょっと駆け足になってしまった んですがここまでで20分ぐらいかかって しまいましたねえここからがレモに入り ますで実際にですねラングチェイン エージェントを使ってチャトGPTと自社 ツールを連結するような感じのえデモって いうものをお見せしていきますで使う エージェントとしてはゼロショット リアクトとオーエファンクションを使って いきますであとは今回のプレゼンの冒頭の 方でもお話ししたようにChatGPTに ツールの説明をどのように渡せばいいのか であったりとかその辺りを持ち替えて いただければ幸いです はいではではやっていきますちょっと時間 がなくなってきたので駆け足でいきますね でまずは設定ファイル読んでいきますこれ 何をやってるかと言うとえOpenAIの APIキーであったりとかを読んでいます でまずはですねえっとゼロショットの方を 見せしていきたいんですがおマジないとし てですねおじないというかま前回の勉強会 と同じようにですねラングチェイン エージェントを使うためのやり方をお見せ していきますで実際今回使うモデルとして はチットOpenAPAIというか ChatGPTのえGPT3.5のtbo 0613というものを使っていきますで 外部のツールとしてはGoogleサーチ のAPIっていうものを今回用意しました ここであのサプAPIっていうものを書い ているものがえGoogleのAPIと なりますで聞きたい質問としては今日の 東京と北海道の天気を調べて東京から 北海道の出張で来ていくべきを教えて くださいっていうものを聞きたいと思い ますでラングチェインエージェント自体を 走らせてみますでこれがゼロショット リアクトエージェントを使った場合どの ようになるかっていうのをお見せしますで エージェントタイプに関しては イニシャライズエージェントの エージェントっていうえ引数に対して エージェントタイプのこのゼロショット リアクトディスクリプションていうものを 渡すとエージェントを選択することができ ますちなみになんですけれどもえこれって ただあの文字列が返ってくるだけですかね あのゼロショットリアクト ディスクリプションていうものが返って くるだけのものになりますなので文字列で 与えても同じですはいでは実行してみ ましょうチトgpdはもちろん今日の天気 とか今日の東京とか北海道の天気っていう のは知らないので来ていくべき服装って いうことは分からないわけですね気温も 分からないのででこれでラングチェイン エージェントが私はGoogle検索の ツールを持ってますよでそれを使って使う ことができますでこの問題課題っていう ものを解くためにはどうすればいいですか てのを聞くっていうことですねでここで ざーっと出てきてるんですけれども実際に 書いてきた答えとしてはtheWisto isCloudyaboutcrear withtempof歌詞になっちゃって ますけどまこのように帰ってきますと 英語で帰ってきちゃってますねでこういう 風にえどんなアドレスをれ切ればいい かっていうものがちゃんと帰ってきてい ますと前にやった時はそんなに綺麗に帰っ てきてなかったんですけれども今回は うまく帰ってきていますはいこちらがゼロ ショットリアクトディスクリプションで やった場合の結果になりますでどういう ことをやってるかっていうのなこの辺りを 見ていただくといいのかなという風には 思いますでチットGPTにまずラング チェインエージェントが問題とツールを 渡してチットGPTに何をすればいい かっていうのを聞きますでチャットGPT から帰ってきた答えとしては次の アクションとしてはサーチをしてください ですサーチのインプットとしてはウザー 東京todayていうものを渡してくださ いっていうのを渡してますで次の アクションとしてはサーチをしてください でそのインプットとしてはウザ北海道 todayていうものを渡すようにして くださいとで最終的にその帰ってきた答 っていうものをチャットGPTに渡して 最終的な文が出てくるということですはい でこれが1つ目ですねアエージェント タイプ1つ目のえゼロショットリアクト ディスクリプションで元々ラングチェイン エイジェントがリアクトフレームワークで ツールを渡したりとかしてチャットGPT とうまく連携しながら外部ツールを使う ようにしたえ機能となっていますでこれが ですねえ最近出てきた6月にリリースされ たOpenAIのファンクションコーリン グっていうものを使うとどうなるかって いうのをお見せします はいでOpenAIファンクション OpenAIのファンクションコーリング を使うためにはエージェントタイプの OpenAIファンクションズっていう ものを渡しますでそうするとえこのように 動きますで実際にやってることとしては ラングチェインエージェントがOpen AIのファンクションコーリングっていう ものを使ってうまくチャットGPTと やり取りしながら答えを導き出していくっ ていうことをやっていますはい出てきた 答えはこちらですねで同じようにですね どのようにやっているかっていうものは ここら辺の途中で出てきているんです けれども途中結果でえサーチとしてはえ クエリー東京の今日の天気っていうものを 調べてくださいでサーチっていうツルを 使って今日の北海道今日の天気っていう ものを調べてくださいでこれらが帰ってき た答えっていうものをえチャットGPTに 返してチャトGPTがその与えられた情報 を元に東京の今日の天気は曇りですが後に は晴れる予報ですで気温は接し30°歌 86°でコス確率はどうのこうのっていう ものがあってであとは東京は暑くなる可能 性があるので軽い服装が適しています ただし北海道は比較的涼しいので ジャケットなどを持っていくことがお勧め しますというような感じに帰ってきてます ま見て分かると思うんですけれどもま日本 語にな質問が日本語だったので日本語で ちゃんと返してくれるっていうのは もちろんのことなんですけれども書いてき た答えっていうものに関してより詳しく 返してくれているっていうことが分かると 思いますなので使ってみた所管としては ですねここで言うオAIファンクションの ファンクションコーリングっていうのの ファンクションっていうのはラング チェインエージェントでいうツールと同じ なんですけれどもそのファンクションで あったりとかツールを渡す時にですね渡し て何かを判断判させたいチャットGPTに 判断してもらいたいっていう時はえ エージェントタイプゼロショットリアクト ディスクリプションよりもエージェント タイプオAIファンクションの方がより いい結果が返ってきやすいっていうことが 分かると思い ますもう少しあの詳しく見ていきます ちなみにですねOpenAIにどんな ファンクションを渡してるかっていうのは イニシャライズエージェントのところで エージェントを定義してるんですけども そのエージェントの中からエージェントっ ていうものとファンクションズってものを 実行するとどんなえファンクションが渡さ れてかっていうのを見ることができますで 今回渡されたファンクションっていうのは ネームサーチっていうファンクションが 渡されていてどんなことができるかって いうのでサチエンジンですっていうような ものをやってますであとはパラメーターと してはこういうものを渡していきますって いうのが分かりますとはいでここまでが 基本的なラングチェインエージェントを 使って外部ツールを連携するっていうお話 をしてきましたとで次にですねえ自社で 作ったツールっていうものを連携させ るっていところについてお見せしていき ますで一旦はですねレコメンドアイテムっ ていうものを考えてみようと思いますで ここ何をやりたいかって言うとレコメンド アイテムまこの中身はほとんど意味のない ものになってるんですけれどもレコメンド アイテムっていう関数を自社で持っていて でレコメンドアイテムっていうものは自社 のデータベースなどにアクセスして日付を 持ったにこの日のレコメンドっていうもの はこういうものですっていうものを返す ような関数っていうところになりますとな のでえっとこのレコメンドアイテムって いうところをこの関数ををえChat GPTのフローの中に組み込みたいって いうことをやっていきますでどのように ラングチェインエージェントを使って ChatGPTに連携すればいいかという とツールっていうものにえ変換していく 必要がありますとでレコメンドアイテム ツールっていうものをえツールfrom ファンクションズっていうものを使って どのようなファンクションを実行したい ですかレコメンドアイテムっていう ファンクションを実行したいですまこう いう風にですねこの今回のあまり意味の ないファンクションなんですけれどもこれ を実行しますとでツールの名前としてはえ レコメンドアイテムっていうものを渡し ますでツールのディスクリプション何を やってくれるかっていうことに関してはお すめの商品を教えてくれますっていうのを ツールとして定義するというところです はいでこれをちょっとツールズっていうま ツールまとめて渡さないといけないで ツールの配列の中に入れておきます実際に ですねラングチェインエージェントにどの ようにツールを渡せばいいかっていうとま 先ほどもカラっとやってたんですけれども このツール1覧っていうのを最初に渡して 引数として渡しているのでまこのようにし てどんなツールが使えるかっていうものを ラングチェインのAに渡しでチャット GPTと連携していきますはいでえっと 今回聞きたいこととしてはえおすすめの 商品を教えてくださいっていうものを投げ てみようと思いますで使うエージェントと しては先ほどゼロシトリアクトよりもより いい結果が返ってきたオーエ ファンクションズってものを選びます はいはいでは実行し ますはいで書いてきたものとしてはですね コメドアイテムってものが実行されて withファッションという謎のものをえ 入力に入れてで実際この引数がですね デートの中にファッションが入ってで プリントファッションっていうところでこ でファッションが出てきてしまってるとで ただおすめの商品は今回レコメンド アイテムで書いてきたAppleですって いうのが書いてきてることが分かると思い ますはい見て分かるようにですねなんか よくわからない引数は与えてしまってる なっていうところがありますでこを どんどん解決していこうと思いますで次に やることとしては今回なんでこのように変 なファッションとかいう引数を与えて しまったかというと引数に関する情報が ないんですね与えられてないというところ になりますとなので引数何与えればいいん ですかみたいなところが情報としてないが ゆえにチャットGPTもレコメンド アイテムを使うことは分かるとレコメンド アイテムを使えばいいのは分かるんだけど レコメンドアイテムに対してちょっと 分からないからカテゴリー情報でも渡して みようかなっていうのでファッションを 渡してしまっているというような状態に なりますで例えばまファンクションを見た としても今回使ってるファンクションに 関してはレコメンドアイテムでおすめの 商品を教えてくれますでパラメーターの 情報は全くありませんっていう状態なので どんなパラメーターを渡せばいいかって いうのはオープAIファンクションは 分からないというところですねじゃあこれ に対してどうすればいいかっていうのをお 見せしますでえっと結構簡単でしてパダ ティックっていうものをご存知ですかね こう最近えっとファストAPIとかですか ねこう結構パであったりとかそのヒントを 与えるっていうことがまPython主流 になってきてると思うんですけれども フダンティックを使っていきますでえっと ご注意していただきたいこととしては ファインディッてもうV2が出てるんです けれどもV2を使うとラングチェイン エージェントを動きませんとなので パティックV1を指定して方に関する情報 をえ与えるためのそのベイスモデルとか フィールドを使ってえやっていきますで これ何をやってるかと言うとま今回の インプットに対してデイトっていうものが ありますがデイトていうものはお勧めした 商品を絞るために使われる日付ですって いうものを与えていますこれをえっと アーグススキーマ先ほどのツールを定義 する時にアグススキーマっていう引きを 使ってこのレコメンドアイテムインットっ ていうベースモデルから継承した レコメンドアイテムインプットっていう インプットの報引数の情報を持ったクラ スっていうものを渡すということをします とでこうするとどうなるかっていうのをお 見せしていき ますはいじゃあどう変わる かいきますはい実行します全く同じですね 実行するものとしてはで今回はレコメンド アイテムっていうものに対して先ほどと 同じレコメンドアイテムっていうものが 選ばれましたとコアイテムに対してデイト が2020年の12月1日っていうものを 渡しているっていうことが分かりますで それによってですね気付がちゃんと渡して くれたっていうことがわか分かるわけです ねで今回あのここであのデートっていう ものはディスクリプションでお勧めしたい 商品を絞るために使われる日付っていう ことを伝えたのでチャットGPTの方があ これは日付のことなんだっていうところで 引数はファッションのようなカテゴリーを 渡すんではなくてデートの引数っていう ものを渡すことができてるというところ ですはいでこちらがデトのまあ2022年 の12月1日もはや日付が意味が分から ないですけれどもこういうものが渡されて いるというところですねでおすめ商品は Appleですという風にでこういう風に ですねこうツールの説明をどんどん どんどん入れていってええ解決していく 必要があるというところですねで同じよう にですねちょっとエージェントの ファンクションがどのように渡されるてる かっていうのをお見せしますとで今回渡さ れたのはレコメンドアイテムインプトって いうのをえパイダンテックの方で定義して 渡しているのでえ実際にえっと ファンクションOpenAIのえ ファンクションコーリングにどのように 渡されてるかというとパラメーターとして ですねこういう風なレコメンドアイテム インプットっていうものが渡されていて プロパティとしてはデートがあってデー トっていうのはお勧めしたい商品を絞る ために使われる日付っていうものが入れ られてえ飛ばされてるということが分かる と思いますはい で他にはですねこのようにツールフロン ファンクションで作るっていうこともある んですけれども同じようにですね デコレーターを使うっていうこともでき ますとデコレーターの方が結構シンプルに 書けるので私は結構デコレーターを使って 書くことが多いですとでデコレーター何を すればいいかというとデコターっていうの はま関数の前にですねアマークツールなん ちゃらかんちゃらっていうものがあってで それを使ってえっとこのツールってのは何 ができるかっていうものをえドッ ストリングで与えるんですねな先ほどと 同じにするとレコメンドアイテムツールっ ていうものがあったら中身は同じですね プリントデイトとAppっていうものを 返しますとプリントデイトしてApple っていうものを返しますとででやることと してはおすめ商品を教えてくれます全く 同じ内容ですねでこのツールの名前って いうものはデコレーターの引数として レコメンドアイテムとして渡していると いうところですで同じようにですね ツールズの一覧として入れてでこれをえ 実行してみ ましょうはいこれデコレーター使った場合 です ね実行するものは全部同じですで同じよう にこれも実行できますという風にデコレー ターってかなりこうシンプルにかけてで ファンクションの中のドックストリングス とかを使ってくれるので結構あの家読性と かもう悪くないかなという風には思います 先ほどと同じようにですねこレコメンド アイテムってものはちゃんと選んでくれた んですけれどもレコメンドアイテムに渡し ているデートっていうのは2021年10 月15日という意味の分からない日付を 渡してしまっていますとこれはなぜかと言 とまもちろんそのおすすめ商品を教えて くださいって言ってるので勝手に日付を 決めて聞いてしまっているというところ ですね はいでデコレーターの方もですね同じよう にファンクションを見てみるとえっとこう いう風になるという感じですねただあの 先ほどと同じようにですねパダンティック とか使っていないのでプロパティの パラメーターとかは適当になってしまって いるというような感じになりますただ今回 の場合はですねこうデコレーターの場合は フロムファンクションの方を使ったこっち の場合ですねこのフロムファンクションを 使った時と違ってきちんとデートを入れて くれているっていうことが特徴的かなと いう風には思いますフロンファンクション を使った場合っていうのはパダンティック を渡さないとデートっていうものに対して ファッションを入れたりとかよく分から ないものを入れたりすることがあったと 思うんですけれどもデコレーターを使った 場合はこのデートっていうものを渡した時 にきちんとこうデートを返してくれるよう な情報っていうものを渡せてくれている ような感じですねいずれにせを正しい共同 を求めるためにはこのままあまり情報を 与えないで実行するってのは適切ではない ので特に自社ツールサービスとして提供 する側としてはですねなのでそういう意味 ではきちんとバイダンティックなどを使っ て適切な情報を渡しながら関数っていう ものを渡していく必要があるかなという風 に思いますなのでえっと先ほどと同じよう にですねデコレーターを使った場合も パイナティックの情報っていうものを渡す ことができるのでデコレーターの引き数に アーグススキーマていうものを渡してで そのパイランティックの情報を渡すことで 適切な答えを返すことができるようになる というところです同じようにですねま結果 も変わらないので はいまこういう風に変えてきました ちょっと答えがどんどんどんどん変わって きてますApple製品だったりとか AppleStoreとかオンライン ショップとかうんまここら辺に関しては 本当に適切な答えがまだ返ってきてない なっていうような状態だとは思います エージェントのファンクションを見るとえ きちんとファナティックで渡した情報って いうのが渡っていることが分かると思い ますこのようにですねこのデートっていう ものはデートっていう引き数はお勧めし たい商品を絞るために使われる日付ですっ ていうのが渡っているというところです もう少しですね今までこうおすめの商品を 教えてくださいっていう風に言っていたの でデートっていうものが1月10日であっ たりとかよくわかんない日付になっていた のではないかっていうことを推測してでは 昨日のおすすめ商品を教えてくださいって いうものを聞いてみますてそうするとどう なるかと言うとえ昨日のおすめ商品を教え てくださいって言ってるのにレコメンド アイテムに渡されてるのは2021の10 月20日っていうものが渡されれてること が分かりますとチャットGPTにこう昨日 とか今日とか昨日とか渡したら日付を ちゃんと入れてくれるのかなと期待するっ ていうことがあると思うんですけれども 実際はそんな入れてくれないっていうこと がまこの結果から分かるということですね もう少し例えばディスクリプションとかを 適切にすればうまくいくのかもしれないん ですけど今回は違う手段でえっと解決 しようと思いますで今回どういう風に解決 するかと言うとデートタイムを使って 新しい日付けを取得するような関数自社 ツールってものを設けようと思います ゲットデートっていうツールっていうもの を作ってでそのツールは何をするかという とま今日や昨日など日付を取得する時に 使います引き数には今日を起点として何日 か前をNで与えますとでここのそのドック ストリングスにですねこう引数に関する 情報を書いてるんですけども実際はですね パダンティックで引き数の説明した方が ベタだとは思いますただえっとドック スリンに書いてツールの説明として与える だけでも結構チャットGPTは賢いので うまく判断してくれるということですでお 見せしますはいこの日付関数を入れると どうなるかと言うとこういう風な感じです ねはい今日のおすめの商品を教えてくださ いっていう風にするとtodayて書い てき来てこれもうまく動いてないですね こういうことが結構ありますとうまく動か ないことがあるとでもう1回実行してみ ましょうかはいでもならではですね ちょっと30分で終える予定が46分に なってしまったんですがもう少しで終わる ので続けます前回のやつだとですね直接 レコメンドアイテムを呼び出してデートに 対してtoデイていうものを渡してしまっ てたんですけれども今回はうまく動いてい てまずは日付を取得するためにゲットデイ トっていうものを呼び出しますとで今日な ので今日からの起点のえN日前だったらN に0を渡せばいいということが分かるとだ からゲットデートに0を渡して帰ってきた ものが2023年の9月30日っていう ものが帰ってきてで9月30日のえお勧め 商品は何ですがま書いてくるものは全部 AppleなのでAppleってものが 帰ってきてることが分かると思います同じ ようにですねじゃあどういう ファンクションが渡されてたかっていうの をお見せするとサクっとですねで今回あの ファンクション2つ渡されていて レコメンドアイテムっていう ファンクションとゲットデイトっていう ファンクションが渡されていますで それぞれのファンクションの説明がこの ように与えられているということが分かり ますもう少しツールを複雑にしていって ですね奇数日だったらオレンジ偶数日だっ たらえAppleっていうものを返すよう なツールにちょっとだけレコメンド アイテムを変えみようと思い ますはいではえこれを実行するとですね 今日はえ9月30日なのでもちろん9月 30日偶数日ですね偶数日なので Appleが返ってくるきちんと帰ってき ますねAppleが帰ってきましたでは 昨日の場合はどうなのか 昨日はい昨日の場合は29日なので1日前 ってものをゲットデイトで取得して29日 ってものが書いていますで29日を使って え昨日のおすめ商品オレンジっていうもの を取ることができるというところ ですやとうまく動きました ねでもう少しだけちょっと時間が1時間 ぐらい喋ってしまうような感になるんです けれどもう少しだけお話させてくださいで これだけだともちろんまだまだ連携が 少ないかなっていうところがあってで次に じゃあAppleだったら100円 オレンジだったら200円っていうような ものを商品の値段を返すような関数って いうものを組み込んでみましょうでツール としては元々グビだったらAppleキビ だったらオレンジを返すようなものとあと は日付を取得するツールであとは今回定義 したAppleとオレンジAppleだっ たら100円オレンジだったら200円 っていうのを返すツールっていうものを 定義しますでこれでエージェントのですね おすめの商品の値段まで教えてくださいっ ていうことをやるとそうするとまず最初に え昨日なのでゲットデートでN=1を渡し て1日前のデートを取ってきますで9月 29日ですで次にレコメンドアイテムって いうツールを呼び出してレコメンド アイテムにえ9月29日を渡してで最後に ですねアイテムプライスっていうので アイテムネームをえオレンジにとして渡し てでオレンジはは200円なのでえ昨日の おすめ商品であるオレンジの値段は 200円ですっていう最終的な答えが返っ てきましたというところになりますだいぶ 柔軟なものができるようになってきたって いうことが理解できると思いますで他には ですね最後なんですけれども取得した結果 を吐き出したいとかデータベースに 書き込みたいとかあったりすると思うん ですけどもそういうことももちろん簡単に できますと基本的にはファンクションが あってそのファンクション中身は自分で 書いていくので何でもすることができると いうところですねちょっとあの output.tchっていうところに 書き込むようなものを記事的にですね データベースに書き込んでいるようなもの として紹介しようと思いますで今回は さらに追加するツールとしてはですね セーブデータっていうツールを追加します とセーブデータっていうものは何なのかと いうとここにも説明書いてあるんです けれども日付と商品名と商品の値段を 受け取ってデータベースに書き込みますっ ていうようなものをツールとして作りまし たデータベースといながらもアウトプット テキストにただ書き込むだけのものになる んですけどもデートアイテムネーム アイテムプライスを書き込むだけのツール となりますま実際に自社ツールを使う時に はここにSQL分であったりとかを書いて データベスに書き込むとかをやったりする ことになると思いますこちらもですね ちょっと簡易的にですねフダンティックを 使わずにえドックストリングだけで何を やるかってのを渡して書き込めるかどう かってのをお見せしていこうと思いますで 同様にですね同じように何をしたいかと 言うと今日のおすめ商品の値段を調べて データベースに書き込んでくださいって いうことを実行し ますで最初に動くものとしてはまず日付を 取得するゲットデイトが動いてN=0で 今日の日付を取ります9月30日ですと9 月30日が取れたら9月30日の レコメンドアイテムは何ですかていうので レコメンドアイテムのツールを起動して 書いてきたものがAppleですとであ次 にやることしてはAppleの値段は いくらですかていうものをアイテム プライスに聞いてアイテムプライスが返し てきた答えが100円ですとで100円 っっていうものをえセーブデータに渡して セーブデータに渡すものとしてはデートと アイテムネームとアイテムプライスこれは 日付と商品目と商品の値段を受け取 りっていう風に書いてあるのでこういう ようなものを基数として与えて データベースに書き込みますていうことを この操作でやるということですねなので 細かいことを書いた方がミスは少ないん ですけれどもあんまり書かなくても動いて くれるっていうのがまやっぱりこう チャットGPTのうまいところかなという 風には思いますで最終的に帰ってきたこと としては今日のえおすすめ商品の値段は 100円ですとでデータベースに正常に 書き込まれましたっていうので データベースではないんですけどれも テキストファイルっていうものを中身見て みましょうと今日の日付2023年9月 30日でおすすめの商品Appleで最終 的に100円っていうのが書き込まれた ことが確認できましたとはいとても長く なってしまったんですけれどもこちらで テモは終わりとさせていただきたいと思い ますはい私の発表は以上となります ありがとうございました\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 19305 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msummarize_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43myoutube\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chains/base.py:292\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    293\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    294\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    295\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    296\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chains/base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    279\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    280\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    281\u001b[0m     inputs,\n\u001b[1;32m    282\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chains/combine_documents/base.py:105\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    104\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 105\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chains/combine_documents/refine.py:151\u001b[0m, in \u001b[0;36mRefineDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Combine by mapping first chain over all, then stuffing into final chain.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    150\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_initial_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 151\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_llm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m refine_steps \u001b[38;5;241m=\u001b[39m [res]\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs[\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chains/llm.py:257\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chains/base.py:292\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    293\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    294\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    295\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    296\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chains/base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    279\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    280\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    281\u001b[0m     inputs,\n\u001b[1;32m    282\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chains/llm.py:93\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m     91\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 93\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chains/llm.py:103\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chat_models/base.py:458\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    452\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    456\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    457\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chat_models/base.py:348\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    347\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 348\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    349\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    350\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    352\u001b[0m ]\n\u001b[1;32m    353\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chat_models/base.py:338\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    337\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 338\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m         )\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chat_models/base.py:490\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m     )\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chat_models/openai.py:343\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    342\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 343\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chat_models/openai.py:282\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/chat_models/openai.py:280\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 19305 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "summarize_chain(youtube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b770a4-b92a-48e3-a8f3-548e2c490116",
   "metadata": {},
   "source": [
    "もとの文書が長すぎると、token数制限に引っかかりエラーとなってしまう。\n",
    "\n",
    "長い文書はload_and_splitを使ってドキュメントを分割する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bb9fbbb8-a1bd-459b-adb1-b75c7463d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_youtube = YoutubeLoader.from_youtube_url(\n",
    "   youtube_url=\"https://www.youtube.com/watch?v=I2UZutRrtEE\",\n",
    "   language=\"ja\"\n",
    ").load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7f7c237a-59df-4d18-96ff-6c12171b19bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"では早速なんですけれども始めさせて いただきます私からはラングチェイン エージェントを使って自社ツールと チャットGPTを連携するというタイトル で発表させていただきますChatGPT 自体は皆さんご存知のように非常に優れた 能力を持っていますが最新のデータに アクセスできないとか特定の数学的な推論 が得意でないなどいくつかの課題が存在し ますこれらの課題は自社プロダクトに チャートGPTを導入するなどチャット GPTを社会実装しようとした際に解決し なければなりません今回お伝えするラグ チェインエイジェントを導入すると ChatGPTと外部ツールをうまく連携 させることができるようになりChat GPTが抱える課題を克服しチットGPT の可能性をさらに広げることができます 本日はラングチェインエージェントをどう 活用することでチャットGPTの能力を 拡張できるのか具体的にご紹介していき ますそれでは始めていきますこちらが私の プロフィールとなります内容はいつもと 同じなのでスキップさせていただきます はいでは具体的な話に入る前にですね今回 のテーマの結論に先に触れておきます結論 なんですけれどもラグチェインには様々な エージェントの種類が存在するが自社 ツールとチャットGPTを効果的に連結 する際汎用的なエージェントタイプである ゼロショットリアクトまたはオAI ファンクションを利用することが簡単と なりますゼロショットリアクトとOpen AIファンクションを比較するとOpen AIファンクションの方がチットGPT からより適切かつ精度の高い答えを 引き出しやすくなっていますこちらはデモ のでお見せしますねでそしてえっとここで ChatGPTと自社ツール連携で非常に 重要となるものとしてツールの説明という ものがありますと通常のプロンプトチャト GPTを使う時のプロンプトと同様に ツールの特性や機能をChatGPTに 正確かつ効果的に伝えることで自社ツール とチャットGPTの連携の成功率が格段に 向上していきますこのポイントに着目し ながら具体的な方法論について今日は 深掘りしていきたいと思いますそれでは これらのポイントを年頭にきつつ今回の 発表をお聞きください本日の発表は以下の 3つのセクションから構成されています 初めにチットGPTの基本的な概要につい て触れます次にラングチェイン エイジェントの特徴について紹介し最後に デモンストレーションを行いますデモでは 特にツールの説明について着目して いただけると幸いですそれでは最初の トピックに移りましょうここではチット GPTの全体的な特徴を振り返っていき ますすでにご存知の内容も多いと思うん ですけれども一般的なはチャットGPTの 性質っていうものをご紹介していきます チットGPTは実用的な大規模言語モデル llmで現在多くのビジネスに活用されて きています例えばベネッセ ホールディングスでは車内AIChat ベネッセGPTを開発しグループ社員 1.5万人に向けて提供しているという ようなプレスを出していたりとか立メカ 大学では大学の英語事業において機械翻訳 とChatGPTを組み合わせたサービス が試験導入されているであったりとか他に も色々なサービスがチャトGPTを 組み込み始めておりChatGPTの実用 化っていうものがすごい勢いで広がってい ますChatGPTを活用する際によく 言われることとしてはチャットGPTから 良い結果回答を得るためには良い質問が 必要であるということですでこれは プロンプトエンジニアリングと呼ばれる 領域で今最も伸びている業種の1つと言え ますプロンプトエンジニアリングではどの ような項目をプロンプトに入れるべきか またプロンプトを複数回うまく使いこ なくして良い回答をるテクニックなどが 研究開発されていますリアクトであったり とかいろんなことを聞いたことあると思う んですけれどもプロンプトの種類っていう のがどんどんどんど開発されているという ことですでチャットGPTの活用は多に 渡っておりその可能性は広がりつつあり ますでプロンプトエンジニアリングの テクニックを活用することでチャット GPTとの適切なコミュニケーションが 可能になりますプロンプト エンジニアリングでは色々なテクニックが 存在するのですが今回はプロンプト エンジニアリング自体がメインのテーマで はないのでプロンプトエジライニングの中 で抑えるべき基本的な項目とそしてよく 使われるまフーショットプロンティングと 呼ばれるような手法を簡単にだけ紹介して いきますはいまず基礎として抑えていき たいこととしてプロンプトに何を与える べきかっていうものをいくつか紹介して いきますで1点目としては具体的で明確に 質問することですで例えばその日本の歴史 において重要な出来事は何ですかという 質問よりも日本の戦国時代における重要な 出来事について教えてくださいという具体 的に質問し方が的確なコ答が得られますと で2点目に状況やコンテクストを明示する ことですて背景情報があるとチャット GPTはコンテクストをより理解し適切な 回答が返ってきますとで3点目期待する 回答の形式を指定するとこれにより望む 情報が整理されて帰ってきますリスト形式 で返してくださいであったりとか ジェイソン形式で返してくださいであっ たりとかまそういうことを質問する時に プロンプトの中に入れるということですね で4点目範囲を限定すること範囲が広す すぎると回答も後半で一般的になりがちに なりますなので範囲を狭めるということが 重要となってきますで最後5点目自性や 認証を明確にすることですと例えば子供で も分かるようにしてくださいとか中学生で も分かるようにしてくださいとかなんか そういう風にどの視点で書て思ってみるか を示したりであったりとか何年の情報が 欲しいとかそういうことを指定することに よって回答が明確なものが帰ってきやすく なるということですで次にプロンティ テクニックの1つののフューショット プロンティについて簡単に説明します フューショットプロンティはモデルに少数 の例を示すことで特定のタスクの解決方法 を学習させるアプローチになりますこれに よりモデルは様々な問に対して適切な回答 を生成することができますここに例として 上げてるんですけれども例えばレストラン 最高でしたこれは感情ポジティブですこの 映画は本当に時間の無駄だった感情 ネガティブですというようにいくつかの例 を与えて出したいものでそれぞれの文に 対しての感情っていうものをこのように 並べていきますてで最後に聞きたいものと してそのサービスは非常に遅く不満ですっ ていうものがあった時にどんな感情で すかっていうことを聞くと感情ネガティブ という風に返ってくるというところですで こういう風にすることによってモデルって いうものが新しい文章がポジティブか ネガティブかっていうのをま過去の例を元 に判断することができるようになるという ことですでこの方法によってですねフュー ショットプロンティはモデルに多様な タスクを効率的に学習させることができ 柔軟な対応が可能になってくるということ でですでここまでいくつかのプロンティの テクニックであったりとかをお話ししてき ましたもちろん他にもたくさんあってです ねチェインオブソツであったりとかあと セルフアスクであったりとかいろんなもの があるんですけれども今回のメインテーマ ではないので一旦プロンティグの話はここ で終わりにしてでここでちょっと一息つい てですねこれらのテクニックが持つ限界に ついて考えてみましょうでまず1点目なん ですけれどもプロンプトのテクニックは 日々進化し開発されていますとこれによっ てチットGPTは多様なタスクや質問に 対応でできるようになってきていますただ 全ての作業やタスクをチャットGPTに 依存すること自体が現実的ではありません とえ例えば先ほどの例で出てきた文章から の感情抽出に関してはチャートGPTに 無理やりさせることが適切かと言われると そうでもないかもしれないAWSコプリン のような勘定分析に特化した学習をして いるAPIっていうものがあってそれを 使った方がいい結果が返ってくるかもしれ ませんっていうのがまず1つ目ですねで次 に2点目としてそのChatGPTが持つ 情報っていうものは訓練データに限定され ていてチットGPTはそれ以外のデータに ついては知らないというところですなので えっとチャットGPTは過去のデータで 学習されておりその学習データ以降に出て きた情報に関してはチットGPTに尋ね られませんとよく言われるものとしては 2021年のデータですかねまでのデータ で学習してるので例えば2022年であっ たりとか2023年のデータについて聞い たとしてもチャットGPTがうまく答え られないというようなところですね最新の 価格的な研究であったりとかニュース記事 とか特定の地域の天気情報であったりとか チットGPTの訓練データに含まれてい ないような情報っていうものはChat GPTだけでは取得できませんし インターネットにあるデータだけではなく て例えば今回のテーマである自社のツール と連携するっていう時のためには自社の データを使いたいみたいな話とかあると 思うんですけれども自社データについては チャットGPTが知るわけもないという\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The speaker introduces the topic of integrating the company's tool with ChatGPT using the LangChain Agent. They highlight the challenges faced by ChatGPT, such as limited access to the latest data and specific mathematical inference. The LangChain Agent aims to resolve these challenges and expand the potential of ChatGPT. The presentation will cover the basics of ChatGPT, the features of LangChain Agent, and a demonstration. They also discuss the importance of providing accurate information about the tool to improve the success rate of integrating ChatGPT with external tools. The speaker mentions the growing use of ChatGPT in various business sectors and the need for good prompt engineering techniques to obtain accurate answers. They briefly explain the basics of prompt engineering and mention the Fusion Probing technique as an example. However, they acknowledge that not all tasks can be solely reliant on ChatGPT and that it has limitations in terms of evolving information and accessing external data.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "とか特定の地域の天気情報であったりとか チットGPTの訓練データに含まれてい ないような情報っていうものはChat GPTだけでは取得できませんし インターネットにあるデータだけではなく て例えば今回のテーマである自社のツール と連携するっていう時のためには自社の データを使いたいみたいな話とかあると 思うんですけれども自社データについては チャットGPTが知るわけもないという ことでえっとChatGPTに自社の データについて聞いたとしてもChat GPTはもちろん答えることができません とこれらのチャGPTの限界を理解する ことでチットGPTの適切な使用方法で あったりとかそれらをどのようなシナリオ で活用するべきかっていうのが見えてくる ということですねでChatgpdの概要 の最後なんですけれどもChatgpdや その他大記号言語モデルllm自体の課題 をこちらにまとめますでまず1点目なん ですけれども先ほどと被るんですが ChatGPTは最新の情報にアクセスが できないと例えば為替や株価のように国 一刻と変化していく情報に対してチト GPTははリアルタイムで対応することが できませんで2つ目の課題としては ChatGPTが独自の情報源にアクセス する能力を持たないということです企業の データベースに保存されている顧客名簿で あったりとか特定のプライベートな情報に はアクセスすることができませんで3つ目 に水論の正確性に課題があるというところ ですチャットGPTは時折り簡単な算術 計算でさえも間違えることがありますこれ は特に正確な情報や結果が求められる タスクにおいて大きな問題となりますねで そして最後にチャットgpdができない こととかのデータが足りないんだったら ファインチューニングすればいいんじゃ ないかっていうような意見もあると思うん ですけれどもファインチューニングする ことによって汎用性が劣化してしまうと いうことがありますとでChatGPTを 特定のタスク例えば自社データを食わせる であったりとか特定のタスクに特化した データを食わせてファインチューニング すると他の多様なタスクに対する性能が 低下すると言われていますでこれらの課題 を解決するための1つの手段として今回お 話しするラングチェインエージェントって いうものがありますでここここからは ラングチェインエイジェントを使って チットGPTの性能を生かしつつ苦手な ものは他のツールで解決していくっていう ことについてお話ししていきますで参考 までにですねここにバッと上げている課題 に関してはMrKLシステムズていう有名 な論文があってそ論文から引っ張ってきた ものとなりますMrKLシステムズって いうのはミラクルシステムズと呼ぶもので チャットGPTと他のツールを連携して どんどんどんどん課題を解いていくって いうことを提示しているロムとなってい ますそれではラングチェインエージェント についてのトピックに移っていきますまず ラングチェインエージェントの前にですね ラングチェインとは何かと言うとラング チェインとは言語モデルを利用する アプリケーションのためのフレームワーク ですでラングチェインについては前回の 勉強会でもご紹介しているので基礎的な 内容を知りたい方はYouTubeで前回 の勉強会の動画をご覧くださいでラング チェインエージェントはラングチェインの 中の1つの機能なんですけれどもラング チェインの中でも非常に面白くて重要な 役割を果していますランチェの主なタスク は次にどんなアクションを取るべきかを大 記号言語モデルllmに決定させそれを 実行するということですで具体的な プロセスは以下の通りですとまず各ツール ができることと質問を指定したレスポンス のフォーマットでllmに送信します聞き ますとチットGPTなどのllmに聞き ますそしてllmChatGPTから帰っ てきたレスポンスをパースして次の アクションを決めるということをしますで このプロセスによってラングチェインジと はユーザーの4級に応じて効率的かつ正確 なアクションを実行できるようになって いるというところですでここでラング チェインエージェントがどのように動く かっていうもののイメージを具体的な例と して1つ持ってきましたま正確に正しい 動き方をしてるわけではないんですけれど も大体こういうような動きをするんだよっ ていうこともご理解ください例えばチット GPTに対してコミュニケーションを取る のがラングチェインのエージェントとなる んですけれどもラングチェイン エージェントにですねまずこう今日東京で で出かける時時に適切な服装は何ですかて いうのを聞きますでそうするとラング チェインエージェントが聞きたい内容今日 東京で出かける時に適切な服装っていう ものとあと自分が持っているツールって いうものをプロンプトに入れてチャット GPTに聞いてくれますとで自分が持っ てるツールっていうものは例えば Google検索のツールっていうも持っ てますとGoogle検索のツールでは 最新の情報を引っ張ってくることができ ますと私が持ってるのはGoogle検索 のツールで最新の情報が引っ張ってこれ ますそしてで今回聞質問としては東京で 出かける時に適切な服装って何でかていう ものを聞きますとでその時にチャット GPTはどのように返してくるかというと でまずは検索ツールでインプットは東京の 気温っっていうもので調べなさいっていう ものをエージェントの方に返してきますで エージェントはチャットGPTからから 教えてもらったじゃあえ検索ツールを使え ばいいのかっていうところで検索ツールを 使おうとしますでインプットとしては チャトGPTが教えてくれたように東京の 気温っていうものをインプットとして入れ てで検索ツールから帰ってきた22度って いうものを変してもらいますとでそして またえっとチャットGPTにえこのツール に入れた答えってものが22°でした次何 すればいいんですかであったりとか最終的 にどうなんですすかっていうのをチャット GPTに聞いてチャットGPTが十分な 情報を得たっていう風に判断するとでは 22°なので薄の長袖でがいいでしょうっ ていうものをエージェントとしに返してで あとはえ答えとしてラングチェインの エージェントが東京は22度なので素の 長袖でがいいでしょうっていうものをその まま返してくるというような流れになり ます特徴的なものとしてはチャットGPT だと今日の天気であったりとか東京の天気 であったりていうものは取れないんですね でそこをGoogle検索のツールって いうものを利用することによって新しい データっていうのを取れるようにしてると いうところですねもう1つ押えておきたい こととしてはこの問を解く時に自分が持っ ているツールっていうもの一覧を渡して どのツールを使えばいいかっていうのこと 自体もチャットGPTに判断してもらうっ ていうところが面白いところとなってい ますはいでこれがラングチェイン エージェントリアクトフレームワークとか 呼ばれたりするようなやり方ですねはい これらのこと全部ラングチェインの エージェントがやってくれるというところ ですでラングチェインエージェントって いうものは様々なタイプのエージェントが 用意されていますとここでエージェントの タイプっていうものを紹介するんです けれども解きたい課題に応じて エージェントを使い分けることによって 精度の高い回答を得られるようになると いうことですここで6個ほど上げています でまず1番汎用的なものとしてはゼロ ショットリアクトエージェントというもの ですでこのエージェントはリアクト フレームワークを利用してツールの説明に 基づいてどのツールを使用すべきか判断し ます先ほどの例で出てきたものと同じです ねこれによって特定のトレーニングデータ なしで多様なツールの適切なこう利用が 可能になってきます次にストラクチャード インプットリアクタエージェントですで このエージェントは複数の入力を使う エージェントを活用することができますで これによってより複雑なツールの理由用が 簡単になりタスクの効率が向上しますと 続いてOpenAIファンクション エージェントですこのエージェントはオ AIのファンクションコーリングの機能を 使っており関数が呼び出されるタイミング であったりとかを検出したりとか適切な 入力を関数に渡すような微調整がえオープ AI自体チットGPT自体がされていて それを使うことができますとOpenAI のえファンクションコーリングこの エージェントが使うOpenAIの ファンクションコーリングについてはこの 後のスライドでもう少し詳しく説明します で次にコンバーションエージェントですで このエージェントは会話での使用目的とし て設計されています何かを聞いて何か答え を出すっていうようなま単純なチャット GPTで何かこう答えを作ってくださいと かそういうのとは違くて会話的な インタラクションを通じて効果的な コミュニケーションを可能にするような エージェントとなっていますそして次に あげるものとしてはセルフアクwith sarchエージェントですねでそこの エージェントはですねセルフアクという 質問を自問自としながら分割することに よって正しい解を導き出す方法を使って\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The speaker discusses the limitations of ChatGPT, such as its inability to access the latest data or specific private information. They also mention the challenges of accuracy and the potential degradation of performance through fine-tuning. To address these issues, they introduce the LangChain Agent, a framework within LangChain that utilizes language models to enhance the capabilities of ChatGPT. The LangChain Agent can integrate with other tools, such as Google search, to provide real-time and accurate information. The speaker presents a scenario where the LangChain Agent interacts with ChatGPT to determine the appropriate clothing for a specific weather condition in Tokyo. They also mention different types of agents within the LangChain framework that can be used depending on the specific task requirements.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "GPTで何かこう答えを作ってくださいと かそういうのとは違くて会話的な インタラクションを通じて効果的な コミュニケーションを可能にするような エージェントとなっていますそして次に あげるものとしてはセルフアクwith sarchエージェントですねでそこの エージェントはですねセルフアクという 質問を自問自としながら分割することに よって正しい解を導き出す方法を使って 情報を検索するエージェントですセルフア クっていうプロンティグの技術があるん ですねまそれを使ってちゃんとした正しい 答えを導き出すエージェントとなってい ますで最後に紹介するものがリアクト ドキュメントストアエージェントですで このエージェントはドックストア ドキュメントストアですねと対話するため にリアクトフレームワークを使用してき ますえこれによって文書の管理であったり とか検索が一元化されて効率的に情報 アクセスが可能となりますこちらに関して はまた今後の勉強会でも取り扱っていき たいかなという風に思ってるんですけれど も文書検索管理っていうものに特化した エージェントとなっていますで以上がラン チェインエージェントの各エージェントの 使用となりますでそれぞれのタイプが 異なるシナリオやタスクに適しており ラングチェインの柔軟性と多様性を象徴し ているものとなります今回のセッションで はですね最も汎用的なゼロショット リアクトとチャットGPT自体に色々判断 させるファンクションコーリングを使った オープAIファンクション2を利用して いき ますでは先ほど説明を省いてしまったん ですけれどもOpenAIファンクション コーリンググっていうのはOpenAI ファンクションコーリングじゃないですね OpenAIファンクションですね OpenAIファンクションとは何か エージェントタイプの1つであるOpen AIファンクションとは何かについてお 話ししますこちらはChatGPTを提供 しているOpenAIのファンクション コーリングという機能を利用していますで ファンクションコーリングっていうものは 2023年の6月にリリースされた機能で ラングチェインエージェントが担っていっ たツールを渡してチャットGPTに聞いて 次のステップを決定するやり取りってい ものをチットGPT自体が行えるように ファインチューニングされているものと なりますでこの機能では東京の天気を教え てくれというような先ほどと同じように ですね質問と共にどういう関数を持って いるのかその関数の説明をチトGPTに 直接投げるということをやりますなので 先ほどのラングチェインエイジェントが やってることと同じようなことをやれる ようになっていてチットGPTが ファンクションをどのように受け取るのか そのファンクションのパラメータであっ たりとか説明とかをよりチャトGPTが 理解しやすいようにAPIとして受け取る ことができるようになっているのでより 適切な判断をチャットGPTがしやすく なっているというところがメリットとなり ますなので今メリットをお話ししてしまっ たんですけれどもファンクション コーリングを使うメリットとしては ChatGPTにツールの挙動をより正確 に伝えやすくなっているというところで これによってChatGPTが機能の ツールのえファンクションの使用や動作を より正確に理解しやすくなっていて適切な 判断をしやすくなっているとていうのとで もう1つのメリットとしてはチャット GPTっていうものがラングチェイン エージェンドって元々チャットGPTが 返してきた個体を利用者が頑張って定期 表現を使って解析する必要があったんです がそれをする必要がなくなったっていう ところが利点としてありますとでチャット GPTが実際にラングチェインを使って どのツールを使えばいいですかていうのを 聞くんですけどそれをどのようにパースし てるかというとこのように頑張ってですね えっとレギュラーエクスプレッションを 使って正規表現を使ってパースしたりとか して次のアクションを決めてたりするって いうのがラングチェが今までやっていた ことであるとでこういうパースをしなきゃ いけないっていうことをしなくて済む つまりパースしなきゃいけなかったので今 まではそのパースすることでえっとうまく パースできなくてミスってしまうっていう こともあったんですけどそういうことが なくなるっていうことが期待されるという ところです はいはいちょっと駆け足になってしまった んですがここまでで20分ぐらいかかって しまいましたねえここからがレモに入り ますで実際にですねラングチェイン エージェントを使ってチャトGPTと自社 ツールを連結するような感じのえデモって いうものをお見せしていきますで使う エージェントとしてはゼロショット リアクトとオーエファンクションを使って いきますであとは今回のプレゼンの冒頭の 方でもお話ししたようにChatGPTに ツールの説明をどのように渡せばいいのか であったりとかその辺りを持ち替えて いただければ幸いです はいではではやっていきますちょっと時間 がなくなってきたので駆け足でいきますね でまずは設定ファイル読んでいきますこれ 何をやってるかと言うとえOpenAIの APIキーであったりとかを読んでいます でまずはですねえっとゼロショットの方を 見せしていきたいんですがおマジないとし てですねおじないというかま前回の勉強会 と同じようにですねラングチェイン エージェントを使うためのやり方をお見せ していきますで実際今回使うモデルとして はチットOpenAPAIというか ChatGPTのえGPT3.5のtbo 0613というものを使っていきますで 外部のツールとしてはGoogleサーチ のAPIっていうものを今回用意しました ここであのサプAPIっていうものを書い ているものがえGoogleのAPIと なりますで聞きたい質問としては今日の 東京と北海道の天気を調べて東京から 北海道の出張で来ていくべきを教えて くださいっていうものを聞きたいと思い ますでラングチェインエージェント自体を 走らせてみますでこれがゼロショット リアクトエージェントを使った場合どの ようになるかっていうのをお見せしますで エージェントタイプに関しては イニシャライズエージェントの エージェントっていうえ引数に対して エージェントタイプのこのゼロショット リアクトディスクリプションていうものを 渡すとエージェントを選択することができ ますちなみになんですけれどもえこれって ただあの文字列が返ってくるだけですかね あのゼロショットリアクト ディスクリプションていうものが返って くるだけのものになりますなので文字列で 与えても同じですはいでは実行してみ ましょうチトgpdはもちろん今日の天気 とか今日の東京とか北海道の天気っていう のは知らないので来ていくべき服装って いうことは分からないわけですね気温も 分からないのででこれでラングチェイン エージェントが私はGoogle検索の ツールを持ってますよでそれを使って使う ことができますでこの問題課題っていう ものを解くためにはどうすればいいですか てのを聞くっていうことですねでここで ざーっと出てきてるんですけれども実際に 書いてきた答えとしてはtheWisto isCloudyaboutcrear withtempof歌詞になっちゃって ますけどまこのように帰ってきますと 英語で帰ってきちゃってますねでこういう 風にえどんなアドレスをれ切ればいい かっていうものがちゃんと帰ってきてい ますと前にやった時はそんなに綺麗に帰っ てきてなかったんですけれども今回は うまく帰ってきていますはいこちらがゼロ ショットリアクトディスクリプションで やった場合の結果になりますでどういう ことをやってるかっていうのなこの辺りを 見ていただくといいのかなという風には 思いますでチットGPTにまずラング チェインエージェントが問題とツールを 渡してチットGPTに何をすればいい かっていうのを聞きますでチャットGPT から帰ってきた答えとしては次の アクションとしてはサーチをしてください ですサーチのインプットとしてはウザー 東京todayていうものを渡してくださ いっていうのを渡してますで次の アクションとしてはサーチをしてください でそのインプットとしてはウザ北海道 todayていうものを渡すようにして くださいとで最終的にその帰ってきた答 っていうものをチャットGPTに渡して 最終的な文が出てくるということですはい でこれが1つ目ですねアエージェント タイプ1つ目のえゼロショットリアクト ディスクリプションで元々ラングチェイン エイジェントがリアクトフレームワークで ツールを渡したりとかしてチャットGPT とうまく連携しながら外部ツールを使う ようにしたえ機能となっていますでこれが ですねえ最近出てきた6月にリリースされ たOpenAIのファンクションコーリン グっていうものを使うとどうなるかって いうのをお見せします はいでOpenAIファンクション OpenAIのファンクションコーリング を使うためにはエージェントタイプの OpenAIファンクションズっていう ものを渡しますでそうするとえこのように 動きますで実際にやってることとしては ラングチェインエージェントがOpen\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The speaker discusses the limitations of ChatGPT and introduces the LangChain Agent as a framework to enhance its capabilities. They mention different types of agents within the LangChain framework, including the Self-Acting Search Agent and the Reactive Document Store Agent. The LangChain Agent can integrate with tools like Google search to provide real-time and accurate information. The speaker presents a scenario where the LangChain Agent interacts with ChatGPT to determine appropriate clothing for a specific weather condition. They also mention the benefits of using OpenAI's Function Calling feature to improve communication with ChatGPT and eliminate the need for manual parsing.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "たOpenAIのファンクションコーリン グっていうものを使うとどうなるかって いうのをお見せします はいでOpenAIファンクション OpenAIのファンクションコーリング を使うためにはエージェントタイプの OpenAIファンクションズっていう ものを渡しますでそうするとえこのように 動きますで実際にやってることとしては ラングチェインエージェントがOpen AIのファンクションコーリングっていう ものを使ってうまくチャットGPTと やり取りしながら答えを導き出していくっ ていうことをやっていますはい出てきた 答えはこちらですねで同じようにですね どのようにやっているかっていうものは ここら辺の途中で出てきているんです けれども途中結果でえサーチとしてはえ クエリー東京の今日の天気っていうものを 調べてくださいでサーチっていうツルを 使って今日の北海道今日の天気っていう ものを調べてくださいでこれらが帰ってき た答えっていうものをえチャットGPTに 返してチャトGPTがその与えられた情報 を元に東京の今日の天気は曇りですが後に は晴れる予報ですで気温は接し30°歌 86°でコス確率はどうのこうのっていう ものがあってであとは東京は暑くなる可能 性があるので軽い服装が適しています ただし北海道は比較的涼しいので ジャケットなどを持っていくことがお勧め しますというような感じに帰ってきてます ま見て分かると思うんですけれどもま日本 語にな質問が日本語だったので日本語で ちゃんと返してくれるっていうのは もちろんのことなんですけれども書いてき た答えっていうものに関してより詳しく 返してくれているっていうことが分かると 思いますなので使ってみた所管としては ですねここで言うオAIファンクションの ファンクションコーリングっていうのの ファンクションっていうのはラング チェインエージェントでいうツールと同じ なんですけれどもそのファンクションで あったりとかツールを渡す時にですね渡し て何かを判断判させたいチャットGPTに 判断してもらいたいっていう時はえ エージェントタイプゼロショットリアクト ディスクリプションよりもエージェント タイプオAIファンクションの方がより いい結果が返ってきやすいっていうことが 分かると思い ますもう少しあの詳しく見ていきます ちなみにですねOpenAIにどんな ファンクションを渡してるかっていうのは イニシャライズエージェントのところで エージェントを定義してるんですけども そのエージェントの中からエージェントっ ていうものとファンクションズってものを 実行するとどんなえファンクションが渡さ れてかっていうのを見ることができますで 今回渡されたファンクションっていうのは ネームサーチっていうファンクションが 渡されていてどんなことができるかって いうのでサチエンジンですっていうような ものをやってますであとはパラメーターと してはこういうものを渡していきますって いうのが分かりますとはいでここまでが 基本的なラングチェインエージェントを 使って外部ツールを連携するっていうお話 をしてきましたとで次にですねえ自社で 作ったツールっていうものを連携させ るっていところについてお見せしていき ますで一旦はですねレコメンドアイテムっ ていうものを考えてみようと思いますで ここ何をやりたいかって言うとレコメンド アイテムまこの中身はほとんど意味のない ものになってるんですけれどもレコメンド アイテムっていう関数を自社で持っていて でレコメンドアイテムっていうものは自社 のデータベースなどにアクセスして日付を 持ったにこの日のレコメンドっていうもの はこういうものですっていうものを返す ような関数っていうところになりますとな のでえっとこのレコメンドアイテムって いうところをこの関数ををえChat GPTのフローの中に組み込みたいって いうことをやっていきますでどのように ラングチェインエージェントを使って ChatGPTに連携すればいいかという とツールっていうものにえ変換していく 必要がありますとでレコメンドアイテム ツールっていうものをえツールfrom ファンクションズっていうものを使って どのようなファンクションを実行したい ですかレコメンドアイテムっていう ファンクションを実行したいですまこう いう風にですねこの今回のあまり意味の ないファンクションなんですけれどもこれ を実行しますとでツールの名前としてはえ レコメンドアイテムっていうものを渡し ますでツールのディスクリプション何を やってくれるかっていうことに関してはお すめの商品を教えてくれますっていうのを ツールとして定義するというところです はいでこれをちょっとツールズっていうま ツールまとめて渡さないといけないで ツールの配列の中に入れておきます実際に ですねラングチェインエージェントにどの ようにツールを渡せばいいかっていうとま 先ほどもカラっとやってたんですけれども このツール1覧っていうのを最初に渡して 引数として渡しているのでまこのようにし てどんなツールが使えるかっていうものを ラングチェインのAに渡しでチャット GPTと連携していきますはいでえっと 今回聞きたいこととしてはえおすすめの 商品を教えてくださいっていうものを投げ てみようと思いますで使うエージェントと しては先ほどゼロシトリアクトよりもより いい結果が返ってきたオーエ ファンクションズってものを選びます はいはいでは実行し ますはいで書いてきたものとしてはですね コメドアイテムってものが実行されて withファッションという謎のものをえ 入力に入れてで実際この引数がですね デートの中にファッションが入ってで プリントファッションっていうところでこ でファッションが出てきてしまってるとで ただおすめの商品は今回レコメンド アイテムで書いてきたAppleですって いうのが書いてきてることが分かると思い ますはい見て分かるようにですねなんか よくわからない引数は与えてしまってる なっていうところがありますでこを どんどん解決していこうと思いますで次に やることとしては今回なんでこのように変 なファッションとかいう引数を与えて しまったかというと引数に関する情報が ないんですね与えられてないというところ になりますとなので引数何与えればいいん ですかみたいなところが情報としてないが ゆえにチャットGPTもレコメンド アイテムを使うことは分かるとレコメンド アイテムを使えばいいのは分かるんだけど レコメンドアイテムに対してちょっと 分からないからカテゴリー情報でも渡して みようかなっていうのでファッションを 渡してしまっているというような状態に なりますで例えばまファンクションを見た としても今回使ってるファンクションに 関してはレコメンドアイテムでおすめの 商品を教えてくれますでパラメーターの 情報は全くありませんっていう状態なので どんなパラメーターを渡せばいいかって いうのはオープAIファンクションは 分からないというところですねじゃあこれ に対してどうすればいいかっていうのをお 見せしますでえっと結構簡単でしてパダ ティックっていうものをご存知ですかね こう最近えっとファストAPIとかですか ねこう結構パであったりとかそのヒントを 与えるっていうことがまPython主流 になってきてると思うんですけれども フダンティックを使っていきますでえっと ご注意していただきたいこととしては ファインディッてもうV2が出てるんです けれどもV2を使うとラングチェイン エージェントを動きませんとなので パティックV1を指定して方に関する情報 をえ与えるためのそのベイスモデルとか フィールドを使ってえやっていきますで これ何をやってるかと言うとま今回の インプットに対してデイトっていうものが ありますがデイトていうものはお勧めした 商品を絞るために使われる日付ですって いうものを与えていますこれをえっと アーグススキーマ先ほどのツールを定義 する時にアグススキーマっていう引きを 使ってこのレコメンドアイテムインットっ ていうベースモデルから継承した レコメンドアイテムインプットっていう インプットの報引数の情報を持ったクラ スっていうものを渡すということをします とでこうするとどうなるかっていうのをお 見せしていき ますはいじゃあどう変わる かいきますはい実行します全く同じですね 実行するものとしてはで今回はレコメンド アイテムっていうものに対して先ほどと 同じレコメンドアイテムっていうものが 選ばれましたとコアイテムに対してデイト が2020年の12月1日っていうものを 渡しているっていうことが分かりますで それによってですね気付がちゃんと渡して くれたっていうことがわか分かるわけです ねで今回あのここであのデートっていう ものはディスクリプションでお勧めしたい 商品を絞るために使われる日付っていう ことを伝えたのでチャットGPTの方があ これは日付のことなんだっていうところで 引数はファッションのようなカテゴリーを 渡すんではなくてデートの引数っていう ものを渡すことができてるというところ ですはいでこちらがデトのまあ2022年 の12月1日もはや日付が意味が分から ないですけれどもこういうものが渡されて いるというところですねでおすめ商品は Appleですという風にでこういう風に\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The speaker introduces the LangChain Agent as a framework to enhance ChatGPT's capabilities. They demonstrate the use of OpenAI's Function Calling feature to integrate ChatGPT with external tools. The LangChain Agent interacts with ChatGPT to determine appropriate clothing for specific weather conditions. They also discuss the use of Pathetic to provide additional information and context to ChatGPT's responses.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "ことを伝えたのでチャットGPTの方があ これは日付のことなんだっていうところで 引数はファッションのようなカテゴリーを 渡すんではなくてデートの引数っていう ものを渡すことができてるというところ ですはいでこちらがデトのまあ2022年 の12月1日もはや日付が意味が分から ないですけれどもこういうものが渡されて いるというところですねでおすめ商品は Appleですという風にでこういう風に ですねこうツールの説明をどんどん どんどん入れていってええ解決していく 必要があるというところですねで同じよう にですねちょっとエージェントの ファンクションがどのように渡されるてる かっていうのをお見せしますとで今回渡さ れたのはレコメンドアイテムインプトって いうのをえパイダンテックの方で定義して 渡しているのでえ実際にえっと ファンクションOpenAIのえ ファンクションコーリングにどのように 渡されてるかというとパラメーターとして ですねこういう風なレコメンドアイテム インプットっていうものが渡されていて プロパティとしてはデートがあってデー トっていうのはお勧めしたい商品を絞る ために使われる日付っていうものが入れ られてえ飛ばされてるということが分かる と思いますはい で他にはですねこのようにツールフロン ファンクションで作るっていうこともある んですけれども同じようにですね デコレーターを使うっていうこともでき ますとデコレーターの方が結構シンプルに 書けるので私は結構デコレーターを使って 書くことが多いですとでデコレーター何を すればいいかというとデコターっていうの はま関数の前にですねアマークツールなん ちゃらかんちゃらっていうものがあってで それを使ってえっとこのツールってのは何 ができるかっていうものをえドッ ストリングで与えるんですねな先ほどと 同じにするとレコメンドアイテムツールっ ていうものがあったら中身は同じですね プリントデイトとAppっていうものを 返しますとプリントデイトしてApple っていうものを返しますとででやることと してはおすめ商品を教えてくれます全く 同じ内容ですねでこのツールの名前って いうものはデコレーターの引数として レコメンドアイテムとして渡していると いうところですで同じようにですね ツールズの一覧として入れてでこれをえ 実行してみ ましょうはいこれデコレーター使った場合 です ね実行するものは全部同じですで同じよう にこれも実行できますという風にデコレー ターってかなりこうシンプルにかけてで ファンクションの中のドックストリングス とかを使ってくれるので結構あの家読性と かもう悪くないかなという風には思います 先ほどと同じようにですねこレコメンド アイテムってものはちゃんと選んでくれた んですけれどもレコメンドアイテムに渡し ているデートっていうのは2021年10 月15日という意味の分からない日付を 渡してしまっていますとこれはなぜかと言 とまもちろんそのおすすめ商品を教えて くださいって言ってるので勝手に日付を 決めて聞いてしまっているというところ ですね はいでデコレーターの方もですね同じよう にファンクションを見てみるとえっとこう いう風になるという感じですねただあの 先ほどと同じようにですねパダンティック とか使っていないのでプロパティの パラメーターとかは適当になってしまって いるというような感じになりますただ今回 の場合はですねこうデコレーターの場合は フロムファンクションの方を使ったこっち の場合ですねこのフロムファンクションを 使った時と違ってきちんとデートを入れて くれているっていうことが特徴的かなと いう風には思いますフロンファンクション を使った場合っていうのはパダンティック を渡さないとデートっていうものに対して ファッションを入れたりとかよく分から ないものを入れたりすることがあったと 思うんですけれどもデコレーターを使った 場合はこのデートっていうものを渡した時 にきちんとこうデートを返してくれるよう な情報っていうものを渡せてくれている ような感じですねいずれにせを正しい共同 を求めるためにはこのままあまり情報を 与えないで実行するってのは適切ではない ので特に自社ツールサービスとして提供 する側としてはですねなのでそういう意味 ではきちんとバイダンティックなどを使っ て適切な情報を渡しながら関数っていう ものを渡していく必要があるかなという風 に思いますなのでえっと先ほどと同じよう にですねデコレーターを使った場合も パイナティックの情報っていうものを渡す ことができるのでデコレーターの引き数に アーグススキーマていうものを渡してで そのパイランティックの情報を渡すことで 適切な答えを返すことができるようになる というところです同じようにですねま結果 も変わらないので はいまこういう風に変えてきました ちょっと答えがどんどんどんどん変わって きてますApple製品だったりとか AppleStoreとかオンライン ショップとかうんまここら辺に関しては 本当に適切な答えがまだ返ってきてない なっていうような状態だとは思います エージェントのファンクションを見るとえ きちんとファナティックで渡した情報って いうのが渡っていることが分かると思い ますこのようにですねこのデートっていう ものはデートっていう引き数はお勧めし たい商品を絞るために使われる日付ですっ ていうのが渡っているというところです もう少しですね今までこうおすめの商品を 教えてくださいっていう風に言っていたの でデートっていうものが1月10日であっ たりとかよくわかんない日付になっていた のではないかっていうことを推測してでは 昨日のおすすめ商品を教えてくださいって いうものを聞いてみますてそうするとどう なるかと言うとえ昨日のおすめ商品を教え てくださいって言ってるのにレコメンド アイテムに渡されてるのは2021の10 月20日っていうものが渡されれてること が分かりますとチャットGPTにこう昨日 とか今日とか昨日とか渡したら日付を ちゃんと入れてくれるのかなと期待するっ ていうことがあると思うんですけれども 実際はそんな入れてくれないっていうこと がまこの結果から分かるということですね もう少し例えばディスクリプションとかを 適切にすればうまくいくのかもしれないん ですけど今回は違う手段でえっと解決 しようと思いますで今回どういう風に解決 するかと言うとデートタイムを使って 新しい日付けを取得するような関数自社 ツールってものを設けようと思います ゲットデートっていうツールっていうもの を作ってでそのツールは何をするかという とま今日や昨日など日付を取得する時に 使います引き数には今日を起点として何日 か前をNで与えますとでここのそのドック ストリングスにですねこう引数に関する 情報を書いてるんですけども実際はですね パダンティックで引き数の説明した方が ベタだとは思いますただえっとドック スリンに書いてツールの説明として与える だけでも結構チャットGPTは賢いので うまく判断してくれるということですでお 見せしますはいこの日付関数を入れると どうなるかと言うとこういう風な感じです ねはい今日のおすめの商品を教えてくださ いっていう風にするとtodayて書い てき来てこれもうまく動いてないですね こういうことが結構ありますとうまく動か ないことがあるとでもう1回実行してみ ましょうかはいでもならではですね ちょっと30分で終える予定が46分に なってしまったんですがもう少しで終わる ので続けます前回のやつだとですね直接 レコメンドアイテムを呼び出してデートに 対してtoデイていうものを渡してしまっ てたんですけれども今回はうまく動いてい てまずは日付を取得するためにゲットデイ トっていうものを呼び出しますとで今日な ので今日からの起点のえN日前だったらN に0を渡せばいいということが分かるとだ からゲットデートに0を渡して帰ってきた ものが2023年の9月30日っていう ものが帰ってきてで9月30日のえお勧め 商品は何ですがま書いてくるものは全部 AppleなのでAppleってものが 帰ってきてることが分かると思います同じ ようにですねじゃあどういう ファンクションが渡されてたかっていうの をお見せするとサクっとですねで今回あの ファンクション2つ渡されていて レコメンドアイテムっていう ファンクションとゲットデイトっていう ファンクションが渡されていますで それぞれのファンクションの説明がこの ように与えられているということが分かり ますもう少しツールを複雑にしていって ですね奇数日だったらオレンジ偶数日だっ たらえAppleっていうものを返すよう なツールにちょっとだけレコメンド アイテムを変えみようと思い ますはいではえこれを実行するとですね 今日はえ9月30日なのでもちろん9月 30日偶数日ですね偶数日なので Appleが返ってくるきちんと帰ってき ますねAppleが帰ってきましたでは 昨日の場合はどうなのか 昨日はい昨日の場合は29日なので1日前 ってものをゲットデイトで取得して29日 ってものが書いていますで29日を使って え昨日のおすめ商品オレンジっていうもの を取ることができるというところ ですやとうまく動きました ねでもう少しだけちょっと時間が1時間 ぐらい喋ってしまうような感になるんです\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The LangChain Agent is introduced as a framework to enhance ChatGPT's capabilities. OpenAI's Function Calling feature is used to integrate ChatGPT with external tools. The LangChain Agent interacts with ChatGPT to determine appropriate clothing for specific weather conditions. The use of Pathetic is discussed to provide additional information and context to ChatGPT's responses. The introduction of a new tool called \"GetDate\" is proposed to obtain the current date and improve the accuracy of recommendations. The decorator function is also mentioned as a simpler alternative to create tools. However, it is noted that passing incorrect or insufficient information may lead to inaccurate responses.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Appleが返ってくるきちんと帰ってき ますねAppleが帰ってきましたでは 昨日の場合はどうなのか 昨日はい昨日の場合は29日なので1日前 ってものをゲットデイトで取得して29日 ってものが書いていますで29日を使って え昨日のおすめ商品オレンジっていうもの を取ることができるというところ ですやとうまく動きました ねでもう少しだけちょっと時間が1時間 ぐらい喋ってしまうような感になるんです けれどもう少しだけお話させてくださいで これだけだともちろんまだまだ連携が 少ないかなっていうところがあってで次に じゃあAppleだったら100円 オレンジだったら200円っていうような ものを商品の値段を返すような関数って いうものを組み込んでみましょうでツール としては元々グビだったらAppleキビ だったらオレンジを返すようなものとあと は日付を取得するツールであとは今回定義 したAppleとオレンジAppleだっ たら100円オレンジだったら200円 っていうのを返すツールっていうものを 定義しますでこれでエージェントのですね おすめの商品の値段まで教えてくださいっ ていうことをやるとそうするとまず最初に え昨日なのでゲットデートでN=1を渡し て1日前のデートを取ってきますで9月 29日ですで次にレコメンドアイテムって いうツールを呼び出してレコメンド アイテムにえ9月29日を渡してで最後に ですねアイテムプライスっていうので アイテムネームをえオレンジにとして渡し てでオレンジはは200円なのでえ昨日の おすめ商品であるオレンジの値段は 200円ですっていう最終的な答えが返っ てきましたというところになりますだいぶ 柔軟なものができるようになってきたって いうことが理解できると思いますで他には ですね最後なんですけれども取得した結果 を吐き出したいとかデータベースに 書き込みたいとかあったりすると思うん ですけどもそういうことももちろん簡単に できますと基本的にはファンクションが あってそのファンクション中身は自分で 書いていくので何でもすることができると いうところですねちょっとあの output.tchっていうところに 書き込むようなものを記事的にですね データベースに書き込んでいるようなもの として紹介しようと思いますで今回は さらに追加するツールとしてはですね セーブデータっていうツールを追加します とセーブデータっていうものは何なのかと いうとここにも説明書いてあるんです けれども日付と商品名と商品の値段を 受け取ってデータベースに書き込みますっ ていうようなものをツールとして作りまし たデータベースといながらもアウトプット テキストにただ書き込むだけのものになる んですけどもデートアイテムネーム アイテムプライスを書き込むだけのツール となりますま実際に自社ツールを使う時に はここにSQL分であったりとかを書いて データベスに書き込むとかをやったりする ことになると思いますこちらもですね ちょっと簡易的にですねフダンティックを 使わずにえドックストリングだけで何を やるかってのを渡して書き込めるかどう かってのをお見せしていこうと思いますで 同様にですね同じように何をしたいかと 言うと今日のおすめ商品の値段を調べて データベースに書き込んでくださいって いうことを実行し ますで最初に動くものとしてはまず日付を 取得するゲットデイトが動いてN=0で 今日の日付を取ります9月30日ですと9 月30日が取れたら9月30日の レコメンドアイテムは何ですかていうので レコメンドアイテムのツールを起動して 書いてきたものがAppleですとであ次 にやることしてはAppleの値段は いくらですかていうものをアイテム プライスに聞いてアイテムプライスが返し てきた答えが100円ですとで100円 っっていうものをえセーブデータに渡して セーブデータに渡すものとしてはデートと アイテムネームとアイテムプライスこれは 日付と商品目と商品の値段を受け取 りっていう風に書いてあるのでこういう ようなものを基数として与えて データベースに書き込みますていうことを この操作でやるということですねなので 細かいことを書いた方がミスは少ないん ですけれどもあんまり書かなくても動いて くれるっていうのがまやっぱりこう チャットGPTのうまいところかなという 風には思いますで最終的に帰ってきたこと としては今日のえおすすめ商品の値段は 100円ですとでデータベースに正常に 書き込まれましたっていうので データベースではないんですけどれも テキストファイルっていうものを中身見て みましょうと今日の日付2023年9月 30日でおすすめの商品Appleで最終 的に100円っていうのが書き込まれた ことが確認できましたとはいとても長く なってしまったんですけれどもこちらで テモは終わりとさせていただきたいと思い ますはい私の発表は以上となります ありがとうございました\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "CPU times: user 222 ms, sys: 45.1 ms, total: 267 ms\n",
      "Wall time: 2h 18min 35s\n"
     ]
    }
   ],
   "source": [
    "%time summary_output = summarize_chain(split_youtube)[\"output_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8210327b-96ce-49d8-9b58-eb2a86085aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The LangChain Agent is introduced as a framework to enhance ChatGPT\\'s capabilities. OpenAI\\'s Function Calling feature is used to integrate ChatGPT with external tools. The LangChain Agent interacts with ChatGPT to determine appropriate clothing for specific weather conditions. The use of Pathetic is discussed to provide additional information and context to ChatGPT\\'s responses. The introduction of a new tool called \"GetDate\" is proposed to obtain the current date and improve the accuracy of recommendations. The decorator function is also mentioned as a simpler alternative to create tools. However, it is noted that passing incorrect or insufficient information may lead to inaccurate responses. The addition of a tool called \"SaveData\" is suggested to write data to a database. Examples of using the \"SaveData\" tool to write date, item name, and item price are provided. The potential use of SQL statements for database operations is mentioned. The demonstration showcases the capability to write data to a text file. The final summary confirms that the recommended price for today\\'s item, Apple, is $100 and it has been successfully written to the database.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade72201-29f4-41ec-9e19-6f10955812fc",
   "metadata": {},
   "source": [
    "## QA Chain\n",
    "Document Loaderで取得したDocumentにQA Chainを使って質問をする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "39c9a03f-2fae-4c2f-85df-81b9e76d1a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "qa_chain = load_qa_chain(llm=llm, chain_type=\"map_reduce\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b8ca56c-ffd0-4f30-82ff-857de13cece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "では早速なんですけれども始めさせて いただきます私からはラングチェイン エージェントを使って自社ツールと チャットGPTを連携するというタイトル で発表させていただきますChatGPT 自体は皆さんご存知のように非常に優れた 能力を持っていますが最新のデータに アクセスできないとか特定の数学的な推論 が得意でないなどいくつかの課題が存在し ますこれらの課題は自社プロダクトに チャートGPTを導入するなどチャット GPTを社会実装しようとした際に解決し なければなりません今回お伝えするラグ チェインエイジェントを導入すると ChatGPTと外部ツールをうまく連携 させることができるようになりChat GPTが抱える課題を克服しチットGPT の可能性をさらに広げることができます 本日はラングチェインエージェントをどう 活用することでチャットGPTの能力を 拡張できるのか具体的にご紹介していき ますそれでは始めていきますこちらが私の プロフィールとなります内容はいつもと 同じなのでスキップさせていただきます はいでは具体的な話に入る前にですね今回 のテーマの結論に先に触れておきます結論 なんですけれどもラグチェインには様々な エージェントの種類が存在するが自社 ツールとチャットGPTを効果的に連結 する際汎用的なエージェントタイプである ゼロショットリアクトまたはオAI ファンクションを利用することが簡単と なりますゼロショットリアクトとOpen AIファンクションを比較するとOpen AIファンクションの方がチットGPT からより適切かつ精度の高い答えを 引き出しやすくなっていますこちらはデモ のでお見せしますねでそしてえっとここで ChatGPTと自社ツール連携で非常に 重要となるものとしてツールの説明という ものがありますと通常のプロンプトチャト GPTを使う時のプロンプトと同様に ツールの特性や機能をChatGPTに 正確かつ効果的に伝えることで自社ツール とチャットGPTの連携の成功率が格段に 向上していきますこのポイントに着目し ながら具体的な方法論について今日は 深掘りしていきたいと思いますそれでは これらのポイントを年頭にきつつ今回の 発表をお聞きください本日の発表は以下の 3つのセクションから構成されています 初めにチットGPTの基本的な概要につい て触れます次にラングチェイン エイジェントの特徴について紹介し最後に デモンストレーションを行いますデモでは 特にツールの説明について着目して いただけると幸いですそれでは最初の トピックに移りましょうここではチット GPTの全体的な特徴を振り返っていき ますすでにご存知の内容も多いと思うん ですけれども一般的なはチャットGPTの 性質っていうものをご紹介していきます チットGPTは実用的な大規模言語モデル llmで現在多くのビジネスに活用されて きています例えばベネッセ ホールディングスでは車内AIChat ベネッセGPTを開発しグループ社員 1.5万人に向けて提供しているという ようなプレスを出していたりとか立メカ 大学では大学の英語事業において機械翻訳 とChatGPTを組み合わせたサービス が試験導入されているであったりとか他に も色々なサービスがチャトGPTを 組み込み始めておりChatGPTの実用 化っていうものがすごい勢いで広がってい ますChatGPTを活用する際によく 言われることとしてはチャットGPTから 良い結果回答を得るためには良い質問が 必要であるということですでこれは プロンプトエンジニアリングと呼ばれる 領域で今最も伸びている業種の1つと言え ますプロンプトエンジニアリングではどの ような項目をプロンプトに入れるべきか またプロンプトを複数回うまく使いこ なくして良い回答をるテクニックなどが 研究開発されていますリアクトであったり とかいろんなことを聞いたことあると思う んですけれどもプロンプトの種類っていう のがどんどんどんど開発されているという ことですでチャットGPTの活用は多に 渡っておりその可能性は広がりつつあり ますでプロンプトエンジニアリングの テクニックを活用することでチャット GPTとの適切なコミュニケーションが 可能になりますプロンプト エンジニアリングでは色々なテクニックが 存在するのですが今回はプロンプト エンジニアリング自体がメインのテーマで はないのでプロンプトエジライニングの中 で抑えるべき基本的な項目とそしてよく 使われるまフーショットプロンティングと 呼ばれるような手法を簡単にだけ紹介して いきますはいまず基礎として抑えていき たいこととしてプロンプトに何を与える べきかっていうものをいくつか紹介して いきますで1点目としては具体的で明確に 質問することですで例えばその日本の歴史 において重要な出来事は何ですかという 質問よりも日本の戦国時代における重要な 出来事について教えてくださいという具体 的に質問し方が的確なコ答が得られますと で2点目に状況やコンテクストを明示する ことですて背景情報があるとチャット GPTはコンテクストをより理解し適切な 回答が返ってきますとで3点目期待する 回答の形式を指定するとこれにより望む 情報が整理されて帰ってきますリスト形式 で返してくださいであったりとか ジェイソン形式で返してくださいであっ たりとかまそういうことを質問する時に プロンプトの中に入れるということですね で4点目範囲を限定すること範囲が広す すぎると回答も後半で一般的になりがちに なりますなので範囲を狭めるということが 重要となってきますで最後5点目自性や 認証を明確にすることですと例えば子供で も分かるようにしてくださいとか中学生で も分かるようにしてくださいとかなんか そういう風にどの視点で書て思ってみるか を示したりであったりとか何年の情報が 欲しいとかそういうことを指定することに よって回答が明確なものが帰ってきやすく なるということですで次にプロンティ テクニックの1つののフューショット プロンティについて簡単に説明します フューショットプロンティはモデルに少数 の例を示すことで特定のタスクの解決方法 を学習させるアプローチになりますこれに よりモデルは様々な問に対して適切な回答 を生成することができますここに例として 上げてるんですけれども例えばレストラン 最高でしたこれは感情ポジティブですこの 映画は本当に時間の無駄だった感情 ネガティブですというようにいくつかの例 を与えて出したいものでそれぞれの文に 対しての感情っていうものをこのように 並べていきますてで最後に聞きたいものと してそのサービスは非常に遅く不満ですっ ていうものがあった時にどんな感情で すかっていうことを聞くと感情ネガティブ という風に返ってくるというところですで こういう風にすることによってモデルって いうものが新しい文章がポジティブか ネガティブかっていうのをま過去の例を元 に判断することができるようになるという ことですでこの方法によってですねフュー ショットプロンティはモデルに多様な タスクを効率的に学習させることができ 柔軟な対応が可能になってくるということ でですでここまでいくつかのプロンティの テクニックであったりとかをお話ししてき ましたもちろん他にもたくさんあってです ねチェインオブソツであったりとかあと セルフアスクであったりとかいろんなもの があるんですけれども今回のメインテーマ ではないので一旦プロンティグの話はここ で終わりにしてでここでちょっと一息つい てですねこれらのテクニックが持つ限界に ついて考えてみましょうでまず1点目なん ですけれどもプロンプトのテクニックは 日々進化し開発されていますとこれによっ てチットGPTは多様なタスクや質問に 対応でできるようになってきていますただ 全ての作業やタスクをチャットGPTに 依存すること自体が現実的ではありません とえ例えば先ほどの例で出てきた文章から の感情抽出に関してはチャートGPTに 無理やりさせることが適切かと言われると そうでもないかもしれないAWSコプリン のような勘定分析に特化した学習をして いるAPIっていうものがあってそれを 使った方がいい結果が返ってくるかもしれ ませんっていうのがまず1つ目ですねで次 に2点目としてそのChatGPTが持つ 情報っていうものは訓練データに限定され ていてチットGPTはそれ以外のデータに ついては知らないというところですなので えっとチャットGPTは過去のデータで 学習されておりその学習データ以降に出て きた情報に関してはチットGPTに尋ね られませんとよく言われるものとしては 2021年のデータですかねまでのデータ で学習してるので例えば2022年であっ たりとか2023年のデータについて聞い たとしてもチャットGPTがうまく答え られないというようなところですね最新の 価格的な研究であったりとかニュース記事 とか特定の地域の天気情報であったりとか チットGPTの訓練データに含まれてい ないような情報っていうものはChat GPTだけでは取得できませんし インターネットにあるデータだけではなく て例えば今回のテーマである自社のツール と連携するっていう時のためには自社の データを使いたいみたいな話とかあると 思うんですけれども自社データについては チャットGPTが知るわけもないという\n",
      "Human: ChatGPTが不得意なことを教えて\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "とか特定の地域の天気情報であったりとか チットGPTの訓練データに含まれてい ないような情報っていうものはChat GPTだけでは取得できませんし インターネットにあるデータだけではなく て例えば今回のテーマである自社のツール と連携するっていう時のためには自社の データを使いたいみたいな話とかあると 思うんですけれども自社データについては チャットGPTが知るわけもないという ことでえっとChatGPTに自社の データについて聞いたとしてもChat GPTはもちろん答えることができません とこれらのチャGPTの限界を理解する ことでチットGPTの適切な使用方法で あったりとかそれらをどのようなシナリオ で活用するべきかっていうのが見えてくる ということですねでChatgpdの概要 の最後なんですけれどもChatgpdや その他大記号言語モデルllm自体の課題 をこちらにまとめますでまず1点目なん ですけれども先ほどと被るんですが ChatGPTは最新の情報にアクセスが できないと例えば為替や株価のように国 一刻と変化していく情報に対してチト GPTははリアルタイムで対応することが できませんで2つ目の課題としては ChatGPTが独自の情報源にアクセス する能力を持たないということです企業の データベースに保存されている顧客名簿で あったりとか特定のプライベートな情報に はアクセスすることができませんで3つ目 に水論の正確性に課題があるというところ ですチャットGPTは時折り簡単な算術 計算でさえも間違えることがありますこれ は特に正確な情報や結果が求められる タスクにおいて大きな問題となりますねで そして最後にチャットgpdができない こととかのデータが足りないんだったら ファインチューニングすればいいんじゃ ないかっていうような意見もあると思うん ですけれどもファインチューニングする ことによって汎用性が劣化してしまうと いうことがありますとでChatGPTを 特定のタスク例えば自社データを食わせる であったりとか特定のタスクに特化した データを食わせてファインチューニング すると他の多様なタスクに対する性能が 低下すると言われていますでこれらの課題 を解決するための1つの手段として今回お 話しするラングチェインエージェントって いうものがありますでここここからは ラングチェインエイジェントを使って チットGPTの性能を生かしつつ苦手な ものは他のツールで解決していくっていう ことについてお話ししていきますで参考 までにですねここにバッと上げている課題 に関してはMrKLシステムズていう有名 な論文があってそ論文から引っ張ってきた ものとなりますMrKLシステムズって いうのはミラクルシステムズと呼ぶもので チャットGPTと他のツールを連携して どんどんどんどん課題を解いていくって いうことを提示しているロムとなってい ますそれではラングチェインエージェント についてのトピックに移っていきますまず ラングチェインエージェントの前にですね ラングチェインとは何かと言うとラング チェインとは言語モデルを利用する アプリケーションのためのフレームワーク ですでラングチェインについては前回の 勉強会でもご紹介しているので基礎的な 内容を知りたい方はYouTubeで前回 の勉強会の動画をご覧くださいでラング チェインエージェントはラングチェインの 中の1つの機能なんですけれどもラング チェインの中でも非常に面白くて重要な 役割を果していますランチェの主なタスク は次にどんなアクションを取るべきかを大 記号言語モデルllmに決定させそれを 実行するということですで具体的な プロセスは以下の通りですとまず各ツール ができることと質問を指定したレスポンス のフォーマットでllmに送信します聞き ますとチットGPTなどのllmに聞き ますそしてllmChatGPTから帰っ てきたレスポンスをパースして次の アクションを決めるということをしますで このプロセスによってラングチェインジと はユーザーの4級に応じて効率的かつ正確 なアクションを実行できるようになって いるというところですでここでラング チェインエージェントがどのように動く かっていうもののイメージを具体的な例と して1つ持ってきましたま正確に正しい 動き方をしてるわけではないんですけれど も大体こういうような動きをするんだよっ ていうこともご理解ください例えばチット GPTに対してコミュニケーションを取る のがラングチェインのエージェントとなる んですけれどもラングチェイン エージェントにですねまずこう今日東京で で出かける時時に適切な服装は何ですかて いうのを聞きますでそうするとラング チェインエージェントが聞きたい内容今日 東京で出かける時に適切な服装っていう ものとあと自分が持っているツールって いうものをプロンプトに入れてチャット GPTに聞いてくれますとで自分が持っ てるツールっていうものは例えば Google検索のツールっていうも持っ てますとGoogle検索のツールでは 最新の情報を引っ張ってくることができ ますと私が持ってるのはGoogle検索 のツールで最新の情報が引っ張ってこれ ますそしてで今回聞質問としては東京で 出かける時に適切な服装って何でかていう ものを聞きますとでその時にチャット GPTはどのように返してくるかというと でまずは検索ツールでインプットは東京の 気温っっていうもので調べなさいっていう ものをエージェントの方に返してきますで エージェントはチャットGPTからから 教えてもらったじゃあえ検索ツールを使え ばいいのかっていうところで検索ツールを 使おうとしますでインプットとしては チャトGPTが教えてくれたように東京の 気温っていうものをインプットとして入れ てで検索ツールから帰ってきた22度って いうものを変してもらいますとでそして またえっとチャットGPTにえこのツール に入れた答えってものが22°でした次何 すればいいんですかであったりとか最終的 にどうなんですすかっていうのをチャット GPTに聞いてチャットGPTが十分な 情報を得たっていう風に判断するとでは 22°なので薄の長袖でがいいでしょうっ ていうものをエージェントとしに返してで あとはえ答えとしてラングチェインの エージェントが東京は22度なので素の 長袖でがいいでしょうっていうものをその まま返してくるというような流れになり ます特徴的なものとしてはチャットGPT だと今日の天気であったりとか東京の天気 であったりていうものは取れないんですね でそこをGoogle検索のツールって いうものを利用することによって新しい データっていうのを取れるようにしてると いうところですねもう1つ押えておきたい こととしてはこの問を解く時に自分が持っ ているツールっていうもの一覧を渡して どのツールを使えばいいかっていうのこと 自体もチャットGPTに判断してもらうっ ていうところが面白いところとなってい ますはいでこれがラングチェイン エージェントリアクトフレームワークとか 呼ばれたりするようなやり方ですねはい これらのこと全部ラングチェインの エージェントがやってくれるというところ ですでラングチェインエージェントって いうものは様々なタイプのエージェントが 用意されていますとここでエージェントの タイプっていうものを紹介するんです けれども解きたい課題に応じて エージェントを使い分けることによって 精度の高い回答を得られるようになると いうことですここで6個ほど上げています でまず1番汎用的なものとしてはゼロ ショットリアクトエージェントというもの ですでこのエージェントはリアクト フレームワークを利用してツールの説明に 基づいてどのツールを使用すべきか判断し ます先ほどの例で出てきたものと同じです ねこれによって特定のトレーニングデータ なしで多様なツールの適切なこう利用が 可能になってきます次にストラクチャード インプットリアクタエージェントですで このエージェントは複数の入力を使う エージェントを活用することができますで これによってより複雑なツールの理由用が 簡単になりタスクの効率が向上しますと 続いてOpenAIファンクション エージェントですこのエージェントはオ AIのファンクションコーリングの機能を 使っており関数が呼び出されるタイミング であったりとかを検出したりとか適切な 入力を関数に渡すような微調整がえオープ AI自体チットGPT自体がされていて それを使うことができますとOpenAI のえファンクションコーリングこの エージェントが使うOpenAIの ファンクションコーリングについてはこの 後のスライドでもう少し詳しく説明します で次にコンバーションエージェントですで このエージェントは会話での使用目的とし て設計されています何かを聞いて何か答え を出すっていうようなま単純なチャット GPTで何かこう答えを作ってくださいと かそういうのとは違くて会話的な インタラクションを通じて効果的な コミュニケーションを可能にするような エージェントとなっていますそして次に あげるものとしてはセルフアクwith sarchエージェントですねでそこの エージェントはですねセルフアクという 質問を自問自としながら分割することに よって正しい解を導き出す方法を使って\n",
      "Human: ChatGPTが不得意なことを教えて\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "GPTで何かこう答えを作ってくださいと かそういうのとは違くて会話的な インタラクションを通じて効果的な コミュニケーションを可能にするような エージェントとなっていますそして次に あげるものとしてはセルフアクwith sarchエージェントですねでそこの エージェントはですねセルフアクという 質問を自問自としながら分割することに よって正しい解を導き出す方法を使って 情報を検索するエージェントですセルフア クっていうプロンティグの技術があるん ですねまそれを使ってちゃんとした正しい 答えを導き出すエージェントとなってい ますで最後に紹介するものがリアクト ドキュメントストアエージェントですで このエージェントはドックストア ドキュメントストアですねと対話するため にリアクトフレームワークを使用してき ますえこれによって文書の管理であったり とか検索が一元化されて効率的に情報 アクセスが可能となりますこちらに関して はまた今後の勉強会でも取り扱っていき たいかなという風に思ってるんですけれど も文書検索管理っていうものに特化した エージェントとなっていますで以上がラン チェインエージェントの各エージェントの 使用となりますでそれぞれのタイプが 異なるシナリオやタスクに適しており ラングチェインの柔軟性と多様性を象徴し ているものとなります今回のセッションで はですね最も汎用的なゼロショット リアクトとチャットGPT自体に色々判断 させるファンクションコーリングを使った オープAIファンクション2を利用して いき ますでは先ほど説明を省いてしまったん ですけれどもOpenAIファンクション コーリンググっていうのはOpenAI ファンクションコーリングじゃないですね OpenAIファンクションですね OpenAIファンクションとは何か エージェントタイプの1つであるOpen AIファンクションとは何かについてお 話ししますこちらはChatGPTを提供 しているOpenAIのファンクション コーリングという機能を利用していますで ファンクションコーリングっていうものは 2023年の6月にリリースされた機能で ラングチェインエージェントが担っていっ たツールを渡してチャットGPTに聞いて 次のステップを決定するやり取りってい ものをチットGPT自体が行えるように ファインチューニングされているものと なりますでこの機能では東京の天気を教え てくれというような先ほどと同じように ですね質問と共にどういう関数を持って いるのかその関数の説明をチトGPTに 直接投げるということをやりますなので 先ほどのラングチェインエイジェントが やってることと同じようなことをやれる ようになっていてチットGPTが ファンクションをどのように受け取るのか そのファンクションのパラメータであっ たりとか説明とかをよりチャトGPTが 理解しやすいようにAPIとして受け取る ことができるようになっているのでより 適切な判断をチャットGPTがしやすく なっているというところがメリットとなり ますなので今メリットをお話ししてしまっ たんですけれどもファンクション コーリングを使うメリットとしては ChatGPTにツールの挙動をより正確 に伝えやすくなっているというところで これによってChatGPTが機能の ツールのえファンクションの使用や動作を より正確に理解しやすくなっていて適切な 判断をしやすくなっているとていうのとで もう1つのメリットとしてはチャット GPTっていうものがラングチェイン エージェンドって元々チャットGPTが 返してきた個体を利用者が頑張って定期 表現を使って解析する必要があったんです がそれをする必要がなくなったっていう ところが利点としてありますとでチャット GPTが実際にラングチェインを使って どのツールを使えばいいですかていうのを 聞くんですけどそれをどのようにパースし てるかというとこのように頑張ってですね えっとレギュラーエクスプレッションを 使って正規表現を使ってパースしたりとか して次のアクションを決めてたりするって いうのがラングチェが今までやっていた ことであるとでこういうパースをしなきゃ いけないっていうことをしなくて済む つまりパースしなきゃいけなかったので今 まではそのパースすることでえっとうまく パースできなくてミスってしまうっていう こともあったんですけどそういうことが なくなるっていうことが期待されるという ところです はいはいちょっと駆け足になってしまった んですがここまでで20分ぐらいかかって しまいましたねえここからがレモに入り ますで実際にですねラングチェイン エージェントを使ってチャトGPTと自社 ツールを連結するような感じのえデモって いうものをお見せしていきますで使う エージェントとしてはゼロショット リアクトとオーエファンクションを使って いきますであとは今回のプレゼンの冒頭の 方でもお話ししたようにChatGPTに ツールの説明をどのように渡せばいいのか であったりとかその辺りを持ち替えて いただければ幸いです はいではではやっていきますちょっと時間 がなくなってきたので駆け足でいきますね でまずは設定ファイル読んでいきますこれ 何をやってるかと言うとえOpenAIの APIキーであったりとかを読んでいます でまずはですねえっとゼロショットの方を 見せしていきたいんですがおマジないとし てですねおじないというかま前回の勉強会 と同じようにですねラングチェイン エージェントを使うためのやり方をお見せ していきますで実際今回使うモデルとして はチットOpenAPAIというか ChatGPTのえGPT3.5のtbo 0613というものを使っていきますで 外部のツールとしてはGoogleサーチ のAPIっていうものを今回用意しました ここであのサプAPIっていうものを書い ているものがえGoogleのAPIと なりますで聞きたい質問としては今日の 東京と北海道の天気を調べて東京から 北海道の出張で来ていくべきを教えて くださいっていうものを聞きたいと思い ますでラングチェインエージェント自体を 走らせてみますでこれがゼロショット リアクトエージェントを使った場合どの ようになるかっていうのをお見せしますで エージェントタイプに関しては イニシャライズエージェントの エージェントっていうえ引数に対して エージェントタイプのこのゼロショット リアクトディスクリプションていうものを 渡すとエージェントを選択することができ ますちなみになんですけれどもえこれって ただあの文字列が返ってくるだけですかね あのゼロショットリアクト ディスクリプションていうものが返って くるだけのものになりますなので文字列で 与えても同じですはいでは実行してみ ましょうチトgpdはもちろん今日の天気 とか今日の東京とか北海道の天気っていう のは知らないので来ていくべき服装って いうことは分からないわけですね気温も 分からないのででこれでラングチェイン エージェントが私はGoogle検索の ツールを持ってますよでそれを使って使う ことができますでこの問題課題っていう ものを解くためにはどうすればいいですか てのを聞くっていうことですねでここで ざーっと出てきてるんですけれども実際に 書いてきた答えとしてはtheWisto isCloudyaboutcrear withtempof歌詞になっちゃって ますけどまこのように帰ってきますと 英語で帰ってきちゃってますねでこういう 風にえどんなアドレスをれ切ればいい かっていうものがちゃんと帰ってきてい ますと前にやった時はそんなに綺麗に帰っ てきてなかったんですけれども今回は うまく帰ってきていますはいこちらがゼロ ショットリアクトディスクリプションで やった場合の結果になりますでどういう ことをやってるかっていうのなこの辺りを 見ていただくといいのかなという風には 思いますでチットGPTにまずラング チェインエージェントが問題とツールを 渡してチットGPTに何をすればいい かっていうのを聞きますでチャットGPT から帰ってきた答えとしては次の アクションとしてはサーチをしてください ですサーチのインプットとしてはウザー 東京todayていうものを渡してくださ いっていうのを渡してますで次の アクションとしてはサーチをしてください でそのインプットとしてはウザ北海道 todayていうものを渡すようにして くださいとで最終的にその帰ってきた答 っていうものをチャットGPTに渡して 最終的な文が出てくるということですはい でこれが1つ目ですねアエージェント タイプ1つ目のえゼロショットリアクト ディスクリプションで元々ラングチェイン エイジェントがリアクトフレームワークで ツールを渡したりとかしてチャットGPT とうまく連携しながら外部ツールを使う ようにしたえ機能となっていますでこれが ですねえ最近出てきた6月にリリースされ たOpenAIのファンクションコーリン グっていうものを使うとどうなるかって いうのをお見せします はいでOpenAIファンクション OpenAIのファンクションコーリング を使うためにはエージェントタイプの OpenAIファンクションズっていう ものを渡しますでそうするとえこのように 動きますで実際にやってることとしては ラングチェインエージェントがOpen\n",
      "Human: ChatGPTが不得意なことを教えて\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "たOpenAIのファンクションコーリン グっていうものを使うとどうなるかって いうのをお見せします はいでOpenAIファンクション OpenAIのファンクションコーリング を使うためにはエージェントタイプの OpenAIファンクションズっていう ものを渡しますでそうするとえこのように 動きますで実際にやってることとしては ラングチェインエージェントがOpen AIのファンクションコーリングっていう ものを使ってうまくチャットGPTと やり取りしながら答えを導き出していくっ ていうことをやっていますはい出てきた 答えはこちらですねで同じようにですね どのようにやっているかっていうものは ここら辺の途中で出てきているんです けれども途中結果でえサーチとしてはえ クエリー東京の今日の天気っていうものを 調べてくださいでサーチっていうツルを 使って今日の北海道今日の天気っていう ものを調べてくださいでこれらが帰ってき た答えっていうものをえチャットGPTに 返してチャトGPTがその与えられた情報 を元に東京の今日の天気は曇りですが後に は晴れる予報ですで気温は接し30°歌 86°でコス確率はどうのこうのっていう ものがあってであとは東京は暑くなる可能 性があるので軽い服装が適しています ただし北海道は比較的涼しいので ジャケットなどを持っていくことがお勧め しますというような感じに帰ってきてます ま見て分かると思うんですけれどもま日本 語にな質問が日本語だったので日本語で ちゃんと返してくれるっていうのは もちろんのことなんですけれども書いてき た答えっていうものに関してより詳しく 返してくれているっていうことが分かると 思いますなので使ってみた所管としては ですねここで言うオAIファンクションの ファンクションコーリングっていうのの ファンクションっていうのはラング チェインエージェントでいうツールと同じ なんですけれどもそのファンクションで あったりとかツールを渡す時にですね渡し て何かを判断判させたいチャットGPTに 判断してもらいたいっていう時はえ エージェントタイプゼロショットリアクト ディスクリプションよりもエージェント タイプオAIファンクションの方がより いい結果が返ってきやすいっていうことが 分かると思い ますもう少しあの詳しく見ていきます ちなみにですねOpenAIにどんな ファンクションを渡してるかっていうのは イニシャライズエージェントのところで エージェントを定義してるんですけども そのエージェントの中からエージェントっ ていうものとファンクションズってものを 実行するとどんなえファンクションが渡さ れてかっていうのを見ることができますで 今回渡されたファンクションっていうのは ネームサーチっていうファンクションが 渡されていてどんなことができるかって いうのでサチエンジンですっていうような ものをやってますであとはパラメーターと してはこういうものを渡していきますって いうのが分かりますとはいでここまでが 基本的なラングチェインエージェントを 使って外部ツールを連携するっていうお話 をしてきましたとで次にですねえ自社で 作ったツールっていうものを連携させ るっていところについてお見せしていき ますで一旦はですねレコメンドアイテムっ ていうものを考えてみようと思いますで ここ何をやりたいかって言うとレコメンド アイテムまこの中身はほとんど意味のない ものになってるんですけれどもレコメンド アイテムっていう関数を自社で持っていて でレコメンドアイテムっていうものは自社 のデータベースなどにアクセスして日付を 持ったにこの日のレコメンドっていうもの はこういうものですっていうものを返す ような関数っていうところになりますとな のでえっとこのレコメンドアイテムって いうところをこの関数ををえChat GPTのフローの中に組み込みたいって いうことをやっていきますでどのように ラングチェインエージェントを使って ChatGPTに連携すればいいかという とツールっていうものにえ変換していく 必要がありますとでレコメンドアイテム ツールっていうものをえツールfrom ファンクションズっていうものを使って どのようなファンクションを実行したい ですかレコメンドアイテムっていう ファンクションを実行したいですまこう いう風にですねこの今回のあまり意味の ないファンクションなんですけれどもこれ を実行しますとでツールの名前としてはえ レコメンドアイテムっていうものを渡し ますでツールのディスクリプション何を やってくれるかっていうことに関してはお すめの商品を教えてくれますっていうのを ツールとして定義するというところです はいでこれをちょっとツールズっていうま ツールまとめて渡さないといけないで ツールの配列の中に入れておきます実際に ですねラングチェインエージェントにどの ようにツールを渡せばいいかっていうとま 先ほどもカラっとやってたんですけれども このツール1覧っていうのを最初に渡して 引数として渡しているのでまこのようにし てどんなツールが使えるかっていうものを ラングチェインのAに渡しでチャット GPTと連携していきますはいでえっと 今回聞きたいこととしてはえおすすめの 商品を教えてくださいっていうものを投げ てみようと思いますで使うエージェントと しては先ほどゼロシトリアクトよりもより いい結果が返ってきたオーエ ファンクションズってものを選びます はいはいでは実行し ますはいで書いてきたものとしてはですね コメドアイテムってものが実行されて withファッションという謎のものをえ 入力に入れてで実際この引数がですね デートの中にファッションが入ってで プリントファッションっていうところでこ でファッションが出てきてしまってるとで ただおすめの商品は今回レコメンド アイテムで書いてきたAppleですって いうのが書いてきてることが分かると思い ますはい見て分かるようにですねなんか よくわからない引数は与えてしまってる なっていうところがありますでこを どんどん解決していこうと思いますで次に やることとしては今回なんでこのように変 なファッションとかいう引数を与えて しまったかというと引数に関する情報が ないんですね与えられてないというところ になりますとなので引数何与えればいいん ですかみたいなところが情報としてないが ゆえにチャットGPTもレコメンド アイテムを使うことは分かるとレコメンド アイテムを使えばいいのは分かるんだけど レコメンドアイテムに対してちょっと 分からないからカテゴリー情報でも渡して みようかなっていうのでファッションを 渡してしまっているというような状態に なりますで例えばまファンクションを見た としても今回使ってるファンクションに 関してはレコメンドアイテムでおすめの 商品を教えてくれますでパラメーターの 情報は全くありませんっていう状態なので どんなパラメーターを渡せばいいかって いうのはオープAIファンクションは 分からないというところですねじゃあこれ に対してどうすればいいかっていうのをお 見せしますでえっと結構簡単でしてパダ ティックっていうものをご存知ですかね こう最近えっとファストAPIとかですか ねこう結構パであったりとかそのヒントを 与えるっていうことがまPython主流 になってきてると思うんですけれども フダンティックを使っていきますでえっと ご注意していただきたいこととしては ファインディッてもうV2が出てるんです けれどもV2を使うとラングチェイン エージェントを動きませんとなので パティックV1を指定して方に関する情報 をえ与えるためのそのベイスモデルとか フィールドを使ってえやっていきますで これ何をやってるかと言うとま今回の インプットに対してデイトっていうものが ありますがデイトていうものはお勧めした 商品を絞るために使われる日付ですって いうものを与えていますこれをえっと アーグススキーマ先ほどのツールを定義 する時にアグススキーマっていう引きを 使ってこのレコメンドアイテムインットっ ていうベースモデルから継承した レコメンドアイテムインプットっていう インプットの報引数の情報を持ったクラ スっていうものを渡すということをします とでこうするとどうなるかっていうのをお 見せしていき ますはいじゃあどう変わる かいきますはい実行します全く同じですね 実行するものとしてはで今回はレコメンド アイテムっていうものに対して先ほどと 同じレコメンドアイテムっていうものが 選ばれましたとコアイテムに対してデイト が2020年の12月1日っていうものを 渡しているっていうことが分かりますで それによってですね気付がちゃんと渡して くれたっていうことがわか分かるわけです ねで今回あのここであのデートっていう ものはディスクリプションでお勧めしたい 商品を絞るために使われる日付っていう ことを伝えたのでチャットGPTの方があ これは日付のことなんだっていうところで 引数はファッションのようなカテゴリーを 渡すんではなくてデートの引数っていう ものを渡すことができてるというところ ですはいでこちらがデトのまあ2022年 の12月1日もはや日付が意味が分から ないですけれどもこういうものが渡されて いるというところですねでおすめ商品は Appleですという風にでこういう風に\n",
      "Human: ChatGPTが不得意なことを教えて\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "ことを伝えたのでチャットGPTの方があ これは日付のことなんだっていうところで 引数はファッションのようなカテゴリーを 渡すんではなくてデートの引数っていう ものを渡すことができてるというところ ですはいでこちらがデトのまあ2022年 の12月1日もはや日付が意味が分から ないですけれどもこういうものが渡されて いるというところですねでおすめ商品は Appleですという風にでこういう風に ですねこうツールの説明をどんどん どんどん入れていってええ解決していく 必要があるというところですねで同じよう にですねちょっとエージェントの ファンクションがどのように渡されるてる かっていうのをお見せしますとで今回渡さ れたのはレコメンドアイテムインプトって いうのをえパイダンテックの方で定義して 渡しているのでえ実際にえっと ファンクションOpenAIのえ ファンクションコーリングにどのように 渡されてるかというとパラメーターとして ですねこういう風なレコメンドアイテム インプットっていうものが渡されていて プロパティとしてはデートがあってデー トっていうのはお勧めしたい商品を絞る ために使われる日付っていうものが入れ られてえ飛ばされてるということが分かる と思いますはい で他にはですねこのようにツールフロン ファンクションで作るっていうこともある んですけれども同じようにですね デコレーターを使うっていうこともでき ますとデコレーターの方が結構シンプルに 書けるので私は結構デコレーターを使って 書くことが多いですとでデコレーター何を すればいいかというとデコターっていうの はま関数の前にですねアマークツールなん ちゃらかんちゃらっていうものがあってで それを使ってえっとこのツールってのは何 ができるかっていうものをえドッ ストリングで与えるんですねな先ほどと 同じにするとレコメンドアイテムツールっ ていうものがあったら中身は同じですね プリントデイトとAppっていうものを 返しますとプリントデイトしてApple っていうものを返しますとででやることと してはおすめ商品を教えてくれます全く 同じ内容ですねでこのツールの名前って いうものはデコレーターの引数として レコメンドアイテムとして渡していると いうところですで同じようにですね ツールズの一覧として入れてでこれをえ 実行してみ ましょうはいこれデコレーター使った場合 です ね実行するものは全部同じですで同じよう にこれも実行できますという風にデコレー ターってかなりこうシンプルにかけてで ファンクションの中のドックストリングス とかを使ってくれるので結構あの家読性と かもう悪くないかなという風には思います 先ほどと同じようにですねこレコメンド アイテムってものはちゃんと選んでくれた んですけれどもレコメンドアイテムに渡し ているデートっていうのは2021年10 月15日という意味の分からない日付を 渡してしまっていますとこれはなぜかと言 とまもちろんそのおすすめ商品を教えて くださいって言ってるので勝手に日付を 決めて聞いてしまっているというところ ですね はいでデコレーターの方もですね同じよう にファンクションを見てみるとえっとこう いう風になるという感じですねただあの 先ほどと同じようにですねパダンティック とか使っていないのでプロパティの パラメーターとかは適当になってしまって いるというような感じになりますただ今回 の場合はですねこうデコレーターの場合は フロムファンクションの方を使ったこっち の場合ですねこのフロムファンクションを 使った時と違ってきちんとデートを入れて くれているっていうことが特徴的かなと いう風には思いますフロンファンクション を使った場合っていうのはパダンティック を渡さないとデートっていうものに対して ファッションを入れたりとかよく分から ないものを入れたりすることがあったと 思うんですけれどもデコレーターを使った 場合はこのデートっていうものを渡した時 にきちんとこうデートを返してくれるよう な情報っていうものを渡せてくれている ような感じですねいずれにせを正しい共同 を求めるためにはこのままあまり情報を 与えないで実行するってのは適切ではない ので特に自社ツールサービスとして提供 する側としてはですねなのでそういう意味 ではきちんとバイダンティックなどを使っ て適切な情報を渡しながら関数っていう ものを渡していく必要があるかなという風 に思いますなのでえっと先ほどと同じよう にですねデコレーターを使った場合も パイナティックの情報っていうものを渡す ことができるのでデコレーターの引き数に アーグススキーマていうものを渡してで そのパイランティックの情報を渡すことで 適切な答えを返すことができるようになる というところです同じようにですねま結果 も変わらないので はいまこういう風に変えてきました ちょっと答えがどんどんどんどん変わって きてますApple製品だったりとか AppleStoreとかオンライン ショップとかうんまここら辺に関しては 本当に適切な答えがまだ返ってきてない なっていうような状態だとは思います エージェントのファンクションを見るとえ きちんとファナティックで渡した情報って いうのが渡っていることが分かると思い ますこのようにですねこのデートっていう ものはデートっていう引き数はお勧めし たい商品を絞るために使われる日付ですっ ていうのが渡っているというところです もう少しですね今までこうおすめの商品を 教えてくださいっていう風に言っていたの でデートっていうものが1月10日であっ たりとかよくわかんない日付になっていた のではないかっていうことを推測してでは 昨日のおすすめ商品を教えてくださいって いうものを聞いてみますてそうするとどう なるかと言うとえ昨日のおすめ商品を教え てくださいって言ってるのにレコメンド アイテムに渡されてるのは2021の10 月20日っていうものが渡されれてること が分かりますとチャットGPTにこう昨日 とか今日とか昨日とか渡したら日付を ちゃんと入れてくれるのかなと期待するっ ていうことがあると思うんですけれども 実際はそんな入れてくれないっていうこと がまこの結果から分かるということですね もう少し例えばディスクリプションとかを 適切にすればうまくいくのかもしれないん ですけど今回は違う手段でえっと解決 しようと思いますで今回どういう風に解決 するかと言うとデートタイムを使って 新しい日付けを取得するような関数自社 ツールってものを設けようと思います ゲットデートっていうツールっていうもの を作ってでそのツールは何をするかという とま今日や昨日など日付を取得する時に 使います引き数には今日を起点として何日 か前をNで与えますとでここのそのドック ストリングスにですねこう引数に関する 情報を書いてるんですけども実際はですね パダンティックで引き数の説明した方が ベタだとは思いますただえっとドック スリンに書いてツールの説明として与える だけでも結構チャットGPTは賢いので うまく判断してくれるということですでお 見せしますはいこの日付関数を入れると どうなるかと言うとこういう風な感じです ねはい今日のおすめの商品を教えてくださ いっていう風にするとtodayて書い てき来てこれもうまく動いてないですね こういうことが結構ありますとうまく動か ないことがあるとでもう1回実行してみ ましょうかはいでもならではですね ちょっと30分で終える予定が46分に なってしまったんですがもう少しで終わる ので続けます前回のやつだとですね直接 レコメンドアイテムを呼び出してデートに 対してtoデイていうものを渡してしまっ てたんですけれども今回はうまく動いてい てまずは日付を取得するためにゲットデイ トっていうものを呼び出しますとで今日な ので今日からの起点のえN日前だったらN に0を渡せばいいということが分かるとだ からゲットデートに0を渡して帰ってきた ものが2023年の9月30日っていう ものが帰ってきてで9月30日のえお勧め 商品は何ですがま書いてくるものは全部 AppleなのでAppleってものが 帰ってきてることが分かると思います同じ ようにですねじゃあどういう ファンクションが渡されてたかっていうの をお見せするとサクっとですねで今回あの ファンクション2つ渡されていて レコメンドアイテムっていう ファンクションとゲットデイトっていう ファンクションが渡されていますで それぞれのファンクションの説明がこの ように与えられているということが分かり ますもう少しツールを複雑にしていって ですね奇数日だったらオレンジ偶数日だっ たらえAppleっていうものを返すよう なツールにちょっとだけレコメンド アイテムを変えみようと思い ますはいではえこれを実行するとですね 今日はえ9月30日なのでもちろん9月 30日偶数日ですね偶数日なので Appleが返ってくるきちんと帰ってき ますねAppleが帰ってきましたでは 昨日の場合はどうなのか 昨日はい昨日の場合は29日なので1日前 ってものをゲットデイトで取得して29日 ってものが書いていますで29日を使って え昨日のおすめ商品オレンジっていうもの を取ることができるというところ ですやとうまく動きました ねでもう少しだけちょっと時間が1時間 ぐらい喋ってしまうような感になるんです\n",
      "Human: ChatGPTが不得意なことを教えて\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "Appleが返ってくるきちんと帰ってき ますねAppleが帰ってきましたでは 昨日の場合はどうなのか 昨日はい昨日の場合は29日なので1日前 ってものをゲットデイトで取得して29日 ってものが書いていますで29日を使って え昨日のおすめ商品オレンジっていうもの を取ることができるというところ ですやとうまく動きました ねでもう少しだけちょっと時間が1時間 ぐらい喋ってしまうような感になるんです けれどもう少しだけお話させてくださいで これだけだともちろんまだまだ連携が 少ないかなっていうところがあってで次に じゃあAppleだったら100円 オレンジだったら200円っていうような ものを商品の値段を返すような関数って いうものを組み込んでみましょうでツール としては元々グビだったらAppleキビ だったらオレンジを返すようなものとあと は日付を取得するツールであとは今回定義 したAppleとオレンジAppleだっ たら100円オレンジだったら200円 っていうのを返すツールっていうものを 定義しますでこれでエージェントのですね おすめの商品の値段まで教えてくださいっ ていうことをやるとそうするとまず最初に え昨日なのでゲットデートでN=1を渡し て1日前のデートを取ってきますで9月 29日ですで次にレコメンドアイテムって いうツールを呼び出してレコメンド アイテムにえ9月29日を渡してで最後に ですねアイテムプライスっていうので アイテムネームをえオレンジにとして渡し てでオレンジはは200円なのでえ昨日の おすめ商品であるオレンジの値段は 200円ですっていう最終的な答えが返っ てきましたというところになりますだいぶ 柔軟なものができるようになってきたって いうことが理解できると思いますで他には ですね最後なんですけれども取得した結果 を吐き出したいとかデータベースに 書き込みたいとかあったりすると思うん ですけどもそういうことももちろん簡単に できますと基本的にはファンクションが あってそのファンクション中身は自分で 書いていくので何でもすることができると いうところですねちょっとあの output.tchっていうところに 書き込むようなものを記事的にですね データベースに書き込んでいるようなもの として紹介しようと思いますで今回は さらに追加するツールとしてはですね セーブデータっていうツールを追加します とセーブデータっていうものは何なのかと いうとここにも説明書いてあるんです けれども日付と商品名と商品の値段を 受け取ってデータベースに書き込みますっ ていうようなものをツールとして作りまし たデータベースといながらもアウトプット テキストにただ書き込むだけのものになる んですけどもデートアイテムネーム アイテムプライスを書き込むだけのツール となりますま実際に自社ツールを使う時に はここにSQL分であったりとかを書いて データベスに書き込むとかをやったりする ことになると思いますこちらもですね ちょっと簡易的にですねフダンティックを 使わずにえドックストリングだけで何を やるかってのを渡して書き込めるかどう かってのをお見せしていこうと思いますで 同様にですね同じように何をしたいかと 言うと今日のおすめ商品の値段を調べて データベースに書き込んでくださいって いうことを実行し ますで最初に動くものとしてはまず日付を 取得するゲットデイトが動いてN=0で 今日の日付を取ります9月30日ですと9 月30日が取れたら9月30日の レコメンドアイテムは何ですかていうので レコメンドアイテムのツールを起動して 書いてきたものがAppleですとであ次 にやることしてはAppleの値段は いくらですかていうものをアイテム プライスに聞いてアイテムプライスが返し てきた答えが100円ですとで100円 っっていうものをえセーブデータに渡して セーブデータに渡すものとしてはデートと アイテムネームとアイテムプライスこれは 日付と商品目と商品の値段を受け取 りっていう風に書いてあるのでこういう ようなものを基数として与えて データベースに書き込みますていうことを この操作でやるということですねなので 細かいことを書いた方がミスは少ないん ですけれどもあんまり書かなくても動いて くれるっていうのがまやっぱりこう チャットGPTのうまいところかなという 風には思いますで最終的に帰ってきたこと としては今日のえおすすめ商品の値段は 100円ですとでデータベースに正常に 書き込まれましたっていうので データベースではないんですけどれも テキストファイルっていうものを中身見て みましょうと今日の日付2023年9月 30日でおすすめの商品Appleで最終 的に100円っていうのが書き込まれた ことが確認できましたとはいとても長く なってしまったんですけれどもこちらで テモは終わりとさせていただきたいと思い ますはい私の発表は以上となります ありがとうございました\n",
      "Human: ChatGPTが不得意なことを教えて\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Given the following extracted parts of a long document and a question, create a final answer. \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "______________________\n",
      "ChatGPT 自体は皆さんご存知のように非常に優れた\n",
      "能力を持っていますが最新のデータに\n",
      "アクセスできないとか特定の数学的な推論\n",
      "が得意でないなどいくつかの課題が存在し\n",
      "ます\n",
      "\n",
      "ChatGPTの不得意な点は、以下のようなものがあります：\n",
      "\n",
      "1. 最新の情報へのアクセス：ChatGPTはリアルタイムで変化する為替や株価などの情報には対応できません。\n",
      "\n",
      "2. 独自の情報源へのアクセス：企業のデータベースなど、特定のプライベートな情報へのアクセスはできません。\n",
      "\n",
      "3. 正確性の課題：ChatGPTは時折簡単な算術計算ですら間違えることがあり、正確な情報や結果が求められるタスクにおいて問題となることがあります。\n",
      "\n",
      "4. データの不足への対応：ファインチューニングによる性能改善は可能ですが、特定のタスクに特化したファインチューニングは他の多様なタスクへの性能低下をもたらす可能性があります。\n",
      "\n",
      "これらの課題を解決するために、ラングチェインエージェントなどの他のツールを組み合わせることが有効です。\n",
      "\n",
      "ChatGPTは以下のような点で苦手とされています：\n",
      "- ファクトチェックや正確性の保証ができない：ChatGPTはトレーニングデータから学習した情報を元に応答を生成するため、正確性や信頼性の保証はされていません。そのため、重要な情報や専門的な知識については、確認や補足の必要があります。\n",
      "- 統一性の欠如：ChatGPTは矛盾する情報や相反する意見を提供することがあります。また、同じ質問に対して異なる回答を返すこともあります。そのため、一貫性のある情報を得るためには、応答の検証や重複した質問の試行が必要です。\n",
      "- センシティブな情報の処理：ChatGPTは個人情報や機密情報の取り扱いには適していません。特に、信用カード番号やパスワードなどの個人識別情報を提供しないように注意する必要があります。\n",
      "- 長文への対応：ChatGPTは一度に生成できるテキストの長さに制限があり、長文や複雑な文章に対しては適切な応答ができないことがあります。短い質問や要約的な回答に向いています。\n",
      "- ユーザーの意図の理解：ChatGPTはユーザーの意図を完全に理解することができない場合があります。そのため、質問や指示を明確にすることが重要です。\n",
      "\n",
      "ChatGPTは以下のような点で苦手とされています：\n",
      "\n",
      "1. 文脈の理解：ChatGPTは単一の入力文を受け取るため、前後の文脈や会話の流れを理解するのが難しい場合があります。そのため、一部の質問や指示に対して適切な回答をすることができないことがあります。\n",
      "\n",
      "2. ファクトチェックの不足：ChatGPTはトレーニングデータから学習した情報に基づいて回答を生成するため、正確な情報を保証することはできません。特に、時事的な情報や最新の研究結果に関しては、確認が必要です。\n",
      "\n",
      "3. ユーザーの意図の理解：ユーザーの意図を正確に理解することができない場合があります。そのため、誤解や不適切な回答をすることがあります。\n",
      "\n",
      "4. 偏った情報：ChatGPTはトレーニングデータに含まれる情報に偏りがある場合があります。そのため、特定のグループやテーマに対してバイアスがかかった回答をすることがあります。\n",
      "\n",
      "これらの制約を考慮しながら、ChatGPTを利用する際は注意が必要です。\n",
      "\n",
      "ChatGPTは、日付や特定の情報に基づいた推論や状況判断が苦手です。また、適切な情報を与えずに実行すると、意図しない結果を返すことがあります。また、複雑なツールや関数の説明を正確に伝えることも難しい場合があります。\n",
      "\n",
      "ChatGPTの不得意な点は、特定の文脈やドメインに関する深い知識がないことです。ChatGPTは大量の一般的な情報を学習していますが、専門的な知識や特定の業界の情報については限定的です。そのため、専門的な質問や特定の業界に関する詳細な情報を提供することは難しい場合があります。また、文脈の把握や推論の能力も限定的なため、一連の質問や長い文章の理解においても制約があります。\n",
      "Human: ChatGPTが不得意なことを教えて\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "CPU times: user 102 ms, sys: 19.9 ms, total: 122 ms\n",
      "Wall time: 3min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qa_output = qa_chain(\n",
    "    {\n",
    "        \"input_documents\": split_youtube,\n",
    "        \"question\": \"ChatGPTが不得意なことを教えて\",\n",
    "    },\n",
    "    return_only_outputs=True,\n",
    ")[\"output_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "faedb445-daa6-4c93-83f3-ce93e0ec5a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPTが不得意とされることは以下の通りです：\n",
      "\n",
      "1. 最新の情報へのアクセス：ChatGPTはリアルタイムで変化する情報には対応できません。例えば、為替や株価のような情報に関しては最新のデータにアクセスすることができません。\n",
      "\n",
      "2. 独自の情報源へのアクセス：ChatGPTは特定のプライベートな情報源や企業のデータベースへのアクセスはできません。\n",
      "\n",
      "3. 正確性の課題：ChatGPTは時折簡単な算術計算ですら間違えることがあり、正確な情報や結果が求められるタスクにおいて問題となることがあります。\n",
      "\n",
      "4. データの不足への対応：ChatGPTはファインチューニングによる性能改善は可能ですが、特定のタスクに特化したファインチューニングは他の多様なタスクへの性能低下をもたらす可能性があります。\n",
      "\n",
      "5. 文脈の理解：ChatGPTは単一の入力文を受け取るため、前後の文脈や会話の流れを理解するのが難しい場合があります。そのため、一部の質問や指示に対して適切な回答をすることができないことがあります。\n",
      "\n",
      "6. ユーザーの意図の理解：ChatGPTはユーザーの意図を完全に理解することができない場合があります。そのため、質問や指示を明確にすることが重要です。\n",
      "\n",
      "7. ファクトチェックや正確性の保証ができない：ChatGPTはトレーニングデータから学習した情報を元に応答を生成するため、正確性や信頼性の保証はされていません。\n",
      "\n",
      "8. 長文への対応：ChatGPTは一度に生成できるテキストの長さに制限があり、長文や複雑な文章に対しては適切な応答ができないことがあります。\n",
      "\n",
      "これらの制約を考慮しながら、ChatGPTを利用する際は注意が必要です。\n"
     ]
    }
   ],
   "source": [
    "print(qa_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
