# 認知ツール実験 最終レポート

## 実験概要

論文「Teaching Language Models to Decode Cognitive Psychology」の認知ツールアプローチをLangGraphで実装し、数学問題解決における効果を検証しました。

## 実装の改善点

### 1. use_codeツールのリファクタリング
- **改善前**: 制限された実行環境により、多くの標準ライブラリが使用不可
- **改善後**: 完全なPython実行環境を提供し、数学計算に必要なライブラリ（math、fractions等）が利用可能に

### 2. 回答抽出ロジックの強化
- 複数の回答パターンに対応（ANSWER:、Final Answer:、Result:等）
- 実行結果からの自動抽出機能
- 推論トレース全体からの回答検索

### 3. システムプロンプトの明確化
- 回答フォーマットの強調
- ツール使用後の明示的な回答要求

## 実験結果

### GPT-4（簡易テスト、3問）
| 手法 | 正答率 | 正解数/問題数 |
|------|--------|---------------|
| 認知ツール | 66.67% | 2/3 |
| ベースライン | 100% | 3/3 |

### GPT-3.5-turbo（中規模テスト、5問）
| 手法 | 正答率 | 正解数/問題数 |
|------|--------|---------------|
| 認知ツール | 0% | 0/5 |
| ベースライン | 80% | 4/5 |

## 分析

### 改善された点
1. **use_codeツールの動作**: リファクタリング後、コード実行が正常に機能
2. **ツール選択**: GPT-4では適切なツール選択が観察された

### 残る課題
1. **回答フォーマット**: LLMが一貫して指定フォーマットで回答しない
2. **ツールチェーンの複雑性**: 複数ツールの連携が期待通りに機能しない
3. **モデル依存性**: GPT-3.5-turboでは認知ツールアプローチが機能しない

## 考察

### 1. 認知ツールアプローチの限界
- **オーバーヘッド**: 簡単な問題では認知ツールが逆効果
- **複雑性**: ツール選択と調整の難しさ
- **モデル要件**: 高度な推論能力を持つモデルが必要

### 2. 実装上の教訓
- **シンプルさの重要性**: 複雑なツールチェーンより単純なアプローチが有効な場合が多い
- **エラーハンドリング**: ツール実行失敗時の適切な対処が重要
- **明確な指示**: LLMへの明確で具体的な指示が不可欠

### 3. 今後の改善案
1. **ツール設計の簡素化**
   - より直感的なツールインターフェース
   - エラー時の自動リトライメカニズム

2. **適応的アプローチ**
   - 問題の複雑さに応じたツール使用の自動判断
   - ベースラインと認知ツールのハイブリッドアプローチ

3. **評価の拡張**
   - より複雑な問題での評価
   - 人間の問題解決プロセスとの比較研究

## 結論

認知ツールアプローチは理論的には興味深いが、現在の実装では以下の理由により実用的でない：

1. **実装の複雑性** vs **得られる利益**のバランスが悪い
2. **モデルの制約**により、意図した動作が実現できない
3. **シンプルなアプローチ**の方が安定して高い性能を示す

しかし、以下の条件下では価値がある可能性がある：
- より高性能なLLMの使用
- 特定の複雑な問題領域への特化
- ツール設計とプロンプトエンジニアリングの更なる最適化

## 技術的詳細

### 使用技術
- LangGraph 0.4.8
- LangChain 0.3.25
- GPT-4 / GPT-3.5-turbo
- Python 3.13.1

### リポジトリ構造
```
langchain/eliciting_reasoning/
├── src/               # 認知ツール実装
├── experiments/       # 実験フレームワーク
├── results/          # 実験結果
└── docs/            # ドキュメント
```

### 実行方法
```bash
# 環境セットアップ
uv venv
source .venv/bin/activate
uv pip install -e .

# 実験実行
python experiments/run_experiment.py --model gpt-4
```