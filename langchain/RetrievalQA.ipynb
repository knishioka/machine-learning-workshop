{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3068168f-7a88-4171-80ca-10d4bbcfd176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028a02a-a196-4c37-ab77-ff7cc1c9bf69",
   "metadata": {},
   "source": [
    "## Embeddingの基本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6421b36-597a-4873-89b4-ccdb4629a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "783bc1f4-65ef-4f90-bd47-d27bff15eb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0016985070060851744,\n",
       " -0.01390356855942869,\n",
       " 0.0016152468895438766,\n",
       " -0.018369816955919344,\n",
       " -0.007179653753743353]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_sample = embeddings.embed_query(\"test\")\n",
    "embedding_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e3d894-f615-4247-8230-e5d2626a01da",
   "metadata": {},
   "source": [
    "embeddingすると、vectorに変換される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f57c7a76-3a86-4bae-8e15-2a2bf0496018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "818faecc-d032-4f59-91cf-856294ad0c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text-embedding-ada-002'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b4a69c-353a-4844-815f-8d71028ade1f",
   "metadata": {},
   "source": [
    "OpenAIのembeddingでは `text-embedding-ada-002` というモデルが使われており、1536次元に変換される。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1612b78-226a-4e7e-894c-6f3ed45a0527",
   "metadata": {},
   "source": [
    "複数の文字列を同時に変換することも可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90eec2bb-da3e-4cfc-872a-0c5b294f24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_embeddings = embeddings.embed_documents([\"Apple\", \"Orange\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e1d5b8d-bfdb-408e-ba97-20adbd3b5200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1536, 1536]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len, multiple_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e56c5-cb15-4e1d-9646-53c52a89826c",
   "metadata": {},
   "source": [
    "## Vector Databaseの基本\n",
    "\n",
    "Vector Databaseの一つであるChromaを使ってみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b06cfdf-bbc7-4595-a655-2b1046593411",
   "metadata": {},
   "source": [
    "### Vector Database用のデータを用意\n",
    "ChatGPTが処理できる文書の長さに限界があるため、短めに文書を分割しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "120288f7-3c2a-410e-a2ae-db8cb9017fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    add_start_index=True   # 分割された位置を保存できる\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bff073b-aa65-4c17-98fe-27b32c04d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "arxiv_no = \"2311.00681\"\n",
    "arxiv_loader = ArxivLoader(arxiv_no)\n",
    "arxiv_doc = arxiv_loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ae013af-c1ef-45f3-b441-ce5fafef7a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arxiv_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a022e9-3792-43d1-8137-67751102259b",
   "metadata": {},
   "source": [
    "一つの論文が34個のDocumentに分けられました。\n",
    "\n",
    "それぞれのdocumentにmetadataが付与されており、このmetadataも一緒にvector databaseに保存されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb4ba75c-5c0a-4017-ac6a-4ceb84d3c0e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2023-11-01',\n",
       " 'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       " 'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       " 'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       " 'start_index': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_doc[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d1b234-1b68-493d-840c-a36dd315ac49",
   "metadata": {},
   "source": [
    "### Vector Databaseに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48ccafc6-3546-44cf-a50d-72039fc9639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"./chroma_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "076e67e3-64c5-4aef-b890-c2472e9bfd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3996e3d1-fbda-424f-a1dd-5fcd5969c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "collection_name = \"chroma_test\"\n",
    "db = Chroma.from_documents(arxiv_doc, embeddings, collection_name=collection_name, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12619b5b-960a-441a-bc01-e3ba50bb0174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chroma.sqlite3                       \u001b[1m\u001b[36me8ea4e0d-fa4c-4b03-b948-c744059b6fb8\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls $persist_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277acf9e-fe02-4f91-af7a-f54e05d2f1b4",
   "metadata": {},
   "source": [
    "persist_directoryに指定したディレクトリにファイルができていることが確認できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe96afa-f57a-4896-a947-3103f3a2e794",
   "metadata": {},
   "source": [
    "オブジェクトの数は、　`_collection.count()` を使って取得可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6a2f303-a830-4dde-b426-d2970f66c296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab2457e-7d83-4924-a030-dd9a8dcee53c",
   "metadata": {},
   "source": [
    "34個のオブジェクトが保存されていることがわかる。\n",
    "これは分割されたDocumentの一緒となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad26160-34f3-4387-b82b-43438e94b2e2",
   "metadata": {},
   "source": [
    "getメソッドに引数を与えなければすべてのドキュメントを取得できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a55b517-8266-462b-9f3a-e87cba559a21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[OneOrMany[ID]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwhere\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Where]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[int]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[int]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwhere_document\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[WhereDocument]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minclude\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[List[str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Dict[str, Any]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Gets the collection.\n",
       "\n",
       "Args:\n",
       "    ids: The ids of the embeddings to get. Optional.\n",
       "    where: A Where type dict used to filter results by.\n",
       "           E.g. `{\"color\" : \"red\", \"price\": 4.20}`. Optional.\n",
       "    limit: The number of documents to return. Optional.\n",
       "    offset: The offset to start returning results from.\n",
       "            Useful for paging results with limit. Optional.\n",
       "    where_document: A WhereDocument type dict used to filter by the documents.\n",
       "                    E.g. `{$contains: \"hello\"}`. Optional.\n",
       "    include: A list of what to include in the results.\n",
       "             Can contain `\"embeddings\"`, `\"metadatas\"`, `\"documents\"`.\n",
       "             Ids are always included.\n",
       "             Defaults to `[\"metadatas\", \"documents\"]`. Optional.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/vectorstores/chroma.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db.get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9afbbb6-4d8e-4813-9cf9-c4c683a808cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['d662276e-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622818-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd662282c-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622840-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622854-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622868-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd662287c-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd662289a-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd66228ae-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd66228c2-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd66228d6-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd66228e0-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd66228f4-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622908-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622912-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622926-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622930-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622944-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd662294e-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622962-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622976-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622980-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622994-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd662299e-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd66229b2-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd66229c6-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd66229d0-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd66229e4-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd66229ee-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622a02-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622a0c-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622a20-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622a2a-88fe-11ee-8738-66a9796eac2f',\n",
       "  'd6622a3e-88fe-11ee-8738-66a9796eac2f'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 0},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 973},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 1948},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 2948},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 3911},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 4905},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 5863},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 6844},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 7835},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 8813},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 9796},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 10781},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 11778},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 12756},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 13741},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 14723},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 15696},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 16675},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 17672},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 18643},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 19618},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 20581},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 21577},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 22534},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 23513},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 24489},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 25451},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 26447},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 27439},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 28417},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 29369},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 30353},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 31307},\n",
       "  {'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 32287}],\n",
       " 'documents': ['Are Large Language Models Reliable Judges?\\nA Study on the Factuality Evaluation Capabilities of LLMs\\nXue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN\\nDialpad Canada Inc.\\n{xue-yong,tahmid.rahman,cchen,sbhushan}@dialpad.com\\nAbstract\\nIn recent years, Large Language Models\\n(LLMs) have drawn significant attention due\\nto their impressive emergent capabilities that\\nwere not observed in earlier language models.\\nOne emerging area where LLMs have been\\nwidely used in recent times is the utilization\\nof LLMs as the evaluator of the texts gener-\\nated by various generative models. In this pa-\\nper, we also explore the possibility of whether\\nLLMs are reliable in assessing the factual con-\\nsistency of summaries generated by text gen-\\neration models. We first propose a new ap-\\nproach to evaluate the factuality score using\\nLLMs by utilizing one single LLM to perform\\nall steps in the question-answering-based fac-\\ntuality scoring pipeline. Subsequently, we also',\n",
       "  'study the performance of various LLMs to di-\\nrectly score the factuality. Our evaluation is\\nconducted in traditional benchmarks by com-\\nparing their correlation with human annotations.\\nContrary to expectations, our findings reveal\\nthat none of the factuality metrics showed any\\nsignificant correlations (e.g., coefficient scores\\ngreater than 0.3) to human evaluations of fac-\\ntuality for GPT-4 and PaLM-2, with the only\\nexception being GPT-3.5 in two subcategories\\nof factuality. Nonetheless, our findings are con-\\nsistent across almost all factual error types, sug-\\ngesting a fundamental limitation in the ability\\nof current LLMs to assess factuality.\\n1\\nIntroduction\\nText summarization has significantly advanced\\nthrough the utilization of pre-trained language mod-\\nels (Devlin et al., 2018; Liu and Lapata, 2019;\\nLewis et al., 2020; Raffel et al., 2020; Zhang et al.,\\n2020; Laskar et al., 2022c). However, a persistent\\nconcern with current models is their frequent inabil-',\n",
       "  'ity to maintain factual consistency with the origi-\\nnal documents they intend to summarize (Maynez\\net al., 2020; Fabbri et al., 2021a). Consequently,\\nestablishing the factual accuracy of a summary con-\\ntinues to be the key for the evaluation of summa-\\nrization models (Fabbri et al., 2021b, 2022). To\\nresolve this issue, recent studies have utilized tech-\\nniques like natural language inference, question-\\nanswering, or syntactic dependency as factuality\\nevaluation metrics (Honovich et al., 2022). How-\\never, as highlighted by Pagnoni et al. (2021), none\\nof these automatic factuality metrics demonstrate a\\nconsiderable correlation (i.e., fails to achieve a cor-\\nrelation score above 0.3) with human evaluations,\\npointing to the limited efficacy of these measures.\\nThe emergence and subsequent advancements\\nof LLMs, such as ChatGPT1, have transformed the\\nlandscape of natural language processing (NLP).\\nChatGPT-like LLMs (Google, 2023; Touvron et al.,\\n2023b; OpenAI, 2023) have displayed impressive',\n",
       "  'progress across a broad spectrum of NLP tasks,\\nfrom text classification to generation, language\\ntranslation, and beyond (Laskar et al., 2023a,c).\\nGiven the capabilities of these LLMs, our re-\\nsearch explores the possibility of utilizing LLMs\\nfor the critical task of factual consistency evalua-\\ntion (Dubois et al., 2023; Liu et al., 2023b; Manakul\\net al., 2023; Tang et al., 2022; Laban et al., 2023).\\nTo assess the factual consistency of a model, one\\ncommon approach is the utilization of a question-\\nanswering (QA) pipeline (Huang et al., 2021). Tra-\\nditionally, the evaluation of factuality using QA\\nsystems has involved the use of separate, distinct\\nmodels for each of the following tasks: answer se-\\nlection, question generation, and question answer-\\ning (Huang et al., 2021). However, this approach\\ninvolves the intricate task of coordinating between\\nthese disparate models, potentially resulting in in-\\nefficiencies in real-world scenarios. Additionally,',\n",
       "  'these models may fail to capture the comprehensive\\ncontext necessary for optimal factuality evaluation.\\nIn response to these challenges, we propose a novel\\napproach that substitutes the separate models with\\na singular and unified model using LLMs. In ad-\\ndition, we explore another approach where LLMs\\n1https://openai.com/blog/chatgpt\\narXiv:2311.00681v1  [cs.CL]  1 Nov 2023\\nPrompt: QA-based Factuality Metric via LLMs\\nPrompt: LLM-based Factuality Scoring\\n# Answer Selection and Question Generation:\\nFrom the following text, generate a question that can be\\nanswered within 1 or 2 words and also generate an answer that\\nis either a noun phrase/named entity.\\nText: Tom went to a baseball game tonight.\\nOutput:\\n{\\n“question\": \"When did Tom go to a baseball game?\",\\n“answer\": \"Tonight\"\\n}\\nText: [SUMMARY]\\nOutput:\\n# Question Answering:\\nAnswer the following question based on the given context.\\nQuestion: [LLM Generated Question]\\nContext: [ARTICLE]\\nEvaluate the quality of summaries written for a news',\n",
       "  'article. Rate each summary on faithfulness. You should\\nrate on a scale from 1 (worst) to 5 (best) without any\\nexplanation.\\nArticle:\\nTom woke up at 7 AM and he went to\\nschool with his sister right away.\\nSummary: Tom went to school with his sister.\\nFaithfulness: 5\\nArticle: [ARTICLE]\\nSummary: [SUMMARY]\\nfaithfulness:\\nTable 1: Prompts for LLMs as QA-based Factuality Evaluator and LLMs as Direct Faithfulness Scorer. In the\\nQA-based factuality evaluator, the faithfulness score is measured based on the similarity between the initially\\nselected answer (i.e., generated from the Answer Selection and Question Generation step) and the final answer (i.e.,\\nthe answer generated from the Question Answering step)\\nwere directly asked to assess the factuality of a\\ngiven summary. Meanwhile, we also address the\\npotential risk of inaccurate high correlation mea-\\nsures (Pagnoni et al., 2021) by considering partial\\ncorrelations, which are adept at controlling for con-',\n",
       "  'founding variables. In sum, this paper investigates\\nthe following Research Questions (RQ):\\nRQ 1: Can the QA-based factuality metric be\\nimproved by utilizing LLMs?\\nRQ 2: Can LLMs directly generate reliable faith-\\nfulness scores?\\n2\\nRelated Work\\nWhile neural abstractive summarization models\\ncan produce fluent summaries, they often gener-\\nate factual inconsistencies (Honovich et al., 2022).\\nIn the early years of factual consistency evalua-\\ntion, various unsupervised and weakly-supervised\\nmetrics have been used, which include relational\\ntriple-based, textual-entailment-based, as well as\\nQA-based techniques (Huang et al., 2021). Al-\\nthough the QA-based approach is a widely used\\ntechnique for factuality evaluation, it requires sep-\\narate models to perform different steps, such as\\nquestion generation, answer selection, and finally,\\nquestion answering. This makes the QA-based ap-\\nproach quite complicated and inefficient. In this\\nregard, we study whether only one distinct LLM',\n",
       "  'can be used to perform all steps in the QA-based\\nfactuality metric pipeline. Consequently, we also\\nstudy whether LLMs can be directly used to predict\\nthe faithfulness score of the generated summary for\\na given article.\\nMeanwhile, one major limitation in factuality\\nevaluation is the lack of common benchmarks.\\nThis makes the comparison of various factuality\\nmetrics quite difficult. To address this issue, var-\\nious benchmarks have been introduced recently\\nfor factual consistency evaluation, such as Sum-\\nmEval (Fabbri et al., 2021a) and FRANK (Pagnoni\\net al., 2021). These benchmarks are designed to\\nevaluate various metrics on their ability to cap-\\nture factual errors in abstractive summarization.\\nAmong the available benchmarks, the FRANK\\nbenchmark is the largest one consisting of human-\\nannotated factuality scores of summaries from di-\\nverse datasets. More specifically, it is a compila-\\ntion of two datasets, CNN-DM (Nallapati et al.,\\n2016) and XSUM (Narayan et al., 2018), amalga-',\n",
       "  'mating outputs from nine distinct models across\\nthese datasets (5 models for CNN-DM and 4 mod-\\nels for XSUM). In total, the dataset comprises 2250\\nhuman-annotated judgments on different types of\\nfactual errors of model outputs. In addition, this\\nbenchmark addresses the false measurement of\\nhigh correlations in various factuality metrics by\\nintroducing the partial correlation coefficients.\\nIn this paper, we also utilize the FRANK bench-\\nmark to evaluate the factual consistency of model-\\ngenerated summaries by leveraging LLMs as the\\nevaluator. Our paper diverges from that of Gao\\net al. (2023) in several key aspects. Notably, our\\nresearch employs the FRANK dataset, encompass-\\ning the CNN-DM and XSUM datasets. In con-\\ntrast, Gao et al. (2023) base their findings on the\\nSummEval and Newsroom datasets. Additionally,\\nour study presents results using partial correlation\\nas opposed to the straightforward correlation em-\\nployed by Gao et al. (2023). This metric is adept at',\n",
       "  'controlling for confounding variables, potentially\\nmitigating the risk of inaccurate high correlation\\nmeasures (Pagnoni et al., 2021).\\n3\\nMethodology\\nIn this section, we present our methods: (i) Using\\nLLMs as QA-based factuality metric, and (ii) Using\\nLLMs for direct factuality scoring. Below, we first\\npresent these methods.\\n(i) QA-based Factuality Metric via LLMs:\\nThe reason we chose to incorporate LLMs into\\nthe QA-based factuality metric is that it is more re-\\nliable than most other existing automatic factuality\\nmetrics for assessing the factual consistency of a\\nmodel (Huang et al., 2021). The typical process of\\nusing QA-based systems as factuality evaluators is\\ncomprised of 3 tasks:\\n(i) Answer Selection: The commencement of\\nthis procedure involves extracting key points, re-\\nferred to as “answers” from the provided summary.\\n(ii) Question Generation: After identifying the\\nanswers, the next step is to formulate questions\\nbased on these answers, using the summary as the\\ncontext.',\n",
       "  'context.\\n(iii) Question Answering: The final step is re-\\nsponding to the generated questions using the input\\ndocument as a reference.\\nIn this paper, contrary to the traditional approach\\nof utilizing separate models to perform each task\\nthat makes the QA-based factuality evaluation pro-\\ncess very complicated, we propose one single LLM\\nto be used as the QA-based factuality metric evalu-\\nator to perform all steps. For prompt construction,\\nwe first evaluate various prompts in some samples\\nand then select the one for our experiment that per-\\nforms the best. We show our selected prompt for\\nthis task that we use in our experiments in Table 1.\\nIn our prompt, we leverage the in-context learn-\\ning principle and provide an associated example\\nwith our prompt to the LLMs to perform the first\\ntwo tasks: initial answer selection and question\\ngeneration. Since both the initial answer and the\\nquestions are required to be generated from the\\ngiven summary (making both the question and the',\n",
       "  'answer to have some dependencies between them),\\nwe unify these two steps together by asking the\\nLLM to generate both the answer and the ques-\\ntion simultaneously from the given summary. This\\nmakes the first two steps of the QA-based pipeline\\nto be more efficient. Afterward, the generated ques-\\ntion and the article are given as input to the LLM to\\ngenerate the final answer. The evaluation process of\\nthe QA-based factuality metric depends on finding\\nthe similarity between the initially selected answer\\nand the final answer. The higher the similarity, the\\nmore faithful the summary is being considered.\\n(ii) Direct Faithfulness Scoring via LLMs:\\nSimilar to how we constructed prompts for the QA-\\nbased factuality metric evaluation, we first eval-\\nuate various prompts in a set of samples and se-\\nlect the one for full experiments that performs the\\nbest. With in-context example demonstrations, we\\nprompt the target LLM to assess a provided sum-\\nmary based on faithfulness on a scale from 1 to 5',\n",
       "  '(our prompt is shown in Table 1).\\n4\\nExperiments\\nIn this section, we first present the LLMs that we\\nstudy in this paper, followed by defining the evalu-\\nation metrics and finally the experimental results.\\n4.1\\nModels\\nWe use the following LLMs for evaluation.\\nGPT-3.5: GPT-3.5, also known as ChatGPT, is\\na transformer-based (Vaswani et al., 2017) auto-\\nregressive model developed by OpenAI that was\\npre-trained on a vast amount of textual data via su-\\npervised learning alongside reinforcement learning\\nwith human feedback. We use the gpt3.5-turbo-\\n0613 version of this model via OpenAI2.\\nGPT-4: GPT-4 (OpenAI, 2023) is the latest ad-\\ndition to the GPT series models by OpenAI that is\\ntouted as being more reliable, creative, and able to\\nhandle much more nuanced instructions than GPT-\\n3.5. However, GPT-4 is about 25x more costly than\\nGPT-3.5 while being significantly slower. We use\\nthe gpt4-0613 version of this model via OpenAI.\\nPaLM-2: It is also a transformer-based language',\n",
       "  'model proposed by Google that exhibits enhanced\\nreasoning capabilities and improved computing ef-\\nficiency. We use the text-bison@001 version of this\\nmodel through Google’s Vertex API3.\\n2https://platform.openai.com/docs/models\\n3https://cloud.google.com/vertex-ai/docs/\\ngenerative-ai/model-reference/text\\nPearson ρ\\nPearson p-value\\nSpearman r\\nSpearman p-value\\nMetric\\nPaLM-2\\nGPT-3.5\\nGPT-4\\nPaLM-2\\nGPT-3.5\\nGPT-4\\nPaLM-2\\nGPT-3.5\\nGPT-4\\nPaLM-2\\nGPT-3.5\\nGPT-4\\nFactuality Errors\\n-0.0409\\n-0.0016\\n-0.0014\\n0.1050\\n0.9498\\n0.9561\\n-0.0632\\n-0.0259\\n0.0084\\n0.0121\\n0.3037\\n0.7390\\nSemantic Frame Errors\\n-0.0416\\n-0.0533\\n-0.0386\\n0.0985\\n0.0343\\n0.1260\\n-0.0005\\n-0.0752\\n-0.0494\\n0.9845\\n0.0028\\n0.0501\\nPredE\\n-0.0057\\n-0.0145\\n-0.0044\\n0.8220\\n0.5650\\n0.8622\\n0.0928\\n-0.0434\\n-0.0290\\n0.0002\\n0.0848\\n0.2497\\nEntE\\n-0.0211\\n-0.0044\\n-0.0212\\n0.4027\\n0.8617\\n0.4006\\n0.0645\\n-0.0401\\n-0.0327\\n0.0105\\n0.1117\\n0.1941\\nCircE\\n-0.0307\\n-0.0496\\n-0.0444\\n0.2240\\n0.0491\\n0.0782\\n0.1044\\n-0.0915\\n-0.0419\\n0.0000\\n0.0003\\n0.0961\\nDiscourse Errors\\n-0.0177\\n-0.0184\\n-0.0185\\n0.4820',\n",
       "  '-0.0185\\n0.4820\\n0.4649\\n0.4633\\n-0.1073\\n0.0289\\n0.0065\\n0.0000\\n0.2522\\n0.7962\\nCorefE\\n-0.0174\\n-0.0222\\n-0.0158\\n0.4897\\n0.3790\\n0.5306\\n-0.0857\\n0.0158\\n0.0136\\n0.0007\\n0.5314\\n0.5890\\nLinkE\\n-0.0057\\n0.0019\\n-0.0173\\n0.8210\\n0.9385\\n0.4938\\n0.1424\\n-0.0640\\n-0.0567\\n0.0000\\n0.0110\\n0.0245\\nContent Verifiability Errors\\n0.0185\\n0.0692\\n0.0335\\n0.4621\\n0.0060\\n0.1844\\n0.0011\\n0.0846\\n0.0359\\n0.9647\\n0.0008\\n0.1545\\nOutE\\n0.0302\\n0.0570\\n0.0472\\n0.2314\\n0.0237\\n0.0610\\n0.0212\\n0.0375\\n0.0300\\n0.3999\\n0.1373\\n0.2347\\nGramE\\n-0.0187\\n0.0128\\n-0.0297\\n0.4590\\n0.6130\\n0.2395\\n0.1103\\n-0.0641\\n-0.0397\\n0.0000\\n0.0110\\n0.1157\\nTable 2: Correlation scores for different LLMs as QA-based Factuality Metric Evaluator.\\nPearson ρ\\nPearson p-value\\nSpearman r\\nSpearman p-value\\nMetric\\nPaLM-2\\nGPT-3.5\\nGPT-4\\nPaLM-2\\nGPT-3.5\\nGPT-4\\nPaLM-2\\nGPT-3.5\\nGPT-4\\nPaLM-2\\nGPT-3.5\\nGPT-4\\nFactuality Errors\\n-0.0898\\n0.0246\\n0.0915\\n0.0004\\n0.3302\\n0.0003\\n-0.0921\\n-0.0073\\n0.0579\\n0.0003\\n0.7737\\n0.0217\\nSemantic Frame Errors\\n-0.0787\\n0.0111\\n0.0206\\n0.0018\\n0.6590\\n0.4139\\n-0.0826\\n0.0980\\n0.0118\\n0.0010\\n0.0001',\n",
       "  '0.0010\\n0.0001\\n0.6384\\nPredE\\n-0.0465\\n0.0172\\n-0.0266\\n0.0651\\n0.4945\\n0.2917\\n-0.0108\\n0.3337\\n-0.0265\\n0.6687\\n0.0000\\n0.2934\\nEntE\\n-0.0641\\n0.0113\\n-0.0177\\n0.0109\\n0.6554\\n0.4817\\n-0.0569\\n0.1801\\n-0.0243\\n0.0240\\n0.0000\\n0.3356\\nCircE\\n-0.0663\\n0.0266\\n0.0004\\n0.0084\\n0.2909\\n0.9884\\n-0.0503\\n0.3702\\n-0.0246\\n0.0459\\n0.0000\\n0.3288\\nDiscourse Errors\\n-0.0641\\n0.0178\\n-0.0376\\n0.0110\\n0.4806\\n0.1355\\n-0.0484\\n-0.2273\\n-0.0332\\n0.0546\\n0.0000\\n0.1879\\nCorefE\\n-0.0632\\n0.0165\\n-0.0345\\n0.0121\\n0.5131\\n0.1712\\n-0.0519\\n-0.2700\\n-0.0215\\n0.0394\\n0.0000\\n0.3947\\nLinkE\\n-0.0520\\n0.0257\\n-0.0440\\n0.0390\\n0.3086\\n0.0805\\n-0.0219\\n0.2827\\n-0.0499\\n0.3849\\n0.0000\\n0.0477\\nContent Verifiability Errors\\n-0.0147\\n0.0316\\n0.0184\\n0.5612\\n0.2107\\n0.4662\\n-0.0071\\n0.0148\\n0.0190\\n0.7784\\n0.5568\\n0.4510\\nOutE\\n-0.0131\\n0.0267\\n0.0468\\n0.6033\\n0.2891\\n0.0633\\n-0.0052\\n-0.0447\\n0.0483\\n0.8357\\n0.0761\\n0.0551\\nGramE\\n-0.0497\\n0.0285\\n-0.0716\\n0.0488\\n0.2575\\n0.0045\\n-0.0298\\n0.2893\\n-0.0874\\n0.2377\\n0.0000\\n0.0005\\nTable 3: Correlation scores for different LLMs as Faithfulness Scorer.\\n4.2\\nEvaluation Metrics',\n",
       "  'Evaluation Metrics\\nWhile previous studies, such as Gao et al. (2023),\\nhave indicated the potential of automatic metrics in\\nassessing factuality, not accounting for confound-\\ning variables associated with system and dataset\\nproperties in some contexts might influence the\\nperceived correlations Pagnoni et al. (2021). In\\ncontrast, our experiment addresses this concern\\nby incorporating partial correlation coefficients,\\nleveraging the FRANK benchmark (Pagnoni et al.,\\n2021). The FRANK benchmark not only contains\\ndata from diverse datasets but also features a com-\\nprehensive typology of factual errors, allowing for\\na more nuanced understanding of the inaccuracies\\nin generated summaries. As given in the FRANK\\nbenchmark, we measure partial correlation in terms\\nof the following:\\n1. Factuality Errors: This is the overall factual-\\nity error.\\n2. Semantic Frame Errors: Errors that occur\\ndue to the incorrect understanding of the re-\\nlationships and roles in a situation or event.',\n",
       "  'Example: Predicate Errors, Entity Errors and\\nCircumstance Errors.\\n• Predicate Errors (PredE): Incorrect or\\nmisrepresented predicates in summaries.\\n• Entity Errors (EntE): Wrong entities\\nmentioned.\\n• Circumstance Errors (CircE): Inaccu-\\nrate details regarding the circumstances\\nof an event.\\n3. Discourse Errors: It refers to incorrect links\\nbetween different parts of a summarized text.\\nExample: Coreference Errors and Discourse\\nLink Errors.\\n• Coreference Errors (CorefE): Refers\\nto incorrect references (e.g., pronoun).\\n• Discourse Link Errors (LinkE): Errors\\nin connecting statements logically within\\na discourse.\\n4. Content Verifiability Errors: These errors\\narise when the summaries cannot be verified\\nfor accuracy due to a lack of supporting ev-\\nidence. Example: Out of Article Errors and\\nGrammatical Errors.\\n• Out of Article Errors (OutE): State-\\nments containing information not present\\nin the referenced source.\\n• Grammatical Errors (GramE): Gram-\\nmatical mistakes that make sentences fac-',\n",
       "  'tually incorrect.\\n4.3\\nResults and Discussion\\nFor the QA-based factuality, the common metrics\\nused to measure the correlation include the Exact\\nMatch and the word F1 scores. However, the Exact\\nMatch could be excessively stringent. Thus, we\\nopt for the word F1 which offers a more balanced\\nevaluation for answer overlap.\\n(i) LLM as QA-based Factuality Metrics:\\nWe\\nshow our results for the QA-based factuality eval-\\nuation in Table 2. For overall factuality (referred\\nto as “Factuality Errors\"), only PaLM-2 displays\\na statistically significant p-value of 0.0121 for the\\nSpearman partial correlation. This indicates that\\nthere is no linear correlation between human judg-\\nment and the LLM-QA score, as the correlation\\ncoefficient is −0.0632. For the majority of factu-\\nality error subcategories, PaLM-2, GPT-3.5 and\\nGPT-4 do not have statistically significant p-values\\nfor the Pearson correlation coefficient. However,\\nthe correlation values for all are very close to zero,',\n",
       "  'which indicates no linear correlation between hu-\\nman judgment and the LLM-QA score even for the\\nsubcategories. In terms of the Spearman correlation\\ncoefficient that is capable of detecting non-linear\\nrelationships, PaLM-2 exhibits a statistically sig-\\nnificant but very weak correlation (greater than 0.1\\nbut less than 0.3) with human judgment in Dis-\\ncourse Errors, CircE, GramE, and LinkE, where\\nthe absolute value exceeds 0.1.\\n(ii) LLM as Direct Faithfulness Scorer:\\nTa-\\nble 3 shows the correlation coefficients calculated\\nbetween the factuality scores assigned by LLMs\\nand the scores corresponding to different types of\\nhuman-annotated errors. In terms of error subcat-\\negories, we see PaLM-2 doesn’t show any corre-\\nlation with high p-values and close to zero coeffi-\\ncients. Both GPT-3.5 and GPT-4 also do not have\\nany significant Pearson correlation scores. But in-\\nterestingly GPT-3.5 shows statistically significant\\nSpearman correlation scores for Discourse Errors',\n",
       "  '(−0.2273), PredE (0.3337), EntE (0.1801), CircE\\n(0.3702), GramE (0.2893), CorefE (−0.27) and\\nLinkE (0.2827). The observed negative correlation\\nis worrisome, as it could suggest inherent issues\\nwith the model’s reliability as a faithfulness scorer.\\n5\\nConclusion\\nThe central objective of this research was to as-\\nsess the effectiveness of various LLMs, specifically\\nGPT-3.5, GPT-4, and PaLM-2 in the evaluation of\\nfactuality in text summarization tasks. In addition\\nto directly using LLMs to evaluate the factuality\\nof a summary, we also introduce a novel approach\\nthat utilizes one single LLM to perform various\\nsteps of the QA-based factuality scoring pipeline.\\nContrary to expectations, our findings revealed that\\nnone of the approaches showed a significant corre-\\nlation (with a coefficient greater than 3) to human\\nevaluations of factuality for most LLMs, with the\\nonly exception happening while directly generat-\\ning the LLM faithfulness scores by GPT-3.5 in',\n",
       "  'two subcategories of factuality: PredE and CircE.\\nNonetheless, the result is consistent across almost\\nall factual error types, suggesting a fundamental\\nlimitation in the ability of current LLMs to effec-\\ntively assess factuality.\\nWhile previous studies, such as Gao et al. (2023),\\nindicated the potential of automatic metrics in as-\\nsessing factuality, our findings suggest that it is es-\\nsential to consider possible dataset biases Pagnoni\\net al. (2021). In some contexts, not accounting for\\nconfounding variables associated with system and\\nthe dataset properties might influence the perceived\\ncorrelations. To provide a more nuanced perspec-\\ntive, we recommend utilizing partial correlation co-\\nefficients to control for these variables. Our study\\ncalls for an exploration into the inherent deficien-\\ncies of current language models in maintaining\\nfactual consistency and sheds light on the necessity\\nfor developing more accurate and comprehensive\\nmodels and methods for factuality evaluation.',\n",
       "  'In the future, we will study the factuality evalua-\\ntion capabilities of LLMs using other benchmarks\\n(Laban et al., 2022; Wang et al., 2023), as well as\\non noisy datasets (Fu et al., 2022; Khasanova et al.,\\n2022; Laskar et al., 2022a,b, 2023b; Manderscheid\\nand Lee, 2023), alongside investigating new ap-\\nproaches, such as the utilization of few-shot learn-\\ning (Brown et al., 2020), other prompting strategies\\n(Liu et al., 2023a), and whether fine-tuning open-\\nsource LLMs (Touvron et al., 2023a,b; Zhao et al.,\\n2023) for factuality evaluation leads to a better fac-\\ntuality evaluator.\\nLimitations\\nThe closed-source models that have been used in\\nthis paper are continuously updated. This may lead\\nto the potential deprecation or unavailability of the\\nolder versions of the models with the release of\\nnewer versions. Thus, there might be some varia-\\ntions in the results while replicating our study.\\nReferences\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie',\n",
       "  'Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems, 33:1877–1901.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2018. Bert: Pre-training of deep\\nbidirectional transformers for language understand-\\ning. Proceedings of the Annual Conference of the\\nNorth American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technolo-\\ngies, 4171-4186.\\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\\nIshaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\\nLiang, and Tatsunori B Hashimoto. 2023.\\nAl-\\npacafarm: A simulation framework for methods\\nthat learn from human feedback.\\narXiv preprint\\narXiv:2305.14387.\\nAlexander R Fabbri, Prafulla Kumar Choubey, Jesse\\nVig, Chien-Sheng Wu, and Caiming Xiong. 2022.\\nImproving factual consistency in summarization with\\ncompression-based post-editing.\\narXiv preprint',\n",
       "  'arXiv preprint\\narXiv:2211.06196.\\nAlexander R Fabbri, Wojciech Kry´sci´nski, Bryan Mc-\\nCann, Caiming Xiong, Richard Socher, and Dragomir\\nRadev. 2021a. Summeval: Re-evaluating summariza-\\ntion evaluation. Transactions of the Association for\\nComputational Linguistics, 9:391–409.\\nAlexander R Fabbri, Chien-Sheng Wu, Wenhao Liu,\\nand Caiming Xiong. 2021b. Qafacteval: Improved\\nqa-based factual consistency evaluation for summa-\\nrization. arXiv preprint arXiv:2112.08542.\\nXue-yong Fu, Cheng Chen, Md Tahmid Rahman\\nLaskar, Shayna Gardiner, Pooja Hiranandani, and\\nShashi Bhushan Tn. 2022. Entity-level sentiment\\nanalysis in contact center telephone conversations.\\nIn Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing: Industry\\nTrack, pages 484–491, Abu Dhabi, UAE. Association\\nfor Computational Linguistics.\\nMingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Ship-\\ning Yang, and Xiaojun Wan. 2023. Human-like sum-\\nmarization evaluation with chatgpt.',\n",
       "  'Google. 2023. Palm 2 technical report. Goole AI.\\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\\nScialom, Idan Szpektor, Avinatan Hassidim, and\\nYossi Matias. 2022.\\nTrue: Re-evaluating factual\\nconsistency evaluation. In Proceedings of the 2022\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, pages 3905–3920.\\nYichong Huang, Xiachong Feng, Xiaocheng Feng, and\\nBing Qin. 2021. The factual inconsistency problem\\nin abstractive text summarization: A survey. arXiv\\npreprint arXiv:2104.14839.\\nElena Khasanova, Pooja Hiranandani, Shayna Gardiner,\\nCheng Chen, Simon Corston-Oliver, and Xue-Yong\\nFu. 2022. Developing a production system for Pur-\\npose of Call detection in business phone conversa-\\ntions. In Proceedings of the 2022 Conference of the\\nNorth American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technolo-',\n",
       "  'gies: Industry Track, pages 259–267, Hybrid: Seattle,\\nWashington + Online. Association for Computational\\nLinguistics.\\nPhilippe Laban, Wojciech Kry´sci´nski, Divyansh Agar-\\nwal, Alexander R Fabbri, Caiming Xiong, Shafiq\\nJoty, and Chien-Sheng Wu. 2023. Llms as factual\\nreasoners: Insights from existing benchmarks and\\nbeyond. arXiv preprint arXiv:2305.14540.\\nPhilippe Laban, Tobias Schnabel, Paul N Bennett, and\\nMarti A Hearst. 2022.\\nSummac: Re-visiting nli-\\nbased models for inconsistency detection in summa-\\nrization. Transactions of the Association for Compu-\\ntational Linguistics, 10:163–177.\\nMd Tahmid Rahman Laskar, M Saiful Bari, Mizanur\\nRahman, Md Amran Hossen Bhuiyan, Shafiq Joty,\\nand Jimmy Huang. 2023a. A systematic study and\\ncomprehensive evaluation of ChatGPT on benchmark\\ndatasets. In Findings of the Association for Com-\\nputational Linguistics: ACL 2023, pages 431–469,\\nToronto, Canada. Association for Computational Lin-\\nguistics.\\nMd Tahmid Rahman Laskar, Cheng Chen, Xue-yong Fu,',\n",
       "  'Mahsa Azizi, Shashi Bhushan, and Simon Corston-\\noliver. 2023b. AI coach assist: An automated ap-\\nproach for call recommendation in contact centers\\nfor agent coaching. In Proceedings of the 61st An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 5: Industry Track), pages 599–\\n607, Toronto, Canada. Association for Computational\\nLinguistics.\\nMd Tahmid Rahman Laskar, Cheng Chen, Jonathan\\nJohnston, Xue-Yong Fu, Shashi Bhushan TN, and Si-\\nmon Corston-Oliver. 2022a. An auto encoder-based\\ndimensionality reduction technique for efficient en-\\ntity linking in business phone conversations. In Pro-\\nceedings of the 45th International ACM SIGIR Con-\\nference on Research and Development in Information\\nRetrieval, pages 3363–3367.\\nMd Tahmid Rahman Laskar, Cheng Chen, Aliak-\\nsandr Martsinovich, Jonathan Johnston, Xue-Yong\\nFu, Shashi Bhushan Tn, and Simon Corston-Oliver.\\n2022b. BLINK with Elasticsearch for efficient entity\\nlinking in business conversations. In Proceedings of',\n",
       "  'the 2022 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Hu-\\nman Language Technologies: Industry Track, pages\\n344–352, Hybrid: Seattle, Washington + Online. As-\\nsociation for Computational Linguistics.\\nMd Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen,\\nand Shashi Bhushan TN. 2023c. Building real-world\\nmeeting summarization systems using large language\\nmodels: A practical perspective.\\narXiv preprint\\narXiv:2310.19233.\\nMd Tahmid Rahman Laskar, Enamul Hoque, and\\nJimmy Xiangji Huang. 2022c. Domain adaptation\\nwith pre-trained transformers for query-focused ab-\\nstractive text summarization. Computational Linguis-\\ntics, 48(2):279–320.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\\nDenoising sequence-to-sequence pre-training for nat-\\nural language generation, translation, and comprehen-\\nsion. In Proceedings of the 58th Annual Meeting of',\n",
       "  'the Association for Computational Linguistics, pages\\n7871–7880.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\\nHiroaki Hayashi, and Graham Neubig. 2023a. Pre-\\ntrain, prompt, and predict: A systematic survey of\\nprompting methods in natural language processing.\\nACM Computing Surveys, 55(9):1–35.\\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\\nRuochen Xu, and Chenguang Zhu. 2023b. Gpte-\\nval: Nlg evaluation using gpt-4 with better human\\nalignment. arXiv preprint arXiv:2303.16634.\\nYang Liu and Mirella Lapata. 2019.\\nText summa-\\nrization with pretrained encoders. arXiv preprint\\narXiv:1908.08345.\\nPotsawee Manakul, Adian Liusie, and Mark JF Gales.\\n2023. Selfcheckgpt: Zero-resource black-box hal-\\nlucination detection for generative large language\\nmodels. arXiv preprint arXiv:2303.08896.\\nEtienne Manderscheid and Matthias Lee. 2023. Predict-\\ning customer satisfaction with soft labels for ordinal\\nclassification. In Proceedings of the 61st Annual',\n",
       "  'Meeting of the Association for Computational Lin-\\nguistics (Volume 5: Industry Track), pages 652–659,\\nToronto, Canada. Association for Computational Lin-\\nguistics.\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\\nRyan McDonald. 2020. On faithfulness and factu-\\nality in abstractive summarization. In Proceedings\\nof the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 1906–1919.\\nRamesh Nallapati, Bowen Zhou, Cícero Nogueira dos\\nSantos, Çaglar Gülçehre, and Bing Xiang. 2016.\\nAbstractive text summarization using sequence-to-\\nsequence rnns and beyond. In Proceedings of the\\n20th SIGNLL Conference on Computational Natural\\nLanguage Learning, CoNLL 2016, Berlin, Germany,\\nAugust 11-12, 2016, pages 280–290. ACL.\\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\\n2018. Don’t give me the details, just the summary!\\ntopic-aware convolutional neural networks for ex-\\ntreme summarization. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Lan-',\n",
       "  'guage Processing, pages 1797–1807.\\nOpenAI. 2023. Gpt-4 technical report.\\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\\nTsvetkov. 2021. Understanding factuality in abstrac-\\ntive summarization with frank: A benchmark for\\nfactuality metrics. In Proceedings of the 2021 Con-\\nference of the North American Chapter of the Asso-\\nciation for Computational Linguistics: Human Lan-\\nguage Technologies, pages 4812–4829.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer. J. Mach. Learn. Res., 21(140):1–67.\\nLiyan Tang, Tanya Goyal, Alexander R Fabbri, Philippe\\nLaban, Jiacheng Xu, Semih Yavuz, Wojciech Kry´s-\\nci´nski, Justin F Rousseau, and Greg Durrett. 2022.\\nUnderstanding factual errors in summarization: Er-\\nrors, summarizers, datasets, error detectors. arXiv\\npreprint arXiv:2205.12854.',\n",
       "  'Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, et al. 2023a.\\nLlama:\\nOpen and effi-\\ncient foundation language models. arXiv preprint\\narXiv:2302.13971.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, et al. 2023b.\\nLlama 2: Open founda-\\ntion and fine-tuned chat models.\\narXiv preprint\\narXiv:2307.09288.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. Advances in neural information processing\\nsystems, 30.\\nCunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xian-\\ngru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi\\nYao, Wenyang Gao, Xuming Hu, Zehan Qi, et al.\\n2023. Survey on factuality in large language models:\\nKnowledge, retrieval and domain-specificity. arXiv',\n",
       "  'preprint arXiv:2310.07521.\\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\\nter Liu. 2020. Pegasus: Pre-training with extracted\\ngap-sentences for abstractive summarization. In In-\\nternational Conference on Machine Learning, pages\\n11328–11339. PMLR.\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\\nsurvey of large language models.\\narXiv preprint\\narXiv:2303.18223.']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs = db.get()\n",
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91166978-e510-4a33-888d-c1201f9dc1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_docs[\"ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392afc0-ecc2-459c-9817-af1266da0ea5",
   "metadata": {},
   "source": [
    "idを指定すれば個別のドキュメントも取得可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd0da500-5dc7-4001-9672-296fd994a24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['d662276e-88fe-11ee-8738-66a9796eac2f'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN',\n",
       "   'Published': '2023-11-01',\n",
       "   'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\",\n",
       "   'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs',\n",
       "   'start_index': 0}],\n",
       " 'documents': ['Are Large Language Models Reliable Judges?\\nA Study on the Factuality Evaluation Capabilities of LLMs\\nXue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN\\nDialpad Canada Inc.\\n{xue-yong,tahmid.rahman,cchen,sbhushan}@dialpad.com\\nAbstract\\nIn recent years, Large Language Models\\n(LLMs) have drawn significant attention due\\nto their impressive emergent capabilities that\\nwere not observed in earlier language models.\\nOne emerging area where LLMs have been\\nwidely used in recent times is the utilization\\nof LLMs as the evaluator of the texts gener-\\nated by various generative models. In this pa-\\nper, we also explore the possibility of whether\\nLLMs are reliable in assessing the factual con-\\nsistency of summaries generated by text gen-\\neration models. We first propose a new ap-\\nproach to evaluate the factuality score using\\nLLMs by utilizing one single LLM to perform\\nall steps in the question-answering-based fac-\\ntuality scoring pipeline. Subsequently, we also']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_id = all_docs[\"ids\"][0]\n",
    "sample_doc = db.get(doc_id)\n",
    "sample_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382fc280-4c86-42f5-8ddc-0fe5d0a4fa3e",
   "metadata": {},
   "source": [
    "### Vector Databaseを使った検索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca403ac-4ff7-4dca-b902-f37325b254d3",
   "metadata": {},
   "source": [
    "`smilarity_search` メソッドを使って検索をすることができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05575960-dce6-43bc-b249-b898df0375ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Dict[str, str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'List[Document]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Run similarity search with Chroma.\n",
       "\n",
       "Args:\n",
       "    query (str): Query text to search for.\n",
       "    k (int): Number of results to return. Defaults to 4.\n",
       "    filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
       "\n",
       "Returns:\n",
       "    List[Document]: List of documents most similar to the query text.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/vectorstores/chroma.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db.similarity_search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "528717f8-6a62-4f7f-a77e-a1cf6baf5544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='progress across a broad spectrum of NLP tasks,\\nfrom text classification to generation, language\\ntranslation, and beyond (Laskar et al., 2023a,c).\\nGiven the capabilities of these LLMs, our re-\\nsearch explores the possibility of utilizing LLMs\\nfor the critical task of factual consistency evalua-\\ntion (Dubois et al., 2023; Liu et al., 2023b; Manakul\\net al., 2023; Tang et al., 2022; Laban et al., 2023).\\nTo assess the factual consistency of a model, one\\ncommon approach is the utilization of a question-\\nanswering (QA) pipeline (Huang et al., 2021). Tra-\\nditionally, the evaluation of factuality using QA\\nsystems has involved the use of separate, distinct\\nmodels for each of the following tasks: answer se-\\nlection, question generation, and question answer-\\ning (Huang et al., 2021). However, this approach\\ninvolves the intricate task of coordinating between\\nthese disparate models, potentially resulting in in-\\nefficiencies in real-world scenarios. Additionally,', metadata={'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN', 'Published': '2023-11-01', 'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\", 'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs', 'start_index': 2948})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = db.similarity_search(\"LLaMA\")\n",
    "search_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd62be-1534-46b4-975b-2c0d48a30f3e",
   "metadata": {},
   "source": [
    "`similarity_search_with_score` メソッドを使えば、ドキュメントとの距離の近さも返してくれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "111e9e0b-415a-443c-9003-85a9000763ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search_with_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Dict[str, str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwhere_document\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Dict[str, str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'List[Tuple[Document, float]]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Run similarity search with Chroma with distance.\n",
       "\n",
       "Args:\n",
       "    query (str): Query text to search for.\n",
       "    k (int): Number of results to return. Defaults to 4.\n",
       "    filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
       "\n",
       "Returns:\n",
       "    List[Tuple[Document, float]]: List of documents most similar to\n",
       "    the query text and cosine distance in float for each.\n",
       "    Lower score represents more similarity.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/vectorstores/chroma.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db.similarity_search_with_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f12284c-f5e6-4402-aa38-b89d4134de9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='progress across a broad spectrum of NLP tasks,\\nfrom text classification to generation, language\\ntranslation, and beyond (Laskar et al., 2023a,c).\\nGiven the capabilities of these LLMs, our re-\\nsearch explores the possibility of utilizing LLMs\\nfor the critical task of factual consistency evalua-\\ntion (Dubois et al., 2023; Liu et al., 2023b; Manakul\\net al., 2023; Tang et al., 2022; Laban et al., 2023).\\nTo assess the factual consistency of a model, one\\ncommon approach is the utilization of a question-\\nanswering (QA) pipeline (Huang et al., 2021). Tra-\\nditionally, the evaluation of factuality using QA\\nsystems has involved the use of separate, distinct\\nmodels for each of the following tasks: answer se-\\nlection, question generation, and question answer-\\ning (Huang et al., 2021). However, this approach\\ninvolves the intricate task of coordinating between\\nthese disparate models, potentially resulting in in-\\nefficiencies in real-world scenarios. Additionally,', metadata={'Authors': 'Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN', 'Published': '2023-11-01', 'Summary': \"In recent years, Large Language Models (LLMs) have gained immense attention\\ndue to their notable emergent capabilities, surpassing those seen in earlier\\nlanguage models. A particularly intriguing application of LLMs is their role as\\nevaluators for texts produced by various generative models.\\n  In this study, we delve into the potential of LLMs as reliable assessors of\\nfactual consistency in summaries generated by text-generation models.\\nInitially, we introduce an innovative approach for factuality assessment using\\nLLMs. This entails employing a singular LLM for the entirety of the\\nquestion-answering-based factuality scoring process. Following this, we examine\\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\\nagainst traditional measures and human annotations.\\n  Contrary to initial expectations, our results indicate a lack of significant\\ncorrelations between factuality metrics and human evaluations, specifically for\\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\\ntwo factuality subcategories. These consistent findings across various factual\\nerror categories suggest a fundamental limitation in the current LLMs'\\ncapability to accurately gauge factuality.\\n  This version presents the information more concisely while maintaining the\\nmain points and findings of the original text.\", 'Title': 'Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs', 'start_index': 2948}),\n",
       " 0.42234497482685956)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results_with_score = db.similarity_search_with_score(\"LLaMA\")\n",
    "search_results_with_score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aba537bf-4fed-4b46-a6e0-4da92a3bb7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42234497482685956,\n",
       " 0.4338907856430122,\n",
       " 0.44076382254582985,\n",
       " 0.44147816213644003]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[1] for x in search_results_with_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710b2a89-1e1f-4998-991e-33fcf6ed4cce",
   "metadata": {},
   "source": [
    "小さい値であればあるほど、ドキュメントとクエリの距離が近い、つまり、より関連しているといえる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa0b8ae-3846-4883-b10d-c0bb1a769dfd",
   "metadata": {},
   "source": [
    "## RetrievalQAを使った検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13bfdd30-1491-4f51-90f0-860ac25880e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73dde3-c70a-476f-9a6f-1a9689266f9e",
   "metadata": {},
   "source": [
    "### Retrieverの作成\n",
    "\n",
    "ChatGPTとVector Databaseを連携するためにRetrieverを作成する。\n",
    "\n",
    "Retrieverは `as_retriever` メソッドを使って作成できる。\n",
    "検索タイプを指定したり、検索結果数や距離のしきい値を決めることができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7f712a6-ae97-4c6f-98af-3d511b6188b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'VectorStoreRetriever'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return VectorStoreRetriever initialized from this VectorStore.\n",
       "\n",
       "Args:\n",
       "    search_type (Optional[str]): Defines the type of search that\n",
       "        the Retriever should perform.\n",
       "        Can be \"similarity\" (default), \"mmr\", or\n",
       "        \"similarity_score_threshold\".\n",
       "    search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
       "        search function. Can include things like:\n",
       "            k: Amount of documents to return (Default: 4)\n",
       "            score_threshold: Minimum relevance threshold\n",
       "                for similarity_score_threshold\n",
       "            fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\n",
       "            lambda_mult: Diversity of results returned by MMR;\n",
       "                1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
       "            filter: Filter by document metadata\n",
       "\n",
       "Returns:\n",
       "    VectorStoreRetriever: Retriever class for VectorStore.\n",
       "\n",
       "Examples:\n",
       "\n",
       ".. code-block:: python\n",
       "\n",
       "    # Retrieve more documents with higher diversity\n",
       "    # Useful if your dataset has many similar documents\n",
       "    docsearch.as_retriever(\n",
       "        search_type=\"mmr\",\n",
       "        search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
       "    )\n",
       "\n",
       "    # Fetch more documents for the MMR algorithm to consider\n",
       "    # But only return the top 5\n",
       "    docsearch.as_retriever(\n",
       "        search_type=\"mmr\",\n",
       "        search_kwargs={'k': 5, 'fetch_k': 50}\n",
       "    )\n",
       "\n",
       "    # Only retrieve documents that have a relevance score\n",
       "    # Above a certain threshold\n",
       "    docsearch.as_retriever(\n",
       "        search_type=\"similarity_score_threshold\",\n",
       "        search_kwargs={'score_threshold': 0.8}\n",
       "    )\n",
       "\n",
       "    # Only get the single most similar document from the dataset\n",
       "    docsearch.as_retriever(search_kwargs={'k': 1})\n",
       "\n",
       "    # Use a filter to only retrieve documents from a specific paper\n",
       "    docsearch.as_retriever(\n",
       "        search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
       "    )\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/langchain/schema/vectorstore.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db.as_retriever?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "675c21a3-9252-4cd1-b8be-5e56258803df",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331bf67b-33e2-467f-8ad7-753a2596b806",
   "metadata": {},
   "source": [
    "### RetrievalQAで回答を生成してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc09ff61-3d56-4cec-94d6-709a00f4f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model=\"gpt-3.5-turbo-0613\")\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c2b6df0-a1d7-4d49-8ec0-e6130c03246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "controlling for confounding variables, potentially\n",
      "mitigating the risk of inaccurate high correlation\n",
      "measures (Pagnoni et al., 2021).\n",
      "3\n",
      "Methodology\n",
      "In this section, we present our methods: (i) Using\n",
      "LLMs as QA-based factuality metric, and (ii) Using\n",
      "LLMs for direct factuality scoring. Below, we first\n",
      "present these methods.\n",
      "(i) QA-based Factuality Metric via LLMs:\n",
      "The reason we chose to incorporate LLMs into\n",
      "the QA-based factuality metric is that it is more re-\n",
      "liable than most other existing automatic factuality\n",
      "metrics for assessing the factual consistency of a\n",
      "model (Huang et al., 2021). The typical process of\n",
      "using QA-based systems as factuality evaluators is\n",
      "comprised of 3 tasks:\n",
      "(i) Answer Selection: The commencement of\n",
      "this procedure involves extracting key points, re-\n",
      "ferred to as “answers” from the provided summary.\n",
      "(ii) Question Generation: After identifying the\n",
      "answers, the next step is to formulate questions\n",
      "based on these answers, using the summary as the\n",
      "context.\n",
      "\n",
      "progress across a broad spectrum of NLP tasks,\n",
      "from text classification to generation, language\n",
      "translation, and beyond (Laskar et al., 2023a,c).\n",
      "Given the capabilities of these LLMs, our re-\n",
      "search explores the possibility of utilizing LLMs\n",
      "for the critical task of factual consistency evalua-\n",
      "tion (Dubois et al., 2023; Liu et al., 2023b; Manakul\n",
      "et al., 2023; Tang et al., 2022; Laban et al., 2023).\n",
      "To assess the factual consistency of a model, one\n",
      "common approach is the utilization of a question-\n",
      "answering (QA) pipeline (Huang et al., 2021). Tra-\n",
      "ditionally, the evaluation of factuality using QA\n",
      "systems has involved the use of separate, distinct\n",
      "models for each of the following tasks: answer se-\n",
      "lection, question generation, and question answer-\n",
      "ing (Huang et al., 2021). However, this approach\n",
      "involves the intricate task of coordinating between\n",
      "these disparate models, potentially resulting in in-\n",
      "efficiencies in real-world scenarios. Additionally,\n",
      "\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\n",
      "Martinet, Marie-Anne Lachaux, Timothée Lacroix,\n",
      "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\n",
      "Azhar, et al. 2023a.\n",
      "Llama:\n",
      "Open and effi-\n",
      "cient foundation language models. arXiv preprint\n",
      "arXiv:2302.13971.\n",
      "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\n",
      "bert, Amjad Almahairi, Yasmine Babaei, Nikolay\n",
      "Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\n",
      "Bhosale, et al. 2023b.\n",
      "Llama 2: Open founda-\n",
      "tion and fine-tuned chat models.\n",
      "arXiv preprint\n",
      "arXiv:2307.09288.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\n",
      "Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
      "you need. Advances in neural information processing\n",
      "systems, 30.\n",
      "Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xian-\n",
      "gru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi\n",
      "Yao, Wenyang Gao, Xuming Hu, Zehan Qi, et al.\n",
      "2023. Survey on factuality in large language models:\n",
      "Knowledge, retrieval and domain-specificity. arXiv\n",
      "\n",
      "Are Large Language Models Reliable Judges?\n",
      "A Study on the Factuality Evaluation Capabilities of LLMs\n",
      "Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen, Shashi Bhushan TN\n",
      "Dialpad Canada Inc.\n",
      "{xue-yong,tahmid.rahman,cchen,sbhushan}@dialpad.com\n",
      "Abstract\n",
      "In recent years, Large Language Models\n",
      "(LLMs) have drawn significant attention due\n",
      "to their impressive emergent capabilities that\n",
      "were not observed in earlier language models.\n",
      "One emerging area where LLMs have been\n",
      "widely used in recent times is the utilization\n",
      "of LLMs as the evaluator of the texts gener-\n",
      "ated by various generative models. In this pa-\n",
      "per, we also explore the possibility of whether\n",
      "LLMs are reliable in assessing the factual con-\n",
      "sistency of summaries generated by text gen-\n",
      "eration models. We first propose a new ap-\n",
      "proach to evaluate the factuality score using\n",
      "LLMs by utilizing one single LLM to perform\n",
      "all steps in the question-answering-based fac-\n",
      "tuality scoring pipeline. Subsequently, we also\n",
      "Human: what is LLaMA?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LLaMA stands for \"Llama: Open and efficient foundation language models.\" It is a language model that has been developed for various natural language processing (NLP) tasks, including text classification, generation, language translation, and more. LLaMA is known for its impressive capabilities and has been utilized in research exploring the evaluation of factual consistency in text generation models.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is LLaMA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d31f97-0864-45a2-bd19-99f72438a550",
   "metadata": {},
   "source": [
    "ChatGPTに投げるプロンプトの冒頭に\n",
    "```\n",
    "System: Use the following pieces of context to answer the user's question. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "```\n",
    "と書いてあり、このメッセージに続く形で、contextを渡していることがわかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6a09c-5414-4e0c-8dfa-2c8f73c67b51",
   "metadata": {},
   "source": [
    "今回のクエリと近いドキュメントの中身とpromptに組み込まれたcontextの関係を調べてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "302aadce-f2a0-4f30-bad5-af9a94e838fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controlling for confounding variables, potentially\n",
      "mitigating the risk of inaccurate high correlation\n",
      "measures (Pagnoni et al., 2021).\n",
      "3\n",
      "Methodology\n",
      "In this section, we present our methods: (i) Using\n",
      "LLMs as QA-based factuality metric, and (ii) Using\n",
      "LLMs for direct factuality scoring. Below, we first\n",
      "present these methods.\n",
      "(i) QA-based Factuality Metric via LLMs:\n",
      "The reason we chose to incorporate LLMs into\n",
      "the QA-based factuality metric is that it is more re-\n",
      "liable than most other existing automatic factuality\n",
      "metrics for assessing the factual consistency of a\n",
      "model (Huang et al., 2021). The typical process of\n",
      "using QA-based systems as factuality evaluators is\n",
      "comprised of 3 tasks:\n",
      "(i) Answer Selection: The commencement of\n",
      "this procedure involves extracting key points, re-\n",
      "ferred to as “answers” from the provided summary.\n",
      "(ii) Question Generation: After identifying the\n",
      "answers, the next step is to formulate questions\n",
      "based on these answers, using the summary as the\n",
      "context.\n"
     ]
    }
   ],
   "source": [
    "print(db.similarity_search(\"what is LLaMA?\")[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb978ddb-620b-40b3-947e-8c62ca5b51b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress across a broad spectrum of NLP tasks,\n",
      "from text classification to generation, language\n",
      "translation, and beyond (Laskar et al., 2023a,c).\n",
      "Given the capabilities of these LLMs, our re-\n",
      "search explores the possibility of utilizing LLMs\n",
      "for the critical task of factual consistency evalua-\n",
      "tion (Dubois et al., 2023; Liu et al., 2023b; Manakul\n",
      "et al., 2023; Tang et al., 2022; Laban et al., 2023).\n",
      "To assess the factual consistency of a model, one\n",
      "common approach is the utilization of a question-\n",
      "answering (QA) pipeline (Huang et al., 2021). Tra-\n",
      "ditionally, the evaluation of factuality using QA\n",
      "systems has involved the use of separate, distinct\n",
      "models for each of the following tasks: answer se-\n",
      "lection, question generation, and question answer-\n",
      "ing (Huang et al., 2021). However, this approach\n",
      "involves the intricate task of coordinating between\n",
      "these disparate models, potentially resulting in in-\n",
      "efficiencies in real-world scenarios. Additionally,\n"
     ]
    }
   ],
   "source": [
    "print(db.similarity_search(\"what is LLaMA?\")[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad4215f-b26a-4515-8b01-189b2126ad41",
   "metadata": {},
   "source": [
    "実際に距離の近いドキュメントを、距離が近い順に並べてpromptの入力として使っている事がわかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956d77f-f57d-448c-875e-d57831292e14",
   "metadata": {},
   "source": [
    "次に全く関係のない質問を投げてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3298ead8-17a4-4b5b-bff6-68e315b9955d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Google. 2023. Palm 2 technical report. Goole AI.\n",
      "Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai\n",
      "Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas\n",
      "Scialom, Idan Szpektor, Avinatan Hassidim, and\n",
      "Yossi Matias. 2022.\n",
      "True: Re-evaluating factual\n",
      "consistency evaluation. In Proceedings of the 2022\n",
      "Conference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Human\n",
      "Language Technologies, pages 3905–3920.\n",
      "Yichong Huang, Xiachong Feng, Xiaocheng Feng, and\n",
      "Bing Qin. 2021. The factual inconsistency problem\n",
      "in abstractive text summarization: A survey. arXiv\n",
      "preprint arXiv:2104.14839.\n",
      "Elena Khasanova, Pooja Hiranandani, Shayna Gardiner,\n",
      "Cheng Chen, Simon Corston-Oliver, and Xue-Yong\n",
      "Fu. 2022. Developing a production system for Pur-\n",
      "pose of Call detection in business phone conversa-\n",
      "tions. In Proceedings of the 2022 Conference of the\n",
      "North American Chapter of the Association for Com-\n",
      "putational Linguistics: Human Language Technolo-\n",
      "\n",
      "the 2022 Conference of the North American Chapter\n",
      "of the Association for Computational Linguistics: Hu-\n",
      "man Language Technologies: Industry Track, pages\n",
      "344–352, Hybrid: Seattle, Washington + Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Md Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen,\n",
      "and Shashi Bhushan TN. 2023c. Building real-world\n",
      "meeting summarization systems using large language\n",
      "models: A practical perspective.\n",
      "arXiv preprint\n",
      "arXiv:2310.19233.\n",
      "Md Tahmid Rahman Laskar, Enamul Hoque, and\n",
      "Jimmy Xiangji Huang. 2022c. Domain adaptation\n",
      "with pre-trained transformers for query-focused ab-\n",
      "stractive text summarization. Computational Linguis-\n",
      "tics, 48(2):279–320.\n",
      "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan\n",
      "Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\n",
      "Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\n",
      "Denoising sequence-to-sequence pre-training for nat-\n",
      "ural language generation, translation, and comprehen-\n",
      "sion. In Proceedings of the 58th Annual Meeting of\n",
      "\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics (Volume 5: Industry Track), pages 652–659,\n",
      "Toronto, Canada. Association for Computational Lin-\n",
      "guistics.\n",
      "Joshua Maynez, Shashi Narayan, Bernd Bohnet, and\n",
      "Ryan McDonald. 2020. On faithfulness and factu-\n",
      "ality in abstractive summarization. In Proceedings\n",
      "of the 58th Annual Meeting of the Association for\n",
      "Computational Linguistics, pages 1906–1919.\n",
      "Ramesh Nallapati, Bowen Zhou, Cícero Nogueira dos\n",
      "Santos, Çaglar Gülçehre, and Bing Xiang. 2016.\n",
      "Abstractive text summarization using sequence-to-\n",
      "sequence rnns and beyond. In Proceedings of the\n",
      "20th SIGNLL Conference on Computational Natural\n",
      "Language Learning, CoNLL 2016, Berlin, Germany,\n",
      "August 11-12, 2016, pages 280–290. ACL.\n",
      "Shashi Narayan, Shay B Cohen, and Mirella Lapata.\n",
      "2018. Don’t give me the details, just the summary!\n",
      "topic-aware convolutional neural networks for ex-\n",
      "treme summarization. In Proceedings of the 2018\n",
      "Conference on Empirical Methods in Natural Lan-\n",
      "\n",
      "gies: Industry Track, pages 259–267, Hybrid: Seattle,\n",
      "Washington + Online. Association for Computational\n",
      "Linguistics.\n",
      "Philippe Laban, Wojciech Kry´sci´nski, Divyansh Agar-\n",
      "wal, Alexander R Fabbri, Caiming Xiong, Shafiq\n",
      "Joty, and Chien-Sheng Wu. 2023. Llms as factual\n",
      "reasoners: Insights from existing benchmarks and\n",
      "beyond. arXiv preprint arXiv:2305.14540.\n",
      "Philippe Laban, Tobias Schnabel, Paul N Bennett, and\n",
      "Marti A Hearst. 2022.\n",
      "Summac: Re-visiting nli-\n",
      "based models for inconsistency detection in summa-\n",
      "rization. Transactions of the Association for Compu-\n",
      "tational Linguistics, 10:163–177.\n",
      "Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur\n",
      "Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty,\n",
      "and Jimmy Huang. 2023a. A systematic study and\n",
      "comprehensive evaluation of ChatGPT on benchmark\n",
      "datasets. In Findings of the Association for Com-\n",
      "putational Linguistics: ACL 2023, pages 431–469,\n",
      "Toronto, Canada. Association for Computational Lin-\n",
      "guistics.\n",
      "Md Tahmid Rahman Laskar, Cheng Chen, Xue-yong Fu,\n",
      "Human: What are NVIDIA's latest revenues?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to real-time information. It's best to check the latest financial reports or news articles to find out NVIDIA's latest revenues.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvidia_question = \"What are NVIDIA's latest revenues?\"\n",
    "chain.run(nvidia_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667c138-891b-47f1-aa7a-51e1a0b39237",
   "metadata": {},
   "source": [
    "期待した答え `I don't know` とは返ってきていないが、正しい答えを返せないことがわかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11589f9f-13df-4b84-b447-a5084c7db79c",
   "metadata": {},
   "source": [
    "### 別のドキュメントを追加して回答を作成\n",
    "全く別のドキュメントを追加して回答を作成してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44a797e8-6c29-4f3b-9782-cf3d83d44b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "html_loader = WebBaseLoader(\"https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-third-quarter-fiscal-2024\")\n",
    "html_doc = html_loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffeaf98f-f8bc-48e2-ab4a-607e26f4025f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(html_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a47eec6-1d1f-4fd2-b211-ae8cb8ff5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(html_doc, embeddings, collection_name=collection_name, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a410282-2418-4a9d-a9ef-f66726882efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b835b0-3d29-4790-a336-2a619b3a8127",
   "metadata": {},
   "source": [
    "ドキュメントの数が増えていることがわかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4424667-76e6-4d4a-b205-1ac1bac38610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Professional Visualization\n",
      "\n",
      "Third-quarter revenue was $416 million, up 10% from the previous quarter and up 108% from a year ago.\n",
      "Announced that Mercedes-Benz is using NVIDIA Omniverse to create digital twins to help plan, design, build and operate its manufacturing and assembly facilities around the world.\n",
      "Announced a new line of desktop workstations with NVIDIA RTX™ 6000 Ada Generation GPUs and NVIDIA ConnectX® smart interface cards for training smaller AI models, fine-tuning models and running inference locally.\n",
      "\n",
      "Automotive \n",
      "\n",
      "Third-quarter revenue was $261 million, up 3% from the previous quarter and up 4% from a year ago.\n",
      "Furthered its collaboration with Foxconn to develop next-generation electric vehicles for the global market, using the next-generation NVIDIA DRIVE Hyperion™ platform and NVIDIA DRIVE Thor™ system-on-a-chip.\n",
      "\n",
      "CFO Commentary\n",
      "Commentary on the quarter by Colette Kress, NVIDIA’s executive vice president and chief financial officer, is available at https://investor.nvidia.com.\n",
      "Conference Call and Webcast Information\n",
      "NVIDIA will conduct a conference call with analysts and investors to discuss its third quarter fiscal 2024 financial results and current financial prospects today at 2 p.m. Pacific time (5 p.m. Eastern time). A live webcast (listen-only mode) of the conference call will be accessible at NVIDIA’s investor relations website, https://investor.nvidia.com. The webcast will be recorded and available for replay until NVIDIA’s conference call to discuss its financial results for its fourth quarter and fiscal 2024.\n",
      "Non-GAAP Measures\n",
      "To supplement NVIDIA’s condensed consolidated financial statements presented in accordance with GAAP, the company uses non-GAAP measures of certain components of financial performance. These non-GAAP measures include non-GAAP gross profit, non-GAAP gross margin, non-GAAP operating expenses, non-GAAP income from operations, non-GAAP other income (expense), net, non-GAAP net income, non-GAAP net income, or earnings, per diluted share, and free cash flow. For NVIDIA’s investors to be better able to compare its current results with those of previous periods, the company has shown a reconciliation of GAAP to non-GAAP financial measures. These reconciliations adjust the related GAAP financial measures to exclude acquisition termination costs, stock-based compensation expense, acquisition-related and other costs, IP-related costs, other, gains and losses from non-affiliated investments, interest expense related to amortization of debt discount, and the associated tax impact of these items where applicable. Free cash flow is calculated as GAAP net cash provided by operating activities less both purchases of property and equipment and intangible assets and principal payments on property and equipment and intangible assets. NVIDIA believes the presentation of its non-GAAP financial measures enhances the user’s overall understanding of the company’s historical financial performance. The presentation of the company’s non-GAAP financial measures is not meant to be considered in isolation or as a substitute for the company’s financial results prepared in accordance with GAAP, and the company’s non-GAAP measures may be different from non-GAAP measures used by other companies.\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "NVIDIA CORPORATION\n",
      "\n",
      "\n",
      " CONDENSED CONSOLIDATED STATEMENTS OF INCOME\n",
      "\n",
      "\n",
      "(In millions, except per share data)\n",
      "\n",
      "\n",
      "(Unaudited)\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "Three Months Ended\n",
      " \n",
      "Nine Months Ended\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "October 29,\n",
      " \n",
      "October 30,\n",
      " \n",
      "October 29,\n",
      " \n",
      "October 30,\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " 2023 \n",
      " \n",
      " 2022 \n",
      " \n",
      " 2023 \n",
      " \n",
      " 2022 \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "Revenue\n",
      "$\n",
      "18,120\n",
      " \n",
      " \n",
      "$\n",
      "5,931\n",
      " \n",
      " \n",
      "$\n",
      "38,819\n",
      " \n",
      " \n",
      "$\n",
      "20,923\n",
      " \n",
      "\n",
      "\n",
      "Cost of revenue\n",
      " \n",
      "4,720\n",
      " \n",
      " \n",
      " \n",
      "2,754\n",
      " \n",
      " \n",
      " \n",
      "11,309\n",
      " \n",
      " \n",
      " \n",
      "9,400\n",
      " \n",
      "\n",
      "\n",
      "Gross profit\n",
      " \n",
      "13,400\n",
      " \n",
      " \n",
      " \n",
      "3,177\n",
      " \n",
      " \n",
      " \n",
      "27,510\n",
      " \n",
      " \n",
      " \n",
      "11,523\n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "Operating expenses\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "Research and development\n",
      " \n",
      "2,294\n",
      " \n",
      " \n",
      " \n",
      "1,945\n",
      " \n",
      " \n",
      " \n",
      "6,210\n",
      " \n",
      " \n",
      " \n",
      "5,387\n",
      "\n",
      "Diluted earnings per share\n",
      "$4.02\n",
      "$2.70\n",
      "$0.58\n",
      "Up 49%\n",
      "Up 593%\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Outlook\n",
      "NVIDIA’s outlook for the fourth quarter of fiscal 2024 is as follows:\n",
      "\n",
      "Revenue is expected to be $20.00 billion, plus or minus 2%.\n",
      "GAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points.\n",
      "GAAP and non-GAAP operating expenses are expected to be approximately $3.17 billion and $2.20 billion, respectively.\n",
      "GAAP and non-GAAP other income and expense are expected to be an income of approximately $200 million, excluding gains and losses from non-affiliated investments.\n",
      "GAAP and non-GAAP tax rates are expected to be 15.0%, plus or minus 1%, excluding any discrete items.\n",
      "\n",
      "Highlights \n",
      "NVIDIA achieved progress since its previous earnings announcement in these areas: \n",
      "Data Center \n",
      "\n",
      "Third-quarter revenue was a record $14.51 billion, up 41% from the previous quarter and up 279% from a year ago.\n",
      "Announced NVIDIA HGX™ H200 with the new NVIDIA H200 Tensor Core GPU, the first GPU with HBM3e memory, with systems expected to be available in the second quarter of next year.\n",
      "Introduced an AI foundry service — with NVIDIA AI Foundation Models, NVIDIA NeMo™ framework and NVIDIA DGX™ Cloud AI supercomputing — to accelerate the development and tuning of custom generative AI applications, first available on Microsoft Azure, with SAP and Amdocs among the first customers.\n",
      "Announced that the NVIDIA Spectrum-X™ Ethernet networking platform for AI will be integrated into servers from Dell Technologies, Hewlett Packard Enterprise and Lenovo in the first quarter of next year.\n",
      "Announced that NVIDIA GH200 Grace Hopper Superchips, including a new quad configuration, will power more than 40 new supercomputers, including the JUPITER system at Jülich Supercomputing Centre and Isambard-AI at the University of Bristol.\n",
      "Made advances with global cloud service providers:\n",
      "\t\n",
      "Google Cloud Platform made generally available new A3 instances powered by NVIDIA H100 Tensor Core GPUs and NVIDIA AI Enterprise software in Google Cloud Marketplace.\n",
      "Microsoft Azure will be offering customers access to NVIDIA Omniverse™ Cloud Services for accelerating automotive digitalization, as well as new instances featuring NVL H100 Tensor Core GPUs and H100 with confidential computing, with H200 GPUs coming next year.\n",
      "Oracle Cloud Infrastructure made NVIDIA DGX Cloud and NVIDIA AI Enterprise software available in Oracle Cloud Marketplace.\n",
      "\n",
      "\n",
      "Partnered with a range of leading companies on AI initiatives, including Amdocs, Dropbox, Foxconn, Genentech (member of Roche Group), Infosys, Lenovo, Reliance Industries, Scaleway and Tata Group.\n",
      "Announced record-setting performance in the latest two sets of MLPerf benchmarks for inference and training, with the NVIDIA Eos AI supercomputer training a GPT-3 model 3x faster than the previous record.\n",
      "Announced growing worldwide support for the NVIDIA® CUDA® Quantum platform, including new efforts in Israel, the Netherlands, the U.K. and the U.S.\n",
      "\n",
      "Gaming \n",
      "\n",
      "Third-quarter revenue was $2.86 billion, up 15% from the previous quarter and up 81% from a year ago. \n",
      "Launched DLSS 3.5 Ray Reconstruction, which creates high-quality ray-traced images for intensive ray-traced games and apps, including Alan Wake 2 and Cyberpunk 2077. \n",
      "Released TensorRT-LLM™ for Windows, speeding on-device LLM inference by up to 4x. \n",
      "Added 56 DLSS games and over 15 Reflex games, bringing the total number of RTX games and applications to over 475. \n",
      "Surpassed 1,700 games on GeForce NOW™, including launches of Alan Wake 2, Baldur’s Gate 3, Cyberpunk 2077: Phantom Liberty, Forza Motorsport and Starfield.\n",
      "\n",
      "Professional Visualization\n",
      "\n",
      "NVIDIA Announces Financial Results for Third Quarter Fiscal 2024 | NVIDIA Newsroom\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Artificial Intelligence Computing Leadership from NVIDIA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PLATFORMS\n",
      "\n",
      "\n",
      "  Autonomous Machines\n",
      "\n",
      "\n",
      "\n",
      "  Cloud & Data Center\n",
      "\n",
      "\n",
      "\n",
      "  Deep Learning & Ai\n",
      "\n",
      "\n",
      "\n",
      "  Design & Pro Visualization\n",
      "\n",
      "\n",
      "\n",
      "  Healthcare\n",
      "\n",
      "\n",
      "\n",
      "  High Performance Computing\n",
      "\n",
      "\n",
      "\n",
      "  Self-Driving Cars\n",
      "\n",
      "\n",
      "\n",
      "  Gaming & Entertainment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "other links\n",
      "\n",
      "\n",
      "Developers\n",
      "Industries\n",
      "Shop\n",
      "Drivers\n",
      "Support\n",
      "About NVIDIA\n",
      "\n",
      "\n",
      "View All Products\n",
      "GPU TECHNOLOGY CONFERENCE\n",
      "NVIDIA Blog\n",
      "Community\n",
      "Careers\n",
      "TECHNOLOGIES\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Newsroom\n",
      "\n",
      "\n",
      "\n",
      "NVIDIA in Brief\n",
      "\n",
      "\n",
      "Exec Bios\n",
      "\n",
      "\n",
      "NVIDIA Blog\n",
      "\n",
      "\n",
      "Podcast\n",
      "\n",
      "\n",
      "Media Assets\n",
      "\n",
      "\n",
      "In the News\n",
      "\n",
      "\n",
      "Press Contacts\n",
      "\n",
      "\n",
      "Online Press Kit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NVIDIA in Brief\n",
      "\n",
      "\n",
      "Exec Bios\n",
      "\n",
      "\n",
      "NVIDIA Blog\n",
      "\n",
      "\n",
      "Podcast\n",
      "\n",
      "\n",
      "Media Assets\n",
      "\n",
      "\n",
      "In the News\n",
      "\n",
      "\n",
      "Press Contacts\n",
      "\n",
      "\n",
      "Online Press Kit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Press Release\n",
      "    \n",
      "  \n",
      "\n",
      "Share\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tweet\n",
      "\n",
      "\n",
      "Twitter\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Share\n",
      "\n",
      "\n",
      "LinkedIn\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Share\n",
      "\n",
      "\n",
      "Facebook\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Email\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ic_arrow-back-to-top\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NVIDIA Announces Financial Results for Third Quarter Fiscal 2024\n",
      "\n",
      "          November 21, 2023\n",
      "        \n",
      "\n",
      "\n",
      "Record revenue of $18.12 billion, up 34% from Q2, up 206% from year ago\n",
      "Record Data Center revenue of $14.51 billion, up 41% from Q2, up 279% from year ago\n",
      "\n",
      "NVIDIA (NASDAQ: NVDA) today reported revenue for the third quarter ended October 29, 2023, of $18.12 billion, up 206% from a year ago and up 34% from the previous quarter.\n",
      "GAAP earnings per diluted share for the quarter were $3.71, up more than 12x from a year ago and up 50% from the previous quarter. Non-GAAP earnings per diluted share were $4.02, up nearly 6x from a year ago and up 49% from the previous quarter.\n",
      "“Our strong growth reflects the broad industry platform transition from general-purpose to accelerated computing and generative AI,” said Jensen Huang, founder and CEO of NVIDIA.\n",
      "“Large language model startups, consumer internet companies and global cloud service providers were the first movers, and the next waves are starting to build. Nations and regional CSPs are investing in AI clouds to serve local demand, enterprise software companies are adding AI copilots and assistants to their platforms, and enterprises are creating custom AI to automate the world’s largest industries.\n",
      "“NVIDIA GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software are all growth engines in full throttle. The era of generative AI is taking off,” he said.\n",
      "NVIDIA will pay its next quarterly cash dividend of $0.04 per share on December 28, 2023, to all shareholders of record on December 6, 2023.\n",
      "Q3 Fiscal 2024 Summary\n",
      "\n",
      "\n",
      "\n",
      "GAAP\n",
      "\n",
      "\n",
      "($ in millions, except earnings \n",
      "per share)\n",
      "Q3 FY24\n",
      "Q2 FY24\n",
      "Q3 FY23\n",
      "Q/Q\n",
      "Y/Y\n",
      "\n",
      "\n",
      "Revenue\n",
      "$18,120\n",
      "$13,507\n",
      "$5,931\n",
      "Up 34%\n",
      "Up 206%\n",
      "\n",
      "\n",
      "Gross margin\n",
      "74.0%\n",
      "70.1%\n",
      "53.6%\n",
      "Up 3.9 pts\n",
      "Up 20.4 pts\n",
      "\n",
      "\n",
      "Operating expenses\n",
      "$2,983\n",
      "$2,662\n",
      "$2,576\n",
      "Up 12%\n",
      "Up 16%\n",
      "\n",
      "\n",
      "Operating income\n",
      "$10,417\n",
      "$6,800\n",
      "$601\n",
      "Up 53%\n",
      "Up 1,633%\n",
      "\n",
      "\n",
      "Net income\n",
      "$9,243\n",
      "$6,188\n",
      "$680\n",
      "Up 49%\n",
      "Up 1,259%\n",
      "\n",
      "\n",
      "Diluted earnings per share\n",
      "$3.71\n",
      "$2.48\n",
      "$0.27\n",
      "Up 50%\n",
      "Up 1,274%\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Non-GAAP\n",
      "\n",
      "\n",
      "($ in millions, except earnings \n",
      "per share)\n",
      "Q3 FY24\n",
      "Q2 FY24\n",
      "Q3 FY23\n",
      "Q/Q\n",
      "Y/Y\n",
      "\n",
      "\n",
      "Revenue\n",
      "$18,120\n",
      "$13,507\n",
      "$5,931\n",
      "Up 34%\n",
      "Up 206%\n",
      "\n",
      "\n",
      "Gross margin\n",
      "75.0%\n",
      "71.2%\n",
      "56.1%\n",
      "Up 3.8 pts\n",
      "Up 18.9 pts\n",
      "\n",
      "\n",
      "Operating expenses\n",
      "$2,026\n",
      "$1,838\n",
      "$1,793\n",
      "Up 10%\n",
      "Up 13%\n",
      "\n",
      "\n",
      "Operating income\n",
      "$11,557\n",
      "$7,776\n",
      "$1,536\n",
      "Up 49%\n",
      "Up 652%\n",
      "\n",
      "\n",
      "Net income\n",
      "$10,020\n",
      "$6,740\n",
      "$1,456\n",
      "Up 49%\n",
      "Up 588%\n",
      "\n",
      "\n",
      "Diluted earnings per share\n",
      "$4.02\n",
      "$2.70\n",
      "$0.58\n",
      "Up 49%\n",
      "Up 593%\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Outlook\n",
      "NVIDIA’s outlook for the fourth quarter of fiscal 2024 is as follows:\n",
      "\n",
      "About NVIDIA\n",
      "Since its founding in 1993, NVIDIA (NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling industrial digitalization across markets. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information at https://nvidianews.nvidia.com/.\n",
      "Certain statements in this press release including, but not limited to, statements as to: the broad industry platform transition from general-purpose to accelerated computing and generative AI and the next waves starting to build; nations and regional CSPs investing in AI clouds to serve local demand, enterprise software companies adding AI copilots and assistants to their platforms, and enterprises creating custom AI to automate the world’s largest industries; NVIDIA GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software as growth engines for generative AI; the era of generative AI taking off; the NVIDIA AI foundry service accelerating the development and tuning of custom generative AI applications; the usage of NVIDIA GH200 Superchips in supercomputers globally; NVIDIA’s next quarterly cash dividend; NVIDIA’s financial outlook and expected tax rates for the fourth quarter of fiscal 2024; the benefits, impact, performance, features and availability of NVIDIA’s products and technologies, including the NVIDIA HGX H200, NVIDIA H200 Tensor Core GPU, NVIDIA AI Foundation Models, NVIDIA NeMo, NVIDIA GH200 Grace Hopper Superchip, NVIDIA AI Enterprise, NVIDIA Omniverse, NVIDIA Spectrum-X, NVIDIA RTX workstations, NVIDIA RTX 6000 Ada GPU, NVIDIA Omniverse Enterprise software, NVIDIA H100 Tensor Core GPU, NVIDIA DGX Cloud AI, GeForce NOW, NVIDIA CUDA Quantum platform, DLSS, DLSS 3.5 Ray Reconstruction, NVIDIA DRIVE Hyperion, NVIDIA DRIVE Thor, TensorRT-LLM, NVIDIA ConnectX; the benefits and impact of NVIDIA’s partnerships with Amdocs, Dropbox, Foxconn, Genentech, Infosys, Lenovo, Reliance Industries, Scaleway and Tata Group; growing support for NVIDIA CUDA Quantum platform; and the usage of NVIDIA Omniverse by Mercedes-Benz are forward-looking statements that are subject to risks and uncertainties that could cause results to be materially different than expectations. Important factors that could cause actual results to differ materially include: global economic conditions; our reliance on third parties to manufacture, assemble, package and test our products; the impact of technological development and competition; development of new products and technologies or enhancements to our existing product and technologies; market acceptance of our products or our partners’ products; design, manufacturing or software defects; changes in consumer preferences or demands; changes in industry standards and interfaces; and unexpected loss of performance of our products or technologies when integrated into systems, as well as other factors detailed from time to time in the most recent reports NVIDIA files with the Securities and Exchange Commission, or SEC, including, but not limited to, its annual report on Form 10-K and quarterly reports on Form 10-Q. Copies of reports filed with the SEC are posted on the company’s website and are available from NVIDIA without charge. These forward-looking statements are not guarantees of future performance and speak only as of the date hereof, and, except as required by law, NVIDIA disclaims any obligation to update these forward-looking statements to reflect future events or circumstances.\n",
      "Human: What are NVIDIA's latest revenues?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"NVIDIA's latest revenues for the third quarter of fiscal 2024 were $18.12 billion.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "chain.run(nvidia_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45725146-2a92-4a32-be7c-9d1c5214f69a",
   "metadata": {},
   "source": [
    "### 回答のソースとなるドキュメントを取得\n",
    "回答のソースとなるドキュメントは `return_source_cocuments=True` とすると取得できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e275d463-a778-417f-b2f9-6b6c708a8ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Professional Visualization\n",
      "\n",
      "Third-quarter revenue was $416 million, up 10% from the previous quarter and up 108% from a year ago.\n",
      "Announced that Mercedes-Benz is using NVIDIA Omniverse to create digital twins to help plan, design, build and operate its manufacturing and assembly facilities around the world.\n",
      "Announced a new line of desktop workstations with NVIDIA RTX™ 6000 Ada Generation GPUs and NVIDIA ConnectX® smart interface cards for training smaller AI models, fine-tuning models and running inference locally.\n",
      "\n",
      "Automotive \n",
      "\n",
      "Third-quarter revenue was $261 million, up 3% from the previous quarter and up 4% from a year ago.\n",
      "Furthered its collaboration with Foxconn to develop next-generation electric vehicles for the global market, using the next-generation NVIDIA DRIVE Hyperion™ platform and NVIDIA DRIVE Thor™ system-on-a-chip.\n",
      "\n",
      "CFO Commentary\n",
      "Commentary on the quarter by Colette Kress, NVIDIA’s executive vice president and chief financial officer, is available at https://investor.nvidia.com.\n",
      "Conference Call and Webcast Information\n",
      "NVIDIA will conduct a conference call with analysts and investors to discuss its third quarter fiscal 2024 financial results and current financial prospects today at 2 p.m. Pacific time (5 p.m. Eastern time). A live webcast (listen-only mode) of the conference call will be accessible at NVIDIA’s investor relations website, https://investor.nvidia.com. The webcast will be recorded and available for replay until NVIDIA’s conference call to discuss its financial results for its fourth quarter and fiscal 2024.\n",
      "Non-GAAP Measures\n",
      "To supplement NVIDIA’s condensed consolidated financial statements presented in accordance with GAAP, the company uses non-GAAP measures of certain components of financial performance. These non-GAAP measures include non-GAAP gross profit, non-GAAP gross margin, non-GAAP operating expenses, non-GAAP income from operations, non-GAAP other income (expense), net, non-GAAP net income, non-GAAP net income, or earnings, per diluted share, and free cash flow. For NVIDIA’s investors to be better able to compare its current results with those of previous periods, the company has shown a reconciliation of GAAP to non-GAAP financial measures. These reconciliations adjust the related GAAP financial measures to exclude acquisition termination costs, stock-based compensation expense, acquisition-related and other costs, IP-related costs, other, gains and losses from non-affiliated investments, interest expense related to amortization of debt discount, and the associated tax impact of these items where applicable. Free cash flow is calculated as GAAP net cash provided by operating activities less both purchases of property and equipment and intangible assets and principal payments on property and equipment and intangible assets. NVIDIA believes the presentation of its non-GAAP financial measures enhances the user’s overall understanding of the company’s historical financial performance. The presentation of the company’s non-GAAP financial measures is not meant to be considered in isolation or as a substitute for the company’s financial results prepared in accordance with GAAP, and the company’s non-GAAP measures may be different from non-GAAP measures used by other companies.\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "NVIDIA CORPORATION\n",
      "\n",
      "\n",
      " CONDENSED CONSOLIDATED STATEMENTS OF INCOME\n",
      "\n",
      "\n",
      "(In millions, except per share data)\n",
      "\n",
      "\n",
      "(Unaudited)\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "Three Months Ended\n",
      " \n",
      "Nine Months Ended\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "October 29,\n",
      " \n",
      "October 30,\n",
      " \n",
      "October 29,\n",
      " \n",
      "October 30,\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " 2023 \n",
      " \n",
      " 2022 \n",
      " \n",
      " 2023 \n",
      " \n",
      " 2022 \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "Revenue\n",
      "$\n",
      "18,120\n",
      " \n",
      " \n",
      "$\n",
      "5,931\n",
      " \n",
      " \n",
      "$\n",
      "38,819\n",
      " \n",
      " \n",
      "$\n",
      "20,923\n",
      " \n",
      "\n",
      "\n",
      "Cost of revenue\n",
      " \n",
      "4,720\n",
      " \n",
      " \n",
      " \n",
      "2,754\n",
      " \n",
      " \n",
      " \n",
      "11,309\n",
      " \n",
      " \n",
      " \n",
      "9,400\n",
      " \n",
      "\n",
      "\n",
      "Gross profit\n",
      " \n",
      "13,400\n",
      " \n",
      " \n",
      " \n",
      "3,177\n",
      " \n",
      " \n",
      " \n",
      "27,510\n",
      " \n",
      " \n",
      " \n",
      "11,523\n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "Operating expenses\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "Research and development\n",
      " \n",
      "2,294\n",
      " \n",
      " \n",
      " \n",
      "1,945\n",
      " \n",
      " \n",
      " \n",
      "6,210\n",
      " \n",
      " \n",
      " \n",
      "5,387\n",
      "\n",
      "Diluted earnings per share\n",
      "$4.02\n",
      "$2.70\n",
      "$0.58\n",
      "Up 49%\n",
      "Up 593%\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Outlook\n",
      "NVIDIA’s outlook for the fourth quarter of fiscal 2024 is as follows:\n",
      "\n",
      "Revenue is expected to be $20.00 billion, plus or minus 2%.\n",
      "GAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points.\n",
      "GAAP and non-GAAP operating expenses are expected to be approximately $3.17 billion and $2.20 billion, respectively.\n",
      "GAAP and non-GAAP other income and expense are expected to be an income of approximately $200 million, excluding gains and losses from non-affiliated investments.\n",
      "GAAP and non-GAAP tax rates are expected to be 15.0%, plus or minus 1%, excluding any discrete items.\n",
      "\n",
      "Highlights \n",
      "NVIDIA achieved progress since its previous earnings announcement in these areas: \n",
      "Data Center \n",
      "\n",
      "Third-quarter revenue was a record $14.51 billion, up 41% from the previous quarter and up 279% from a year ago.\n",
      "Announced NVIDIA HGX™ H200 with the new NVIDIA H200 Tensor Core GPU, the first GPU with HBM3e memory, with systems expected to be available in the second quarter of next year.\n",
      "Introduced an AI foundry service — with NVIDIA AI Foundation Models, NVIDIA NeMo™ framework and NVIDIA DGX™ Cloud AI supercomputing — to accelerate the development and tuning of custom generative AI applications, first available on Microsoft Azure, with SAP and Amdocs among the first customers.\n",
      "Announced that the NVIDIA Spectrum-X™ Ethernet networking platform for AI will be integrated into servers from Dell Technologies, Hewlett Packard Enterprise and Lenovo in the first quarter of next year.\n",
      "Announced that NVIDIA GH200 Grace Hopper Superchips, including a new quad configuration, will power more than 40 new supercomputers, including the JUPITER system at Jülich Supercomputing Centre and Isambard-AI at the University of Bristol.\n",
      "Made advances with global cloud service providers:\n",
      "\t\n",
      "Google Cloud Platform made generally available new A3 instances powered by NVIDIA H100 Tensor Core GPUs and NVIDIA AI Enterprise software in Google Cloud Marketplace.\n",
      "Microsoft Azure will be offering customers access to NVIDIA Omniverse™ Cloud Services for accelerating automotive digitalization, as well as new instances featuring NVL H100 Tensor Core GPUs and H100 with confidential computing, with H200 GPUs coming next year.\n",
      "Oracle Cloud Infrastructure made NVIDIA DGX Cloud and NVIDIA AI Enterprise software available in Oracle Cloud Marketplace.\n",
      "\n",
      "\n",
      "Partnered with a range of leading companies on AI initiatives, including Amdocs, Dropbox, Foxconn, Genentech (member of Roche Group), Infosys, Lenovo, Reliance Industries, Scaleway and Tata Group.\n",
      "Announced record-setting performance in the latest two sets of MLPerf benchmarks for inference and training, with the NVIDIA Eos AI supercomputer training a GPT-3 model 3x faster than the previous record.\n",
      "Announced growing worldwide support for the NVIDIA® CUDA® Quantum platform, including new efforts in Israel, the Netherlands, the U.K. and the U.S.\n",
      "\n",
      "Gaming \n",
      "\n",
      "Third-quarter revenue was $2.86 billion, up 15% from the previous quarter and up 81% from a year ago. \n",
      "Launched DLSS 3.5 Ray Reconstruction, which creates high-quality ray-traced images for intensive ray-traced games and apps, including Alan Wake 2 and Cyberpunk 2077. \n",
      "Released TensorRT-LLM™ for Windows, speeding on-device LLM inference by up to 4x. \n",
      "Added 56 DLSS games and over 15 Reflex games, bringing the total number of RTX games and applications to over 475. \n",
      "Surpassed 1,700 games on GeForce NOW™, including launches of Alan Wake 2, Baldur’s Gate 3, Cyberpunk 2077: Phantom Liberty, Forza Motorsport and Starfield.\n",
      "\n",
      "Professional Visualization\n",
      "\n",
      "NVIDIA Announces Financial Results for Third Quarter Fiscal 2024 | NVIDIA Newsroom\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Artificial Intelligence Computing Leadership from NVIDIA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PLATFORMS\n",
      "\n",
      "\n",
      "  Autonomous Machines\n",
      "\n",
      "\n",
      "\n",
      "  Cloud & Data Center\n",
      "\n",
      "\n",
      "\n",
      "  Deep Learning & Ai\n",
      "\n",
      "\n",
      "\n",
      "  Design & Pro Visualization\n",
      "\n",
      "\n",
      "\n",
      "  Healthcare\n",
      "\n",
      "\n",
      "\n",
      "  High Performance Computing\n",
      "\n",
      "\n",
      "\n",
      "  Self-Driving Cars\n",
      "\n",
      "\n",
      "\n",
      "  Gaming & Entertainment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "other links\n",
      "\n",
      "\n",
      "Developers\n",
      "Industries\n",
      "Shop\n",
      "Drivers\n",
      "Support\n",
      "About NVIDIA\n",
      "\n",
      "\n",
      "View All Products\n",
      "GPU TECHNOLOGY CONFERENCE\n",
      "NVIDIA Blog\n",
      "Community\n",
      "Careers\n",
      "TECHNOLOGIES\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Newsroom\n",
      "\n",
      "\n",
      "\n",
      "NVIDIA in Brief\n",
      "\n",
      "\n",
      "Exec Bios\n",
      "\n",
      "\n",
      "NVIDIA Blog\n",
      "\n",
      "\n",
      "Podcast\n",
      "\n",
      "\n",
      "Media Assets\n",
      "\n",
      "\n",
      "In the News\n",
      "\n",
      "\n",
      "Press Contacts\n",
      "\n",
      "\n",
      "Online Press Kit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NVIDIA in Brief\n",
      "\n",
      "\n",
      "Exec Bios\n",
      "\n",
      "\n",
      "NVIDIA Blog\n",
      "\n",
      "\n",
      "Podcast\n",
      "\n",
      "\n",
      "Media Assets\n",
      "\n",
      "\n",
      "In the News\n",
      "\n",
      "\n",
      "Press Contacts\n",
      "\n",
      "\n",
      "Online Press Kit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Press Release\n",
      "    \n",
      "  \n",
      "\n",
      "Share\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tweet\n",
      "\n",
      "\n",
      "Twitter\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Share\n",
      "\n",
      "\n",
      "LinkedIn\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Share\n",
      "\n",
      "\n",
      "Facebook\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Email\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ic_arrow-back-to-top\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NVIDIA Announces Financial Results for Third Quarter Fiscal 2024\n",
      "\n",
      "          November 21, 2023\n",
      "        \n",
      "\n",
      "\n",
      "Record revenue of $18.12 billion, up 34% from Q2, up 206% from year ago\n",
      "Record Data Center revenue of $14.51 billion, up 41% from Q2, up 279% from year ago\n",
      "\n",
      "NVIDIA (NASDAQ: NVDA) today reported revenue for the third quarter ended October 29, 2023, of $18.12 billion, up 206% from a year ago and up 34% from the previous quarter.\n",
      "GAAP earnings per diluted share for the quarter were $3.71, up more than 12x from a year ago and up 50% from the previous quarter. Non-GAAP earnings per diluted share were $4.02, up nearly 6x from a year ago and up 49% from the previous quarter.\n",
      "“Our strong growth reflects the broad industry platform transition from general-purpose to accelerated computing and generative AI,” said Jensen Huang, founder and CEO of NVIDIA.\n",
      "“Large language model startups, consumer internet companies and global cloud service providers were the first movers, and the next waves are starting to build. Nations and regional CSPs are investing in AI clouds to serve local demand, enterprise software companies are adding AI copilots and assistants to their platforms, and enterprises are creating custom AI to automate the world’s largest industries.\n",
      "“NVIDIA GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software are all growth engines in full throttle. The era of generative AI is taking off,” he said.\n",
      "NVIDIA will pay its next quarterly cash dividend of $0.04 per share on December 28, 2023, to all shareholders of record on December 6, 2023.\n",
      "Q3 Fiscal 2024 Summary\n",
      "\n",
      "\n",
      "\n",
      "GAAP\n",
      "\n",
      "\n",
      "($ in millions, except earnings \n",
      "per share)\n",
      "Q3 FY24\n",
      "Q2 FY24\n",
      "Q3 FY23\n",
      "Q/Q\n",
      "Y/Y\n",
      "\n",
      "\n",
      "Revenue\n",
      "$18,120\n",
      "$13,507\n",
      "$5,931\n",
      "Up 34%\n",
      "Up 206%\n",
      "\n",
      "\n",
      "Gross margin\n",
      "74.0%\n",
      "70.1%\n",
      "53.6%\n",
      "Up 3.9 pts\n",
      "Up 20.4 pts\n",
      "\n",
      "\n",
      "Operating expenses\n",
      "$2,983\n",
      "$2,662\n",
      "$2,576\n",
      "Up 12%\n",
      "Up 16%\n",
      "\n",
      "\n",
      "Operating income\n",
      "$10,417\n",
      "$6,800\n",
      "$601\n",
      "Up 53%\n",
      "Up 1,633%\n",
      "\n",
      "\n",
      "Net income\n",
      "$9,243\n",
      "$6,188\n",
      "$680\n",
      "Up 49%\n",
      "Up 1,259%\n",
      "\n",
      "\n",
      "Diluted earnings per share\n",
      "$3.71\n",
      "$2.48\n",
      "$0.27\n",
      "Up 50%\n",
      "Up 1,274%\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Non-GAAP\n",
      "\n",
      "\n",
      "($ in millions, except earnings \n",
      "per share)\n",
      "Q3 FY24\n",
      "Q2 FY24\n",
      "Q3 FY23\n",
      "Q/Q\n",
      "Y/Y\n",
      "\n",
      "\n",
      "Revenue\n",
      "$18,120\n",
      "$13,507\n",
      "$5,931\n",
      "Up 34%\n",
      "Up 206%\n",
      "\n",
      "\n",
      "Gross margin\n",
      "75.0%\n",
      "71.2%\n",
      "56.1%\n",
      "Up 3.8 pts\n",
      "Up 18.9 pts\n",
      "\n",
      "\n",
      "Operating expenses\n",
      "$2,026\n",
      "$1,838\n",
      "$1,793\n",
      "Up 10%\n",
      "Up 13%\n",
      "\n",
      "\n",
      "Operating income\n",
      "$11,557\n",
      "$7,776\n",
      "$1,536\n",
      "Up 49%\n",
      "Up 652%\n",
      "\n",
      "\n",
      "Net income\n",
      "$10,020\n",
      "$6,740\n",
      "$1,456\n",
      "Up 49%\n",
      "Up 588%\n",
      "\n",
      "\n",
      "Diluted earnings per share\n",
      "$4.02\n",
      "$2.70\n",
      "$0.58\n",
      "Up 49%\n",
      "Up 593%\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Outlook\n",
      "NVIDIA’s outlook for the fourth quarter of fiscal 2024 is as follows:\n",
      "\n",
      "About NVIDIA\n",
      "Since its founding in 1993, NVIDIA (NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling industrial digitalization across markets. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information at https://nvidianews.nvidia.com/.\n",
      "Certain statements in this press release including, but not limited to, statements as to: the broad industry platform transition from general-purpose to accelerated computing and generative AI and the next waves starting to build; nations and regional CSPs investing in AI clouds to serve local demand, enterprise software companies adding AI copilots and assistants to their platforms, and enterprises creating custom AI to automate the world’s largest industries; NVIDIA GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software as growth engines for generative AI; the era of generative AI taking off; the NVIDIA AI foundry service accelerating the development and tuning of custom generative AI applications; the usage of NVIDIA GH200 Superchips in supercomputers globally; NVIDIA’s next quarterly cash dividend; NVIDIA’s financial outlook and expected tax rates for the fourth quarter of fiscal 2024; the benefits, impact, performance, features and availability of NVIDIA’s products and technologies, including the NVIDIA HGX H200, NVIDIA H200 Tensor Core GPU, NVIDIA AI Foundation Models, NVIDIA NeMo, NVIDIA GH200 Grace Hopper Superchip, NVIDIA AI Enterprise, NVIDIA Omniverse, NVIDIA Spectrum-X, NVIDIA RTX workstations, NVIDIA RTX 6000 Ada GPU, NVIDIA Omniverse Enterprise software, NVIDIA H100 Tensor Core GPU, NVIDIA DGX Cloud AI, GeForce NOW, NVIDIA CUDA Quantum platform, DLSS, DLSS 3.5 Ray Reconstruction, NVIDIA DRIVE Hyperion, NVIDIA DRIVE Thor, TensorRT-LLM, NVIDIA ConnectX; the benefits and impact of NVIDIA’s partnerships with Amdocs, Dropbox, Foxconn, Genentech, Infosys, Lenovo, Reliance Industries, Scaleway and Tata Group; growing support for NVIDIA CUDA Quantum platform; and the usage of NVIDIA Omniverse by Mercedes-Benz are forward-looking statements that are subject to risks and uncertainties that could cause results to be materially different than expectations. Important factors that could cause actual results to differ materially include: global economic conditions; our reliance on third parties to manufacture, assemble, package and test our products; the impact of technological development and competition; development of new products and technologies or enhancements to our existing product and technologies; market acceptance of our products or our partners’ products; design, manufacturing or software defects; changes in consumer preferences or demands; changes in industry standards and interfaces; and unexpected loss of performance of our products or technologies when integrated into systems, as well as other factors detailed from time to time in the most recent reports NVIDIA files with the Securities and Exchange Commission, or SEC, including, but not limited to, its annual report on Form 10-K and quarterly reports on Form 10-Q. Copies of reports filed with the SEC are posted on the company’s website and are available from NVIDIA without charge. These forward-looking statements are not guarantees of future performance and speak only as of the date hereof, and, except as required by law, NVIDIA disclaims any obligation to update these forward-looking statements to reflect future events or circumstances.\n",
      "Human: What are NVIDIA's latest revenues?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "res = chain(nvidia_question)   # return_source_documentsを使うときはrunができない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c2d9142-697c-40af-8afe-0eee813963c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res['source_documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f5803-4fa6-4b28-a836-84ea43a0ff0d",
   "metadata": {},
   "source": [
    "retrieverで検索するドキュメントはデフォルトで4なので、4つのドキュメントがソースとして返される。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ee60e-86db-4207-8860-9ee3d6d7af1e",
   "metadata": {},
   "source": [
    "retrieverが検索するドキュメントの数などはあとから変更することも可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3721fa53-5c50-47dd-a773-66fb3a4b45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.search_kwargs[\"k\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ae0cd12-d5e9-470e-8460-22220cd3f460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Diluted earnings per share\n",
      "$4.02\n",
      "$2.70\n",
      "$0.58\n",
      "Up 49%\n",
      "Up 593%\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Outlook\n",
      "NVIDIA’s outlook for the fourth quarter of fiscal 2024 is as follows:\n",
      "\n",
      "Revenue is expected to be $20.00 billion, plus or minus 2%.\n",
      "GAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points.\n",
      "GAAP and non-GAAP operating expenses are expected to be approximately $3.17 billion and $2.20 billion, respectively.\n",
      "GAAP and non-GAAP other income and expense are expected to be an income of approximately $200 million, excluding gains and losses from non-affiliated investments.\n",
      "GAAP and non-GAAP tax rates are expected to be 15.0%, plus or minus 1%, excluding any discrete items.\n",
      "\n",
      "Highlights \n",
      "NVIDIA achieved progress since its previous earnings announcement in these areas: \n",
      "Data Center \n",
      "\n",
      "Third-quarter revenue was a record $14.51 billion, up 41% from the previous quarter and up 279% from a year ago.\n",
      "Announced NVIDIA HGX™ H200 with the new NVIDIA H200 Tensor Core GPU, the first GPU with HBM3e memory, with systems expected to be available in the second quarter of next year.\n",
      "Introduced an AI foundry service — with NVIDIA AI Foundation Models, NVIDIA NeMo™ framework and NVIDIA DGX™ Cloud AI supercomputing — to accelerate the development and tuning of custom generative AI applications, first available on Microsoft Azure, with SAP and Amdocs among the first customers.\n",
      "Announced that the NVIDIA Spectrum-X™ Ethernet networking platform for AI will be integrated into servers from Dell Technologies, Hewlett Packard Enterprise and Lenovo in the first quarter of next year.\n",
      "Announced that NVIDIA GH200 Grace Hopper Superchips, including a new quad configuration, will power more than 40 new supercomputers, including the JUPITER system at Jülich Supercomputing Centre and Isambard-AI at the University of Bristol.\n",
      "Made advances with global cloud service providers:\n",
      "\t\n",
      "Google Cloud Platform made generally available new A3 instances powered by NVIDIA H100 Tensor Core GPUs and NVIDIA AI Enterprise software in Google Cloud Marketplace.\n",
      "Microsoft Azure will be offering customers access to NVIDIA Omniverse™ Cloud Services for accelerating automotive digitalization, as well as new instances featuring NVL H100 Tensor Core GPUs and H100 with confidential computing, with H200 GPUs coming next year.\n",
      "Oracle Cloud Infrastructure made NVIDIA DGX Cloud and NVIDIA AI Enterprise software available in Oracle Cloud Marketplace.\n",
      "\n",
      "\n",
      "Partnered with a range of leading companies on AI initiatives, including Amdocs, Dropbox, Foxconn, Genentech (member of Roche Group), Infosys, Lenovo, Reliance Industries, Scaleway and Tata Group.\n",
      "Announced record-setting performance in the latest two sets of MLPerf benchmarks for inference and training, with the NVIDIA Eos AI supercomputer training a GPT-3 model 3x faster than the previous record.\n",
      "Announced growing worldwide support for the NVIDIA® CUDA® Quantum platform, including new efforts in Israel, the Netherlands, the U.K. and the U.S.\n",
      "\n",
      "Gaming \n",
      "\n",
      "Third-quarter revenue was $2.86 billion, up 15% from the previous quarter and up 81% from a year ago. \n",
      "Launched DLSS 3.5 Ray Reconstruction, which creates high-quality ray-traced images for intensive ray-traced games and apps, including Alan Wake 2 and Cyberpunk 2077. \n",
      "Released TensorRT-LLM™ for Windows, speeding on-device LLM inference by up to 4x. \n",
      "Added 56 DLSS games and over 15 Reflex games, bringing the total number of RTX games and applications to over 475. \n",
      "Surpassed 1,700 games on GeForce NOW™, including launches of Alan Wake 2, Baldur’s Gate 3, Cyberpunk 2077: Phantom Liberty, Forza Motorsport and Starfield.\n",
      "\n",
      "Professional Visualization\n",
      "Human: How much is Nvidia's latest revenue?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "res = chain(\"How much is Nvidia's latest revenue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3dbada4-c0dd-4977-a6ce-67a37558106b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd45d3-2ff4-4568-8080-7a6fd6aa671b",
   "metadata": {},
   "source": [
    "ソースドキュメントの数が1になっていることがわかる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
