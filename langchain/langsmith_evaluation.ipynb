{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7809f17c-44bc-4f19-8863-4bc854340537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4f0cd4-3b87-42fe-aa4f-54b69028d157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env Variables\n",
      "LANGCHAIN_TRACING_V2: true\n",
      "LANGCHAIN_PROJECT: machine-learning-workshop\n",
      "LANGCHAIN_ENDPOINT: https://api.smith.langchain.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f\"\"\"Env Variables\n",
    "LANGCHAIN_TRACING_V2: {os.environ[\"LANGCHAIN_TRACING_V2\"]}\n",
    "LANGCHAIN_PROJECT: {os.environ[\"LANGCHAIN_PROJECT\"]}\n",
    "LANGCHAIN_ENDPOINT: {os.environ[\"LANGCHAIN_ENDPOINT\"]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45564955-2936-4c57-922c-69b525c0c4d0",
   "metadata": {},
   "source": [
    "## LangSmithのトレースの基本\n",
    "traceableデコレータを使用することで、任意の関数の引数と返り値をLangSmithで確認できるようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65c29549-322a-4d07-865f-3f4ec80d36b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "SELECT\n",
       "  user_id,\n",
       "  COUNT(DISTINCT page_id) AS pages_visited,\n",
       "  COUNT(DISTINCT session_id) AS sessions,\n",
       "  COUNT(DISTINCT page_id) / COUNT(DISTINCT session_id) AS page_views_per_session\n",
       "FROM\n",
       "  `your_dataset.your_table`\n",
       "GROUP BY\n",
       "  user_id\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langsmith import traceable\n",
    "import openai\n",
    "\n",
    "openai_client = openai.Client()\n",
    "\n",
    "\n",
    "@traceable\n",
    "def format_prompt(question):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"あなたはBigQueryのエキスパートです. 出したいデータのクエリを作成してください. 出力はクエリのみで他の情報は不要です.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def invoke_llm(messages):\n",
    "    return openai_client.chat.completions.create(\n",
    "        messages=messages, model=\"gpt-4o\", temperature=0\n",
    "    )\n",
    "\n",
    "\n",
    "@traceable\n",
    "def parse_output(response):\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "@traceable\n",
    "def run_pipeline():\n",
    "    messages = format_prompt(\"ウェブサイトの回遊率\")\n",
    "    response = invoke_llm(messages)\n",
    "    return parse_output(response)\n",
    "\n",
    "\n",
    "display(Markdown(run_pipeline()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147b87e-6380-4c7a-8aa8-e2ee0186a19e",
   "metadata": {},
   "source": [
    "openaiとのやり取りの可観測にするラッパー `wrap_openai` を使うと詳細な情報を簡単に取得可能になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a83e551-9053-4dbd-a30f-578b84698965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "SELECT\n",
       "  user_id,\n",
       "  COUNT(DISTINCT page_id) AS pages_visited,\n",
       "  COUNT(DISTINCT session_id) AS sessions,\n",
       "  COUNT(DISTINCT page_id) / COUNT(DISTINCT session_id) AS page_views_per_session\n",
       "FROM\n",
       "  `your_dataset.your_table`\n",
       "GROUP BY\n",
       "  user_id\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "wrap_openai_client = wrap_openai(openai.Client())\n",
    "\n",
    "\n",
    "@traceable(name=\"run_pipeline with wrap_openai\")\n",
    "def run_pipeline_with_wrap_llm():\n",
    "    messages = format_prompt(\"ウェブサイトの回遊率\")\n",
    "    response = wrap_openai_client.chat.completions.create(\n",
    "        messages=messages, model=\"gpt-4o\", temperature=0\n",
    "    )\n",
    "    return parse_output(response)\n",
    "\n",
    "\n",
    "display(Markdown(run_pipeline_with_wrap_llm()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326a818-99b9-4252-943a-e3729c14da4b",
   "metadata": {},
   "source": [
    "LCEL (LangChain Expression Language) を使えば、LangSmithでの観測が楽にできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db794be6-9d7d-4003-84a6-532324cd2627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "以下は、ユーザーの訪問回数とそのセッションの数をカウントするためのBigQuery SQLクエリです。\n",
       "\n",
       "```sql\n",
       "SELECT \n",
       "  fullVisitorId,\n",
       "  COUNT(DISTINCT visitId) as sessions,\n",
       "  COUNT(*) as total_page_views,\n",
       "  ROUND(COUNT(*) / COUNT(DISTINCT visitId), 2) as avg_page_views_per_session\n",
       "FROM \n",
       "  `project_id.dataset_id.table_id`\n",
       "GROUP BY \n",
       "  fullVisitorId\n",
       "ORDER BY \n",
       "  avg_page_views_per_session DESC\n",
       "```\n",
       "\n",
       "ここで、`project_id`, `dataset_id`, `table_id`は適切なプロジェクト、データセット、テーブルIDに置き換える必要があります。\n",
       "\n",
       "このクエリでは、各ユーザー（`fullVisitorId`）のセッション数（`visitId`）、総ページビュー数、およびセッションあたりの平均ページビュー数を計算します。これにより、ウェブサイトの回遊率を把握することができます。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"あなたはBigQueryのエキスパートです. 出したいデータのクエリを作成してください. 出力はクエリのみで他の情報は不要です.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "display(Markdown(chain.invoke(\"ウェブサイトの回遊率\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf00005-30e6-4f85-8878-bf98ba4ad8d7",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3859db3-efbd-407e-8aef-ed1fe2bc396b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a3f0de-4149-4343-b359-ec6c4fa8cb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'how can LangSmith help with testing?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'content': 'How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLet’s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLet’s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)'}, {'url': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'content': 'LangSmith Walkthrough. LangChain makes it easy to prototype LLM applications and Agents. However, delivering LLM applications to production can be deceptively difficult. You will have to iterate on your prompts, chains, and other components to build a high-quality product. LangSmith makes it easy to debug, test, and continuously improve your ...'}, {'url': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'content': 'This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf you’re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.'}, {'url': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'content': \"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\"}, {'url': 'https://python.langchain.com/v0.1/docs/get_started/quickstart/', 'content': 'To properly answer the original question (\"how can langsmith help with testing?\"), we need to provide additional context to the LLM. We can do this via retrieval. Retrieval is useful when you have too much data to pass to the LLM directly. You can then use a retriever to fetch only the most relevant pieces and pass those in.'}]\u001b[0m\u001b[32;1m\u001b[1;3mLangSmith is a platform designed to assist with the development, testing, and deployment of Language Learning Models (LLMs) and AI applications. Here are some ways LangSmith can help with testing:\n",
      "\n",
      "1. **Comprehensive Evaluators**: LangSmith offers built-in and custom evaluators for measuring the correctness of responses to prompts. This is particularly useful for labeled datasets, where you can get detailed metrics like contextual accuracy.\n",
      "\n",
      "2. **Debugging and Iteration**: LangSmith makes it easy to debug and iterate on your prompts, chains, and other components. This helps in refining the model to achieve higher quality outputs.\n",
      "\n",
      "3. **Automated Testing**: You can run multiple prompts and evaluate their outputs in a single line of code using functions like `run_on_dataset`. This streamlines the testing process and makes it more efficient.\n",
      "\n",
      "4. **Exception Handling**: LangSmith provides tools for handling exceptions during agent execution, ensuring that logs and traces are submitted in full before the program proceeds. This helps in identifying and fixing issues promptly.\n",
      "\n",
      "5. **Continuous Improvement**: The platform supports continuous testing and improvement of LLM applications, making it easier to maintain and enhance the quality of your models over time.\n",
      "\n",
      "6. **Real-world Examples**: The LangSmith Cookbook offers hands-on examples and code snippets to help you understand how to implement various testing and debugging strategies in your projects.\n",
      "\n",
      "For more detailed information, you can refer to the [LangSmith Walkthrough](https://python.langchain.com/v0.1/docs/langsmith/walkthrough/) and other resources available online.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can langsmith help with testing?',\n",
       " 'output': 'LangSmith is a platform designed to assist with the development, testing, and deployment of Language Learning Models (LLMs) and AI applications. Here are some ways LangSmith can help with testing:\\n\\n1. **Comprehensive Evaluators**: LangSmith offers built-in and custom evaluators for measuring the correctness of responses to prompts. This is particularly useful for labeled datasets, where you can get detailed metrics like contextual accuracy.\\n\\n2. **Debugging and Iteration**: LangSmith makes it easy to debug and iterate on your prompts, chains, and other components. This helps in refining the model to achieve higher quality outputs.\\n\\n3. **Automated Testing**: You can run multiple prompts and evaluate their outputs in a single line of code using functions like `run_on_dataset`. This streamlines the testing process and makes it more efficient.\\n\\n4. **Exception Handling**: LangSmith provides tools for handling exceptions during agent execution, ensuring that logs and traces are submitted in full before the program proceeds. This helps in identifying and fixing issues promptly.\\n\\n5. **Continuous Improvement**: The platform supports continuous testing and improvement of LLM applications, making it easier to maintain and enhance the quality of your models over time.\\n\\n6. **Real-world Examples**: The LangSmith Cookbook offers hands-on examples and code snippets to help you understand how to implement various testing and debugging strategies in your projects.\\n\\nFor more detailed information, you can refer to the [LangSmith Walkthrough](https://python.langchain.com/v0.1/docs/langsmith/walkthrough/) and other resources available online.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()\n",
    "tools = [search]\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bbb6e-ebce-4bcc-86e5-89260829c215",
   "metadata": {},
   "source": [
    "### Runの情報を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ade91553-80da-49ad-8417-45bc3c070287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'Langsmith testing features'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'content': 'How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLet’s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLet’s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)'}, {'url': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'content': 'This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf you’re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.'}, {'url': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'content': \"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\"}, {'url': 'https://docs.smith.langchain.com/tutorials/Developers/evaluation', 'content': \"Set up automated testing to run in CI/CD; For more information on the evaluation workflows LangSmith supports, check out the how-to guides. Lots to cover, let's dive in! Create a dataset The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:\"}, {'url': 'https://www.langchain.com/langsmith', 'content': \"BETA Sign Up\\nContact Sales\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter 🦜🔗 LangChain\\nLangSmith\\nLangServe\\nAgents\\nRetrieval\\nEvaluation\\nBlog\\nDocs\\n🦜🔗 LangChain\\nBuild and deploy LLM apps with confidence\\nAn all-in-one developer platform for every step of the application lifecycle.\\n Prompt playground\\nCross-team collaboration\\nCatalog of ranging models & tasks\\nProven prompting strategies\\nExplore LangChain Hub\\nTurn the magic of LLM applications into enterprise-ready products\\nNative collaboration\\nBring your team together in LangSmith to craft prompts, debug, and capture feedback.\\n Application-level usage stats\\nFeedback collection\\nFilter traces\\nCost measurement\\nPerformance comparison\\nGo To Docs\\nManage Prompts\\nPrompts power your team's chains and agents, and LangSmith allows you to refine, test, and version them in one place. Dataset curation\\nEvaluate chain performance\\nAI-assisted evaluation\\nEasy benchmarking\\nGo To Docs\\nMonitor\\nGiven the stochastic nature of LLMs, it can be hard to answer the simple question: “what’s happening with my application?”\"}]\u001b[0m\u001b[32;1m\u001b[1;3mLangsmith offers a variety of features to help with testing, debugging, evaluating, and monitoring Language Learning Models (LLMs) and AI applications. Here are some key ways Langsmith can assist with testing:\n",
      "\n",
      "1. **Automated Testing**: Langsmith supports setting up automated testing workflows that can run in CI/CD pipelines. This ensures that your models are consistently evaluated and tested as part of your development process.\n",
      "\n",
      "2. **Dataset Management**: You can create and manage datasets within Langsmith. This includes defining the data points you want to evaluate, which is crucial for structured testing and evaluation.\n",
      "\n",
      "3. **Built-in and Custom Evaluators**: Langsmith provides both built-in and custom evaluators to measure the correctness of responses to prompts. This is particularly useful for labeled datasets where you need to ensure the accuracy and relevance of the model's outputs.\n",
      "\n",
      "4. **Performance Metrics**: Langsmith allows you to track various performance metrics, such as contextual accuracy, cost measurement, and performance comparison. This helps in understanding how well your models are performing and where improvements are needed.\n",
      "\n",
      "5. **Feedback Collection**: The platform supports native collaboration features, enabling teams to collect feedback, debug, and refine prompts together. This collaborative approach helps in quickly identifying and resolving issues.\n",
      "\n",
      "6. **Prompt Management**: Langsmith provides tools to manage, refine, test, and version prompts. This is essential for maintaining the quality and consistency of the prompts used in your models.\n",
      "\n",
      "7. **Monitoring and Logging**: Given the stochastic nature of LLMs, Langsmith offers monitoring and logging features to help you understand what’s happening with your application. This includes capturing logs and traces to ensure that all activities are recorded and can be reviewed.\n",
      "\n",
      "8. **AI-Assisted Evaluation**: The platform includes AI-assisted evaluation tools that can help in benchmarking and evaluating the performance of your models more efficiently.\n",
      "\n",
      "For more detailed information, you can refer to the [Langsmith documentation](https://docs.smith.langchain.com/tutorials/Developers/evaluation) and other resources available on their [official website](https://www.langchain.com/langsmith).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import tracing_v2_enabled\n",
    "\n",
    "with tracing_v2_enabled() as cb:\n",
    "    agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea3b34c5-bab4-4c1b-b4f2-75b17268af71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/o/bd14a154-65e7-52b4-bdce-b9a16d5e3513/projects/p/dc1fff1d-e827-4ee4-b948-e71e0914f7dd/r/d0b9dc17-c299-42e7-9159-5ea2f1814f06?poll=true'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.get_run_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e246e6de-f5cc-4ed1-86f7-f181a9c8b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = cb.latest_run.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a495f-1e89-4e7c-a8de-d6323e7a9280",
   "metadata": {},
   "source": [
    "### Feedbackの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf65cbde-36c8-4d11-8aa5-d17b7798aa11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('964ca95e-9186-44a2-b0c4-36b63ec6fed2'), created_at=datetime.datetime(2024, 6, 25, 22, 35, 27, 683459, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 25, 22, 35, 27, 683463, tzinfo=datetime.timezone.utc), run_id=UUID('d0b9dc17-c299-42e7-9159-5ea2f1814f06'), key='test-score', score=1, value=None, comment=None, correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "ls_client = Client()\n",
    "\n",
    "score_feedback1 = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, key=\"test-score\", score=1\n",
    ")\n",
    "score_feedback1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3a1eb0a-a932-4e1d-9c4e-37364aa55ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('62276b5f-f154-4625-8973-87ee2214a9d6'), created_at=datetime.datetime(2024, 6, 25, 22, 35, 35, 947858, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 25, 22, 35, 35, 947862, tzinfo=datetime.timezone.utc), run_id=UUID('d0b9dc17-c299-42e7-9159-5ea2f1814f06'), key='test-score', score=10, value=None, comment=None, correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_feedback2 = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, key=\"test-score\", score=10\n",
    ")\n",
    "score_feedback2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d96f00ec-b857-47b4-b202-2c8505d0ca8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('835374bc-1867-44d7-867c-1f0471f5a491'), created_at=datetime.datetime(2024, 6, 25, 22, 35, 40, 463695, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 25, 22, 35, 40, 463697, tzinfo=datetime.timezone.utc), run_id=UUID('d0b9dc17-c299-42e7-9159-5ea2f1814f06'), key='test-comment', score=None, value=None, comment='test comment', correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_feedback = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, key=\"test-comment\", comment=\"test comment\"\n",
    ")\n",
    "comment_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f91dab-2c51-4cf4-9fd3-f860a4aa8cc0",
   "metadata": {},
   "source": [
    "### Feedbackの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46f03c34-c7d6-4970-bc1e-861ccc5829fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作成したfeedbackを削除\n",
    "ls_client.delete_feedback(score_feedback1.id)\n",
    "ls_client.delete_feedback(comment_feedback.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36130122-2fb6-4a93-b16c-4eba75acdb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# まとめて消去することも可能\n",
    "for feedback in ls_client.list_feedback(run_ids=[cb.latest_run.id]):\n",
    "    ls_client.delete_feedback(feedback.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12237ca-d2a0-463b-85f0-55658c4dd09b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9553e43f-b1e1-45a0-ac0e-faa11c2e6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from langsmith import Client\n",
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "ls_client = Client()  # LangSmithのクライアント\n",
    "\n",
    "# 作成するデータセット\n",
    "dataset_name = \"SQL Samples\"\n",
    "\n",
    "# データセットがあれば削除\n",
    "if ls_client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = ls_client.delete_dataset(dataset_name=dataset_name)\n",
    "\n",
    "dataset = ls_client.create_dataset(\n",
    "    dataset_name, description=\"ML Workshop用のサンプルクエリ\"\n",
    ")\n",
    "\n",
    "# データセットにexampleを保存\n",
    "ls_client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"MAUを取得\"},\n",
    "        {\"question\": \"新規ユーザ数の推移\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\n",
    "            \"query\": textwrap.dedent(\"\"\"\n",
    "           SELECT\n",
    "               COUNT(DISTINCT user_id) AS monthly_active_users\n",
    "           FROM\n",
    "               `your_dataset.user_activities`\n",
    "           WHERE\n",
    "               activity_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH) AND CURRENT_DATE()\n",
    "        \"\"\"),\n",
    "            \"tables\": [\"user_activities\"],\n",
    "        },\n",
    "        {\n",
    "            \"query\": textwrap.dedent(\"\"\"\n",
    "            SELECT\n",
    "                signup_date,\n",
    "                COUNT(user_id) AS new_users\n",
    "            FROM\n",
    "                `your_dataset.user_activities`\n",
    "            GROUP BY\n",
    "                signup_date\n",
    "            ORDER BY\n",
    "            　　 signup_date\n",
    "        \"\"\"),\n",
    "            \"tables\": [\"user_activities\"],\n",
    "        },\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c94cc301-2dc7-4c8d-8ed2-cb0bdec486f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(name='SQL Samples', description='ML Workshop用のサンプルクエリ', data_type=<DataType.kv: 'kv'>, id=UUID('b5c798dc-ea07-4213-97ae-7e2f3072513b'), created_at=datetime.datetime(2024, 6, 25, 22, 36, 11, 696756, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 25, 22, 36, 11, 696756, tzinfo=datetime.timezone.utc), example_count=0, session_count=0, last_session_start_time=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad151f48-5fed-436b-8221-808184fcf0bf",
   "metadata": {},
   "source": [
    "example_countが0となっているので、LangSmith Clientを使ってdatasetを読み直す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58a3b2c3-fe2b-467b-8444-e9772e6e8f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(name='SQL Samples', description='ML Workshop用のサンプルクエリ', data_type=<DataType.kv: 'kv'>, id=UUID('b5c798dc-ea07-4213-97ae-7e2f3072513b'), created_at=datetime.datetime(2024, 6, 25, 22, 36, 11, 696756, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 25, 22, 36, 11, 696756, tzinfo=datetime.timezone.utc), example_count=2, session_count=0, last_session_start_time=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_client.read_dataset(dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e76d30b6-f2cb-49d0-9986-b8f844b424c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/o/bd14a154-65e7-52b4-bdce-b9a16d5e3513/datasets/b5c798dc-ea07-4213-97ae-7e2f3072513b'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92a4743c-38d8-4ff6-87b5-77e32e8c1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"月ごとのCV数の推移\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\n",
    "            \"query\": textwrap.dedent(\"\"\"\n",
    "           SELECT\n",
    "               FORMAT_TIMESTAMP('%Y-%m', conv_date) AS conversion_month,\n",
    "               COUNT(conv_id) AS conversions\n",
    "           FROM\n",
    "               `your_dataset.your_table`\n",
    "           WHERE\n",
    "               conv_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR) AND CURRENT_DATE()\n",
    "           GROUP BY\n",
    "               conversion_month\n",
    "           ORDER BY\n",
    "               conversion_month\n",
    "        \"\"\"),\n",
    "            \"tables\": [\"user_activities\"],\n",
    "        },\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ca496-6ce7-46d6-9a55-0d795cfb15be",
   "metadata": {},
   "source": [
    "exampleを増やしたことにより、datasetのバージョンも変更されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e92f1240-bab6-44ef-a5dc-21f15a752b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_client.read_dataset(dataset_name=dataset_name).example_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8904cd3-8a18-4978-821f-c6d12e5a6728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "question: 月ごとのCV数の推移\n",
      "query: \n",
      "SELECT\n",
      "    FORMAT_TIMESTAMP('%Y-%m', conv_date) AS conversion_month,\n",
      "    COUNT(conv_id) AS conversions\n",
      "FROM\n",
      "    `your_dataset.your_table`\n",
      "WHERE\n",
      "    conv_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR) AND CURRENT_DATE()\n",
      "GROUP BY\n",
      "    conversion_month\n",
      "ORDER BY\n",
      "    conversion_month\n",
      "\n",
      "    \n",
      "\n",
      "question: MAUを取得\n",
      "query: \n",
      "SELECT\n",
      "    COUNT(DISTINCT user_id) AS monthly_active_users\n",
      "FROM\n",
      "    `your_dataset.user_activities`\n",
      "WHERE\n",
      "    activity_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH) AND CURRENT_DATE()\n",
      "\n",
      "    \n",
      "\n",
      "question: 新規ユーザ数の推移\n",
      "query: \n",
      "SELECT\n",
      "    signup_date,\n",
      "    COUNT(user_id) AS new_users\n",
      "FROM\n",
      "    `your_dataset.user_activities`\n",
      "GROUP BY\n",
      "    signup_date\n",
      "ORDER BY\n",
      "　　 signup_date\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# datasetに保存されているexampleの一覧\n",
    "for example in ls_client.list_examples(dataset_name=dataset_name):\n",
    "    print(f\"\"\"\n",
    "question: {example.inputs[\"question\"]}\n",
    "query: {example.outputs[\"query\"]}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64866e-8183-42bc-b9a5-a0a946579f61",
   "metadata": {},
   "source": [
    "## LangSmith Evaluation\n",
    "### Custom Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00e595a1-18e2-4de5-bfce-01c6a4dd2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'ml-workshop-5202c67c' at:\n",
      "https://smith.langchain.com/o/bd14a154-65e7-52b4-bdce-b9a16d5e3513/datasets/b5c798dc-ea07-4213-97ae-7e2f3072513b/compare?selectedSessions=7ccb23a7-364e-447f-ae54-9813a475ab54\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e942a45758348f58a0ef57c70404a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id: ed966304-d7af-4848-a81c-6f46974f31a3\n",
      "run id: 800fb134-2c72-455d-a9ae-0d209b684b3e\n",
      "\n",
      "['user_activities']\n",
      "```\n",
      "SELECT \n",
      "  DATE(user_activities.timestamp) AS date, \n",
      "  COUNT(DISTINCT user_activities.user_id) AS new_users \n",
      "FROM \n",
      "  `your_project.your_dataset.user_activities` \n",
      "WHERE \n",
      "  user_activities.is_new_user = TRUE \n",
      "GROUP BY \n",
      "  date \n",
      "ORDER BY \n",
      "  date ASC;\n",
      "```\n",
      "run id: aa7a22af-6896-4dc1-bb46-b330a8d77f04\n",
      "\n",
      "['user_activities']\n",
      "```sql\n",
      "SELECT \n",
      "  EXTRACT(YEAR FROM activity_date) AS year, \n",
      "  EXTRACT(MONTH FROM activity_date) AS month, \n",
      "  COUNT(DISTINCT user_id) AS MAU \n",
      "FROM \n",
      "  `project_id.dataset.user_activities` \n",
      "GROUP BY \n",
      "  year, \n",
      "  month \n",
      "ORDER BY \n",
      "  year DESC, \n",
      "  month DESC;\n",
      "```\n",
      "ここで、\"project_id\"と\"dataset\"は適切なプロジェクトIDとデータセットに置き換えてください。\n",
      "\n",
      "['user_activities']\n",
      "```\n",
      "SELECT \n",
      "  FORMAT_TIMESTAMP('%Y-%m', timestamp) AS Month,\n",
      "  COUNT(*) AS CV_Count\n",
      "FROM \n",
      "  `project.dataset.user_activities`\n",
      "WHERE \n",
      "  activity = 'CV'\n",
      "GROUP BY \n",
      "  Month\n",
      "ORDER BY \n",
      "  Month ASC\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# inputsにexampleが1つずつ渡される\n",
    "def predict(inputs: dict) -> dict:\n",
    "    model = ChatOpenAI(model=\"gpt-4\")\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"あなたはBigQueryのエキスパートです. 出したいデータのクエリを作成してください. 出力はクエリのみで他の情報は不要です.\",\n",
    "            ),\n",
    "            (\"human\", \"{question}, tableはuser_activitiesを使います.\"),\n",
    "        ]\n",
    "    )\n",
    "    output_parser = StrOutputParser()\n",
    "    llm = prompt | model | output_parser\n",
    "    return {\"output\": llm.invoke(inputs)}\n",
    "\n",
    "\n",
    "# Custom Evaluation\n",
    "def must_have_user_activities(run: Run, example: Example) -> dict:\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    print(f\"run id: {run.id}\\n\")\n",
    "    required = example.outputs.get(\"tables\") or []  # outputsのキー (tables) と合わせる\n",
    "    print(required)\n",
    "    print(prediction)\n",
    "    score = all(\n",
    "        phrase in prediction for phrase in required\n",
    "    )  # scoreは自分で定義したものでよい\n",
    "    return {\n",
    "        \"key\": \"must_have_user_activities\",\n",
    "        \"score\": score,\n",
    "        \"comment\": \"comment test\",\n",
    "    }  # key, score, commentを返す\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict,\n",
    "    data=dataset_name,  # The data to predict and grade over\n",
    "    evaluators=[must_have_user_activities],  # The evaluators to score the results\n",
    "    experiment_prefix=\"ml-workshop\",  # A prefix for your experiment names to easily identify them\n",
    "    metadata={\n",
    "        \"version\": \"1.0.0\",\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
