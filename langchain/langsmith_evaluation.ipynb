{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e45ac2",
   "metadata": {},
   "source": [
    "# LangSmithを使った評価\n",
    "LangSmithの基本的なトレーシングの方法と、feedbackやevaluationを使った評価を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7809f17c-44bc-4f19-8863-4bc854340537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a069ef0b",
   "metadata": {},
   "source": [
    "## LangSmith関連の環境変数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4f0cd4-3b87-42fe-aa4f-54b69028d157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env Variables\n",
      "LANGCHAIN_TRACING_V2: true\n",
      "LANGCHAIN_PROJECT: machine-learning-workshop\n",
      "LANGCHAIN_ENDPOINT: https://api.smith.langchain.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f\"\"\"Env Variables\n",
    "LANGCHAIN_TRACING_V2: {os.environ[\"LANGCHAIN_TRACING_V2\"]}\n",
    "LANGCHAIN_PROJECT: {os.environ[\"LANGCHAIN_PROJECT\"]}\n",
    "LANGCHAIN_ENDPOINT: {os.environ[\"LANGCHAIN_ENDPOINT\"]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45564955-2936-4c57-922c-69b525c0c4d0",
   "metadata": {},
   "source": [
    "## LangSmithのトレースの基本\n",
    "traceableデコレータを使用することで、任意の関数の引数と返り値をLangSmithで確認できるようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c29549-322a-4d07-865f-3f4ec80d36b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "SELECT\n",
       "  user_id,\n",
       "  COUNT(DISTINCT page_id) AS pages_visited,\n",
       "  COUNT(DISTINCT session_id) AS sessions,\n",
       "  COUNT(DISTINCT page_id) / COUNT(DISTINCT session_id) AS page_views_per_session\n",
       "FROM\n",
       "  `your_dataset.your_table`\n",
       "GROUP BY\n",
       "  user_id\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langsmith import traceable\n",
    "import openai\n",
    "\n",
    "openai_client = openai.Client()\n",
    "\n",
    "\n",
    "@traceable\n",
    "def format_prompt(question):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"あなたはBigQueryのエキスパートです. 出したいデータのクエリを作成してください. 出力はクエリのみで他の情報は不要です.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def invoke_llm(messages):\n",
    "    return openai_client.chat.completions.create(\n",
    "        messages=messages, model=\"gpt-4o\", temperature=0\n",
    "    )\n",
    "\n",
    "\n",
    "@traceable\n",
    "def parse_output(response):\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "@traceable\n",
    "def run_pipeline():\n",
    "    messages = format_prompt(\"ウェブサイトの回遊率\")\n",
    "    response = invoke_llm(messages)\n",
    "    return parse_output(response)\n",
    "\n",
    "\n",
    "display(Markdown(run_pipeline()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147b87e-6380-4c7a-8aa8-e2ee0186a19e",
   "metadata": {},
   "source": [
    "openaiとのやり取りの可観測にするラッパー `wrap_openai` を使うと、モデルの情報などを簡単に取得可能になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a83e551-9053-4dbd-a30f-578b84698965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "SELECT\n",
       "  user_id,\n",
       "  COUNT(DISTINCT page_id) AS pages_visited,\n",
       "  COUNT(DISTINCT session_id) AS sessions,\n",
       "  COUNT(DISTINCT page_id) / COUNT(DISTINCT session_id) AS page_views_per_session\n",
       "FROM\n",
       "  `your_dataset.your_table`\n",
       "GROUP BY\n",
       "  user_id\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "wrap_openai_client = wrap_openai(openai.Client())\n",
    "\n",
    "\n",
    "@traceable(name=\"run_pipeline with wrap_openai\")\n",
    "def run_pipeline_with_wrap_llm():\n",
    "    messages = format_prompt(\"ウェブサイトの回遊率\")\n",
    "    response = wrap_openai_client.chat.completions.create(\n",
    "        messages=messages, model=\"gpt-4o\", temperature=0\n",
    "    )\n",
    "    return parse_output(response)\n",
    "\n",
    "\n",
    "display(Markdown(run_pipeline_with_wrap_llm()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326a818-99b9-4252-943a-e3729c14da4b",
   "metadata": {},
   "source": [
    "LCEL (LangChain Expression Language) を使えば、LangSmithでの観測が楽にできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db794be6-9d7d-4003-84a6-532324cd2627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "以下は、BigQueryでウェブサイトの回遊率を計算するためのSQLクエリです。ここでは、`website_logs`というテーブルが存在し、それぞれのユーザーのセッションとページビューを記録していると仮定しています。\n",
       "\n",
       "```sql\n",
       "SELECT session_id, COUNT(DISTINCT page_id) AS page_views\n",
       "FROM `project_id.dataset_id.website_logs`\n",
       "GROUP BY session_id\n",
       "```\n",
       "\n",
       "このクエリは、各セッションで見られたページの数（page_views）を計算します。回遊率を計算するためには、1つ以上のページを見たセッションの割合を計算する必要があります。以下はそのためのクエリです。\n",
       "\n",
       "```sql\n",
       "WITH page_views_per_session AS (\n",
       "  SELECT session_id, COUNT(DISTINCT page_id) AS page_views\n",
       "  FROM `project_id.dataset_id.website_logs`\n",
       "  GROUP BY session_id\n",
       ")\n",
       "SELECT COUNTIF(page_views > 1) / COUNT(*) AS bounce_rate\n",
       "FROM page_views_per_session\n",
       "```\n",
       "\n",
       "このクエリは、最初のクエリの結果を使用して、ページビューが1以上のセッションの割合（bounce_rate）を計算します。ここで、`COUNTIF(page_views > 1)`はページビューが1以上のセッションの数を、`COUNT(*)`は全体のセッション数をそれぞれ計算します。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"あなたはBigQueryのエキスパートです. 出したいデータのクエリを作成してください. 出力はクエリのみで他の情報は不要です.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "display(Markdown(chain.invoke(\"ウェブサイトの回遊率\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7eeb0",
   "metadata": {},
   "source": [
    "## FeedbackとEvaluationを使った評価\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "769bd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a6ccde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgQVtMYW5nU21pdGhdIC0tPiBCW1VzZXIgYW5kIFByb2R1Y3QgVGVhbSBGZWVkYmFja10KICBBIC0tPiBDW1ByZXBhcmVkIERhdGFzZXQgRXhwZXJpbWVudHNdCgogIHN1YmdyYXBoIEJbRmVlZGJhY2tdCiAgICBCMVtBbm5vdGF0ZSBSdW5zXSAtLT4gQjJbU2F2ZSBGZWVkYmFja10KICBlbmQKCiAgc3ViZ3JhcGggQ1tFdmFsdWF0aW9uXQogICAgQzFbSW5wdXQgYW5kIE91dHB1dF0gLS0+IEMyW1J1biBFeHBlcmltZW50c10KICBlbmQKCiAgY2xhc3NEZWYgbWFpbiBmaWxsOiNmOWYsc3Ryb2tlOiMzMzMsc3Ryb2tlLXdpZHRoOjJweDsKICBjbGFzc0RlZiBzdWIgZmlsbDojYmJmLHN0cm9rZTojMzMzLHN0cm9rZS13aWR0aDoycHg7CiAgY2xhc3NEZWYgZGV0YWlsIGZpbGw6I2ZiMyxzdHJva2U6IzMzMyxzdHJva2Utd2lkdGg6MnB4OwoKICBjbGFzcyBBLEIsQyBtYWluOwogIGNsYXNzIEIxLEIyLEMxLEMyIGRldGFpbDsgICAKICAg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph TD\n",
    "  A[LangSmith] --> B[User and Product Team Feedback]\n",
    "  A --> C[Prepared Dataset Experiments]\n",
    "\n",
    "  subgraph B[Feedback]\n",
    "    B1[Annotate Runs] --> B2[Save Feedback]\n",
    "  end\n",
    "\n",
    "  subgraph C[Evaluation]\n",
    "    C1[Input and Output] --> C2[Run Experiments]\n",
    "  end\n",
    "\n",
    "  classDef main fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "  classDef sub fill:#bbf,stroke:#333,stroke-width:2px;\n",
    "  classDef detail fill:#fb3,stroke:#333,stroke-width:2px;\n",
    "\n",
    "  class A,B,C main;\n",
    "  class B1,B2,C1,C2 detail;   \n",
    "   \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf00005-30e6-4f85-8878-bf98ba4ad8d7",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "- Traceされた実行の中に含まれるRunに、自分で定義したTagやKeyをAnnotate\n",
    "  - trace_id1つに対して複数のrun_idが含まれる構造\n",
    "  - 最初のrun_idはtrace_idと同一\n",
    "- API経由のfeedbackではKey, 手動のfeedbackではTagでAnnotateする仕組みとなっているが、TagもKeyとして保存されている\n",
    "\"- ただし、API経由のfeedbackはrecordが追加・上書きできるのに対して、手動のfeedbackは上書きのみという違いがある\n",
    "\"- 数値データで同じキーのものは集計されて表示される\n",
    "- LLMアプリを使っているユーザからのフィードバックは、基本的にAPI経由の登録となる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3859db3-efbd-407e-8aef-ed1fe2bc396b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a3f0de-4149-4343-b359-ec6c4fa8cb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'how can LangSmith help with testing'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'content': 'This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf you’re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.'}, {'url': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'content': 'How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLet’s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLet’s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)'}, {'url': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'content': 'LangSmith Walkthrough. LangChain makes it easy to prototype LLM applications and Agents. However, delivering LLM applications to production can be deceptively difficult. You will have to iterate on your prompts, chains, and other components to build a high-quality product. LangSmith makes it easy to debug, test, and continuously improve your ...'}, {'url': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'content': \"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\"}, {'url': 'https://python.langchain.com/v0.1/docs/get_started/quickstart/', 'content': 'To properly answer the original question (\"how can langsmith help with testing?\"), we need to provide additional context to the LLM. We can do this via retrieval. Retrieval is useful when you have too much data to pass to the LLM directly. You can then use a retriever to fetch only the most relevant pieces and pass those in.'}]\u001b[0m\u001b[32;1m\u001b[1;3mLangSmith is a platform designed to assist with the development, debugging, testing, and continuous improvement of Language Learning Models (LLMs) and AI applications. Here are some ways LangSmith can help with testing:\n",
      "\n",
      "1. **Prototyping and Debugging**:\n",
      "   - LangSmith makes it easy to prototype LLM applications and agents. It provides tools to iterate on prompts, chains, and other components, which is crucial for building a high-quality product.\n",
      "   - The platform allows for detailed logging and tracing, helping developers identify and fix issues quickly.\n",
      "\n",
      "2. **Evaluating Labeled and Unlabeled Datasets**:\n",
      "   - LangSmith offers comprehensive evaluators for measuring the correctness of responses to prompts, especially useful for labeled datasets.\n",
      "   - For unlabeled datasets, built-in and custom evaluators written in natural language can be used to assess the performance of the models.\n",
      "\n",
      "3. **Automated Testing**:\n",
      "   - The platform supports running multiple test cases asynchronously, gathering results for further processing. This helps in efficiently testing various scenarios and inputs.\n",
      "   - LangSmith provides functions to load specific tools and models required for the task, making it easier to set up and run tests.\n",
      "\n",
      "4. **Continuous Improvement**:\n",
      "   - By using LangSmith, developers can continuously improve their models by iterating on different components and evaluating the performance using various criteria.\n",
      "   - The platform supports exporting predictions, evaluation results, and other information programmatically, which can be used for reporting and further analysis.\n",
      "\n",
      "5. **User Interface and API**:\n",
      "   - LangSmith offers a user-friendly interface to manage datasets, run tests, and view results.\n",
      "   - It also provides an API for more advanced users to interact programmatically, allowing for greater flexibility and automation in testing workflows.\n",
      "\n",
      "For more detailed information, you can refer to the [LangSmith Walkthrough](https://python.langchain.com/v0.1/docs/langsmith/walkthrough/) and other resources available on their official documentation and blog posts.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can langsmith help with testing?',\n",
       " 'output': 'LangSmith is a platform designed to assist with the development, debugging, testing, and continuous improvement of Language Learning Models (LLMs) and AI applications. Here are some ways LangSmith can help with testing:\\n\\n1. **Prototyping and Debugging**:\\n   - LangSmith makes it easy to prototype LLM applications and agents. It provides tools to iterate on prompts, chains, and other components, which is crucial for building a high-quality product.\\n   - The platform allows for detailed logging and tracing, helping developers identify and fix issues quickly.\\n\\n2. **Evaluating Labeled and Unlabeled Datasets**:\\n   - LangSmith offers comprehensive evaluators for measuring the correctness of responses to prompts, especially useful for labeled datasets.\\n   - For unlabeled datasets, built-in and custom evaluators written in natural language can be used to assess the performance of the models.\\n\\n3. **Automated Testing**:\\n   - The platform supports running multiple test cases asynchronously, gathering results for further processing. This helps in efficiently testing various scenarios and inputs.\\n   - LangSmith provides functions to load specific tools and models required for the task, making it easier to set up and run tests.\\n\\n4. **Continuous Improvement**:\\n   - By using LangSmith, developers can continuously improve their models by iterating on different components and evaluating the performance using various criteria.\\n   - The platform supports exporting predictions, evaluation results, and other information programmatically, which can be used for reporting and further analysis.\\n\\n5. **User Interface and API**:\\n   - LangSmith offers a user-friendly interface to manage datasets, run tests, and view results.\\n   - It also provides an API for more advanced users to interact programmatically, allowing for greater flexibility and automation in testing workflows.\\n\\nFor more detailed information, you can refer to the [LangSmith Walkthrough](https://python.langchain.com/v0.1/docs/langsmith/walkthrough/) and other resources available on their official documentation and blog posts.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()\n",
    "tools = [search]\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41b88c",
   "metadata": {},
   "source": [
    "LangSmith上で手動でFeedbackを付与することができる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bbb6e-ebce-4bcc-86e5-89260829c215",
   "metadata": {},
   "source": [
    "#### Runの情報を取得\n",
    "LangSmithのAPIを使ってFeedbackを作成するためには、run idを取得する必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ade91553-80da-49ad-8417-45bc3c070287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'how can LangSmith help with testing?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'content': 'How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLet’s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLet’s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)'}, {'url': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'content': 'LangSmith Walkthrough. LangChain makes it easy to prototype LLM applications and Agents. However, delivering LLM applications to production can be deceptively difficult. You will have to iterate on your prompts, chains, and other components to build a high-quality product. LangSmith makes it easy to debug, test, and continuously improve your ...'}, {'url': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'content': 'This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf you’re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.'}, {'url': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'content': \"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\"}, {'url': 'https://blog.langchain.dev/announcing-langsmith/', 'content': 'We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\nDebugging\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. As a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n A unified platform\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n'}]\u001b[0m\u001b[32;1m\u001b[1;3mLangSmith offers a variety of features that can significantly aid in testing, debugging, and improving Language Learning Models (LLMs) and AI applications. Here are some key ways LangSmith can help with testing:\n",
      "\n",
      "1. **Comprehensive Evaluation**:\n",
      "   - LangSmith provides built-in and custom evaluators for both labeled and unlabeled datasets. This allows for detailed measurement of the correctness and contextual accuracy of responses to prompts.\n",
      "\n",
      "2. **Debugging**:\n",
      "   - The platform offers full visibility into model inputs and outputs at every step in the chain of events. This makes it easier to identify and resolve issues by diving into specific runs that went wrong.\n",
      "\n",
      "3. **Dataset Management**:\n",
      "   - Users can create datasets from existing logs and use them immediately for testing and evaluation. This seamless connection between logging/debugging workflows and testing/evaluation workflows enhances efficiency.\n",
      "\n",
      "4. **Continuous Improvement**:\n",
      "   - LangSmith supports continuous iteration on prompts, chains, and other components, making it easier to build high-quality products. It also helps in tracking system-level performance (like latency and cost) and model/chain performance through feedback association with runs.\n",
      "\n",
      "5. **Unified Platform**:\n",
      "   - LangSmith serves as a single, fully-integrated hub for debugging, testing, evaluating, and monitoring LLM applications. This unified approach reduces the time required to debug and refine prompts, providing confidence for deployment.\n",
      "\n",
      "6. **Real-World Examples and Cookbook**:\n",
      "   - The LangSmith Cookbook offers hands-on examples and code snippets designed to inspire and assist in projects, making it easier to understand and implement best practices.\n",
      "\n",
      "7. **API Interaction**:\n",
      "   - LangSmith allows for the creation of clients to interact with its API, enabling automated and programmatic testing, evaluation, and logging.\n",
      "\n",
      "By leveraging these features, developers can ensure their LLM applications are robust, efficient, and ready for production. For more detailed information, you can refer to resources like the [LangSmith Walkthrough](https://python.langchain.com/v0.1/docs/langsmith/walkthrough/) and the [LangSmith Cookbook](https://cheatsheet.md/langchain-tutorials/langsmith.en).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import tracing_v2_enabled\n",
    "\n",
    "with tracing_v2_enabled() as cb:\n",
    "    agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea3b34c5-bab4-4c1b-b4f2-75b17268af71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/o/bd14a154-65e7-52b4-bdce-b9a16d5e3513/projects/p/dc1fff1d-e827-4ee4-b948-e71e0914f7dd/r/8cf28732-35e3-46aa-a408-548c35257ae0?poll=true'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RunのURLを取得\n",
    "cb.get_run_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e246e6de-f5cc-4ed1-86f7-f181a9c8b9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('8cf28732-35e3-46aa-a408-548c35257ae0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run IDを取得\n",
    "run_id = cb.latest_run.id\n",
    "run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a495f-1e89-4e7c-a8de-d6323e7a9280",
   "metadata": {},
   "source": [
    "#### Feedbackの作成\n",
    "run_idを指定してFeedbackを追加することができる。\n",
    "keyは自分で任意の値を指定することが可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf65cbde-36c8-4d11-8aa5-d17b7798aa11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('b368a5a7-ed79-48a7-bbb4-c3ed06585fc1'), created_at=datetime.datetime(2024, 6, 26, 1, 22, 58, 129345, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 26, 1, 22, 58, 129349, tzinfo=datetime.timezone.utc), run_id=UUID('8cf28732-35e3-46aa-a408-548c35257ae0'), key='test-score', score=1, value=None, comment=None, correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "ls_client = Client()\n",
    "\n",
    "score_feedback1 = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, key=\"test-score\", score=1\n",
    ")\n",
    "score_feedback1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a3dc6",
   "metadata": {},
   "source": [
    "同じキーを指定すると別のfeedbackとして保存される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3a1eb0a-a932-4e1d-9c4e-37364aa55ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('6a3e59d2-f57a-4352-a45e-9782d7a2abdc'), created_at=datetime.datetime(2024, 6, 26, 1, 29, 54, 589717, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 26, 1, 29, 54, 589718, tzinfo=datetime.timezone.utc), run_id=UUID('8cf28732-35e3-46aa-a408-548c35257ae0'), key='test-score', score=100, value=None, comment=None, correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_feedback2 = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, key=\"test-score\", score=100\n",
    ")\n",
    "score_feedback2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e323f9",
   "metadata": {},
   "source": [
    "feedback_idを指定して、上書きすることも可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36caba68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('b368a5a7-ed79-48a7-bbb4-c3ed06585fc1'), created_at=datetime.datetime(2024, 6, 26, 1, 32, 4, 807318, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 26, 1, 32, 4, 807322, tzinfo=datetime.timezone.utc), run_id=UUID('8cf28732-35e3-46aa-a408-548c35257ae0'), key='test-score', score=500, value=None, comment=None, correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_score_feedback1 = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, feedback_id=score_feedback1.id, key=\"test-score\", score=500\n",
    ")\n",
    "update_score_feedback1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d96f00ec-b857-47b4-b202-2c8505d0ca8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('74d0b074-05d7-476f-9097-35301f569d52'), created_at=datetime.datetime(2024, 6, 26, 1, 32, 16, 902910, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 26, 1, 32, 16, 902912, tzinfo=datetime.timezone.utc), run_id=UUID('8cf28732-35e3-46aa-a408-548c35257ae0'), key='test-comment', score=None, value=None, comment='test comment', correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_feedback = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, key=\"test-comment\", comment=\"test comment\"\n",
    ")\n",
    "comment_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f91dab-2c51-4cf4-9fd3-f860a4aa8cc0",
   "metadata": {},
   "source": [
    "#### Feedbackの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46f03c34-c7d6-4970-bc1e-861ccc5829fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作成したfeedbackを削除\n",
    "ls_client.delete_feedback(score_feedback1.id)\n",
    "ls_client.delete_feedback(comment_feedback.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36130122-2fb6-4a93-b16c-4eba75acdb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# まとめて消去することも可能\n",
    "for feedback in ls_client.list_feedback(run_ids=[cb.latest_run.id]):\n",
    "    ls_client.delete_feedback(feedback.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12237ca-d2a0-463b-85f0-55658c4dd09b",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9553e43f-b1e1-45a0-ac0e-faa11c2e6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from langsmith import Client\n",
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "ls_client = Client()  # LangSmithのクライアント\n",
    "\n",
    "# 作成するデータセット\n",
    "dataset_name = \"SQL Samples\"\n",
    "\n",
    "# データセットがあれば削除\n",
    "if ls_client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = ls_client.delete_dataset(dataset_name=dataset_name)\n",
    "\n",
    "dataset = ls_client.create_dataset(\n",
    "    dataset_name, description=\"ML Workshop用のサンプルクエリ\"\n",
    ")\n",
    "\n",
    "# データセットにexampleを保存\n",
    "ls_client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"MAUを取得\"},\n",
    "        {\"question\": \"新規ユーザ数の推移\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\n",
    "            \"query\": textwrap.dedent(\"\"\"\n",
    "           SELECT\n",
    "               COUNT(DISTINCT user_id) AS monthly_active_users\n",
    "           FROM\n",
    "               `your_dataset.user_activities`\n",
    "           WHERE\n",
    "               activity_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH) AND CURRENT_DATE()\n",
    "        \"\"\"),\n",
    "            \"tables\": [\"user_activities\"],\n",
    "        },\n",
    "        {\n",
    "            \"query\": textwrap.dedent(\"\"\"\n",
    "            SELECT\n",
    "                signup_date,\n",
    "                COUNT(user_id) AS new_users\n",
    "            FROM\n",
    "                `your_dataset.user_activities`\n",
    "            GROUP BY\n",
    "                signup_date\n",
    "            ORDER BY\n",
    "            　　 signup_date\n",
    "        \"\"\"),\n",
    "            \"tables\": [\"user_activities\"],\n",
    "        },\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c94cc301-2dc7-4c8d-8ed2-cb0bdec486f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(name='SQL Samples', description='ML Workshop用のサンプルクエリ', data_type=<DataType.kv: 'kv'>, id=UUID('b5c798dc-ea07-4213-97ae-7e2f3072513b'), created_at=datetime.datetime(2024, 6, 25, 22, 36, 11, 696756, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 25, 22, 36, 11, 696756, tzinfo=datetime.timezone.utc), example_count=0, session_count=0, last_session_start_time=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad151f48-5fed-436b-8221-808184fcf0bf",
   "metadata": {},
   "source": [
    "example_countが0となっているので、LangSmith Clientを使ってdatasetを読み直す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58a3b2c3-fe2b-467b-8444-e9772e6e8f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(name='SQL Samples', description='ML Workshop用のサンプルクエリ', data_type=<DataType.kv: 'kv'>, id=UUID('b5c798dc-ea07-4213-97ae-7e2f3072513b'), created_at=datetime.datetime(2024, 6, 25, 22, 36, 11, 696756, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 25, 22, 36, 11, 696756, tzinfo=datetime.timezone.utc), example_count=2, session_count=0, last_session_start_time=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_client.read_dataset(dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e76d30b6-f2cb-49d0-9986-b8f844b424c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/o/bd14a154-65e7-52b4-bdce-b9a16d5e3513/datasets/b5c798dc-ea07-4213-97ae-7e2f3072513b'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92a4743c-38d8-4ff6-87b5-77e32e8c1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"月ごとのCV数の推移\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\n",
    "            \"query\": textwrap.dedent(\"\"\"\n",
    "           SELECT\n",
    "               FORMAT_TIMESTAMP('%Y-%m', conv_date) AS conversion_month,\n",
    "               COUNT(conv_id) AS conversions\n",
    "           FROM\n",
    "               `your_dataset.your_table`\n",
    "           WHERE\n",
    "               conv_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR) AND CURRENT_DATE()\n",
    "           GROUP BY\n",
    "               conversion_month\n",
    "           ORDER BY\n",
    "               conversion_month\n",
    "        \"\"\"),\n",
    "            \"tables\": [\"user_activities\"],\n",
    "        },\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ca496-6ce7-46d6-9a55-0d795cfb15be",
   "metadata": {},
   "source": [
    "exampleを増やしたことにより、datasetのバージョンも変更されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e92f1240-bab6-44ef-a5dc-21f15a752b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_client.read_dataset(dataset_name=dataset_name).example_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8904cd3-8a18-4978-821f-c6d12e5a6728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "question: 月ごとのCV数の推移\n",
      "query: \n",
      "SELECT\n",
      "    FORMAT_TIMESTAMP('%Y-%m', conv_date) AS conversion_month,\n",
      "    COUNT(conv_id) AS conversions\n",
      "FROM\n",
      "    `your_dataset.your_table`\n",
      "WHERE\n",
      "    conv_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR) AND CURRENT_DATE()\n",
      "GROUP BY\n",
      "    conversion_month\n",
      "ORDER BY\n",
      "    conversion_month\n",
      "\n",
      "    \n",
      "\n",
      "question: MAUを取得\n",
      "query: \n",
      "SELECT\n",
      "    COUNT(DISTINCT user_id) AS monthly_active_users\n",
      "FROM\n",
      "    `your_dataset.user_activities`\n",
      "WHERE\n",
      "    activity_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH) AND CURRENT_DATE()\n",
      "\n",
      "    \n",
      "\n",
      "question: 新規ユーザ数の推移\n",
      "query: \n",
      "SELECT\n",
      "    signup_date,\n",
      "    COUNT(user_id) AS new_users\n",
      "FROM\n",
      "    `your_dataset.user_activities`\n",
      "GROUP BY\n",
      "    signup_date\n",
      "ORDER BY\n",
      "　　 signup_date\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# datasetに保存されているexampleの一覧\n",
    "for example in ls_client.list_examples(dataset_name=dataset_name):\n",
    "    print(f\"\"\"\n",
    "question: {example.inputs[\"question\"]}\n",
    "query: {example.outputs[\"query\"]}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64866e-8183-42bc-b9a5-a0a946579f61",
   "metadata": {},
   "source": [
    "## LangSmith Evaluation\n",
    "### Custom Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00e595a1-18e2-4de5-bfce-01c6a4dd2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'ml-workshop-5202c67c' at:\n",
      "https://smith.langchain.com/o/bd14a154-65e7-52b4-bdce-b9a16d5e3513/datasets/b5c798dc-ea07-4213-97ae-7e2f3072513b/compare?selectedSessions=7ccb23a7-364e-447f-ae54-9813a475ab54\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e942a45758348f58a0ef57c70404a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id: ed966304-d7af-4848-a81c-6f46974f31a3\n",
      "run id: 800fb134-2c72-455d-a9ae-0d209b684b3e\n",
      "\n",
      "['user_activities']\n",
      "```\n",
      "SELECT \n",
      "  DATE(user_activities.timestamp) AS date, \n",
      "  COUNT(DISTINCT user_activities.user_id) AS new_users \n",
      "FROM \n",
      "  `your_project.your_dataset.user_activities` \n",
      "WHERE \n",
      "  user_activities.is_new_user = TRUE \n",
      "GROUP BY \n",
      "  date \n",
      "ORDER BY \n",
      "  date ASC;\n",
      "```\n",
      "run id: aa7a22af-6896-4dc1-bb46-b330a8d77f04\n",
      "\n",
      "['user_activities']\n",
      "```sql\n",
      "SELECT \n",
      "  EXTRACT(YEAR FROM activity_date) AS year, \n",
      "  EXTRACT(MONTH FROM activity_date) AS month, \n",
      "  COUNT(DISTINCT user_id) AS MAU \n",
      "FROM \n",
      "  `project_id.dataset.user_activities` \n",
      "GROUP BY \n",
      "  year, \n",
      "  month \n",
      "ORDER BY \n",
      "  year DESC, \n",
      "  month DESC;\n",
      "```\n",
      "ここで、\"project_id\"と\"dataset\"は適切なプロジェクトIDとデータセットに置き換えてください。\n",
      "\n",
      "['user_activities']\n",
      "```\n",
      "SELECT \n",
      "  FORMAT_TIMESTAMP('%Y-%m', timestamp) AS Month,\n",
      "  COUNT(*) AS CV_Count\n",
      "FROM \n",
      "  `project.dataset.user_activities`\n",
      "WHERE \n",
      "  activity = 'CV'\n",
      "GROUP BY \n",
      "  Month\n",
      "ORDER BY \n",
      "  Month ASC\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# inputsにexampleが1つずつ渡される\n",
    "def predict(inputs: dict) -> dict:\n",
    "    model = ChatOpenAI(model=\"gpt-4\")\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"あなたはBigQueryのエキスパートです. 出したいデータのクエリを作成してください. 出力はクエリのみで他の情報は不要です.\",\n",
    "            ),\n",
    "            (\"human\", \"{question}, tableはuser_activitiesを使います.\"),\n",
    "        ]\n",
    "    )\n",
    "    output_parser = StrOutputParser()\n",
    "    llm = prompt | model | output_parser\n",
    "    return {\"output\": llm.invoke(inputs)}\n",
    "\n",
    "\n",
    "# Custom Evaluation\n",
    "def must_have_user_activities(run: Run, example: Example) -> dict:\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    print(f\"run id: {run.id}\\n\")\n",
    "    required = example.outputs.get(\"tables\") or []  # outputsのキー (tables) と合わせる\n",
    "    print(required)\n",
    "    print(prediction)\n",
    "    score = all(\n",
    "        phrase in prediction for phrase in required\n",
    "    )  # scoreは自分で定義したものでよい\n",
    "    return {\n",
    "        \"key\": \"must_have_user_activities\",\n",
    "        \"score\": score,\n",
    "        \"comment\": \"comment test\",\n",
    "    }  # key, score, commentを返す\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict,\n",
    "    data=dataset_name,  # The data to predict and grade over\n",
    "    evaluators=[must_have_user_activities],  # The evaluators to score the results\n",
    "    experiment_prefix=\"ml-workshop\",  # A prefix for your experiment names to easily identify them\n",
    "    metadata={\n",
    "        \"version\": \"1.0.0\",\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
