{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e45ac2",
   "metadata": {},
   "source": [
    "# LangSmithを使った評価\n",
    "LangSmithの基本的なトレーシングの方法と、feedbackやevaluationを使った評価を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7809f17c-44bc-4f19-8863-4bc854340537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a069ef0b",
   "metadata": {},
   "source": [
    "## LangSmith関連の環境変数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc4f0cd4-3b87-42fe-aa4f-54b69028d157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env Variables\n",
      "LANGCHAIN_TRACING_V2: true\n",
      "LANGCHAIN_PROJECT: machine-learning-workshop\n",
      "LANGCHAIN_ENDPOINT: https://api.smith.langchain.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f\"\"\"Env Variables\n",
    "LANGCHAIN_TRACING_V2: {os.environ[\"LANGCHAIN_TRACING_V2\"]}\n",
    "LANGCHAIN_PROJECT: {os.environ[\"LANGCHAIN_PROJECT\"]}\n",
    "LANGCHAIN_ENDPOINT: {os.environ[\"LANGCHAIN_ENDPOINT\"]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45564955-2936-4c57-922c-69b525c0c4d0",
   "metadata": {},
   "source": [
    "## LangSmithのトレースの基本\n",
    "traceableデコレータを使用することで、任意の関数の引数と返り値をLangSmithで確認できるようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c29549-322a-4d07-865f-3f4ec80d36b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "SELECT\n",
       "  (SUM(CASE WHEN pageviews > 1 THEN 1 ELSE 0 END) / COUNT(*)) AS bounce_rate\n",
       "FROM\n",
       "  `your_dataset.your_table`\n",
       "WHERE\n",
       "  session_id IS NOT NULL;\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langsmith import traceable\n",
    "import openai\n",
    "\n",
    "openai_client = openai.Client()\n",
    "\n",
    "\n",
    "@traceable\n",
    "def format_prompt(question):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"あなたはBigQueryのエキスパートです. 出したいデータのクエリを作成してください. 出力はクエリのみで他の情報は不要です.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def invoke_llm(messages):\n",
    "    return openai_client.chat.completions.create(\n",
    "        messages=messages, model=\"gpt-4o\", temperature=0\n",
    "    )\n",
    "\n",
    "\n",
    "@traceable\n",
    "def parse_output(response):\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "@traceable\n",
    "def run_pipeline():\n",
    "    messages = format_prompt(\"ウェブサイトの回遊率\")\n",
    "    response = invoke_llm(messages)\n",
    "    return parse_output(response)\n",
    "\n",
    "\n",
    "display(Markdown(run_pipeline()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147b87e-6380-4c7a-8aa8-e2ee0186a19e",
   "metadata": {},
   "source": [
    "openaiとのやり取りの可観測にするラッパー `wrap_openai` を使うと、モデルの情報などを簡単に取得可能になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a83e551-9053-4dbd-a30f-578b84698965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "SELECT\n",
       "  user_id,\n",
       "  COUNT(DISTINCT page_id) AS pages_visited,\n",
       "  COUNT(DISTINCT session_id) AS sessions,\n",
       "  COUNT(DISTINCT page_id) / COUNT(DISTINCT session_id) AS page_views_per_session\n",
       "FROM\n",
       "  `your_project.your_dataset.your_table`\n",
       "GROUP BY\n",
       "  user_id\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "wrap_openai_client = wrap_openai(openai.Client())\n",
    "\n",
    "\n",
    "@traceable(name=\"run_pipeline with wrap_openai\")\n",
    "def run_pipeline_with_wrap_llm():\n",
    "    messages = format_prompt(\"ウェブサイトの回遊率\")\n",
    "    response = wrap_openai_client.chat.completions.create(\n",
    "        messages=messages, model=\"gpt-4o\", temperature=0\n",
    "    )\n",
    "    return parse_output(response)\n",
    "\n",
    "\n",
    "display(Markdown(run_pipeline_with_wrap_llm()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326a818-99b9-4252-943a-e3729c14da4b",
   "metadata": {},
   "source": [
    "LCEL (LangChain Expression Language) を使えば、LangSmithでの観測が楽にできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db794be6-9d7d-4003-84a6-532324cd2627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "下記のクエリは、ウェブサイトの各セッションにおけるページビュー数を計算します。これは、ウェブサイトの回遊率を理解するための一つの方法です。\n",
       "\n",
       "```\n",
       "SELECT\n",
       "  sessionId,\n",
       "  COUNT(pageViewId) as pageViews\n",
       "FROM\n",
       "  `project.dataset.table`\n",
       "GROUP BY\n",
       "  sessionId\n",
       "```\n",
       "\n",
       "ここで、`project.dataset.table`はあなたのBigQueryデータセットの具体的な場所を指します。また、`sessionId` と `pageViewId` はセッションIDとページビューIDを追跡するためのフィールド名を指します。これらのフィールド名は、あなたの具体的なデータ構造に基づいて変更する必要があるかもしれません。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"あなたはBigQueryのエキスパートです. 出したいデータのクエリを作成してください. 出力はクエリのみで他の情報は不要です.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "display(Markdown(chain.invoke(\"ウェブサイトの回遊率\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7eeb0",
   "metadata": {},
   "source": [
    "## FeedbackとEvaluationを使った評価\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "769bd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a6ccde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgQVtMYW5nU21pdGhdIC0tPiBCW1VzZXIgYW5kIFByb2R1Y3QgVGVhbSBGZWVkYmFja10KICBBIC0tPiBDW1ByZXBhcmVkIERhdGFzZXQgRXhwZXJpbWVudHNdCgogIHN1YmdyYXBoIEJbRmVlZGJhY2tdCiAgICBCMVtBbm5vdGF0ZSBSdW5zXSAtLT4gQjJbU2F2ZSBGZWVkYmFja10KICBlbmQKCiAgc3ViZ3JhcGggQ1tFdmFsdWF0aW9uXQogICAgQzFbSW5wdXQgYW5kIE91dHB1dF0gLS0+IEMyW1J1biBFeHBlcmltZW50c10KICBlbmQKCiAgY2xhc3NEZWYgbWFpbiBmaWxsOiNmOWYsc3Ryb2tlOiMzMzMsc3Ryb2tlLXdpZHRoOjJweDsKICBjbGFzc0RlZiBzdWIgZmlsbDojYmJmLHN0cm9rZTojMzMzLHN0cm9rZS13aWR0aDoycHg7CiAgY2xhc3NEZWYgZGV0YWlsIGZpbGw6I2ZiMyxzdHJva2U6IzMzMyxzdHJva2Utd2lkdGg6MnB4OwoKICBjbGFzcyBBLEIsQyBtYWluOwogIGNsYXNzIEIxLEIyLEMxLEMyIGRldGFpbDsgICAKICAg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph TD\n",
    "  A[LangSmith] --> B[User and Product Team Feedback]\n",
    "  A --> C[Prepared Dataset Experiments]\n",
    "\n",
    "  subgraph B[Feedback]\n",
    "    B1[Annotate Runs] --> B2[Save Feedback]\n",
    "  end\n",
    "\n",
    "  subgraph C[Evaluation]\n",
    "    C1[Input and Output] --> C2[Run Experiments]\n",
    "  end\n",
    "\n",
    "  classDef main fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "  classDef sub fill:#bbf,stroke:#333,stroke-width:2px;\n",
    "  classDef detail fill:#fb3,stroke:#333,stroke-width:2px;\n",
    "\n",
    "  class A,B,C main;\n",
    "  class B1,B2,C1,C2 detail;   \n",
    "   \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf00005-30e6-4f85-8878-bf98ba4ad8d7",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "- Traceされた実行の中に含まれるRunに、自分で定義したTagやKeyをAnnotate\n",
    "  - trace_id1つに対して複数のrun_idが含まれる構造\n",
    "  - 最初のrun_idはtrace_idと同一\n",
    "- API経由のfeedbackではKey, 手動のfeedbackではTagでAnnotateする仕組みとなっているが、TagもKeyとして保存されている\n",
    "\"- ただし、API経由のfeedbackはrecordが追加・上書きできるのに対して、手動のfeedbackは上書きのみという違いがある\n",
    "\"- 数値データで同じキーのものは集計されて表示される\n",
    "- LLMアプリを使っているユーザからのフィードバックは、基本的にAPI経由の登録となる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3859db3-efbd-407e-8aef-ed1fe2bc396b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9a3f0de-4149-4343-b359-ec6c4fa8cb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'Langsmith testing features'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'content': 'This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf you’re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.'}, {'url': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'content': 'How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLet’s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLet’s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)'}, {'url': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'content': \"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\"}, {'url': 'https://www.langchain.com/langsmith', 'content': \"BETA Sign Up\\nContact Sales\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter 🦜🔗 LangChain\\nLangSmith\\nLangServe\\nAgents\\nRetrieval\\nEvaluation\\nBlog\\nDocs\\n🦜🔗 LangChain\\nBuild and deploy LLM apps with confidence\\nAn all-in-one developer platform for every step of the application lifecycle.\\n Prompt playground\\nCross-team collaboration\\nCatalog of ranging models & tasks\\nProven prompting strategies\\nExplore LangChain Hub\\nTurn the magic of LLM applications into enterprise-ready products\\nNative collaboration\\nBring your team together in LangSmith to craft prompts, debug, and capture feedback.\\n Application-level usage stats\\nFeedback collection\\nFilter traces\\nCost measurement\\nPerformance comparison\\nGo To Docs\\nManage Prompts\\nPrompts power your team's chains and agents, and LangSmith allows you to refine, test, and version them in one place. Dataset curation\\nEvaluate chain performance\\nAI-assisted evaluation\\nEasy benchmarking\\nGo To Docs\\nMonitor\\nGiven the stochastic nature of LLMs, it can be hard to answer the simple question: “what’s happening with my application?”\"}, {'url': 'https://docs.smith.langchain.com/tutorials/Developers/evaluation', 'content': \"Set up automated testing to run in CI/CD; For more information on the evaluation workflows LangSmith supports, check out the how-to guides. Lots to cover, let's dive in! Create a dataset The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:\"}]\u001b[0m\u001b[32;1m\u001b[1;3mLangsmith offers a variety of features to help with testing, debugging, and evaluating Language Learning Models (LLMs) and AI applications. Here are some key aspects:\n",
      "\n",
      "1. **Automated Testing**: Langsmith supports setting up automated testing workflows that can be integrated into CI/CD pipelines. This ensures that your models are consistently evaluated and tested as part of your development process.\n",
      "\n",
      "2. **Dataset Creation and Management**: You can create and manage datasets within Langsmith. This includes labeled datasets for more comprehensive evaluation and unlabeled datasets for general testing.\n",
      "\n",
      "3. **Evaluation Metrics**: Langsmith provides built-in and custom evaluators to measure the correctness and performance of your models. This includes metrics like contextual accuracy and other custom criteria.\n",
      "\n",
      "4. **Prompt Management**: Langsmith allows you to refine, test, and version your prompts in one place. This helps in managing and improving the prompts that power your models.\n",
      "\n",
      "5. **Performance Monitoring**: The platform offers tools to monitor application-level usage stats, collect feedback, filter traces, measure costs, and compare performance. This helps in understanding the behavior and efficiency of your models in real-time.\n",
      "\n",
      "6. **Collaboration**: Langsmith supports cross-team collaboration, enabling teams to work together on crafting prompts, debugging issues, and capturing feedback.\n",
      "\n",
      "7. **Debugging and Logging**: The platform provides detailed logging and tracing capabilities to help debug issues and ensure that logs and traces are submitted in full before the program proceeds.\n",
      "\n",
      "8. **AI-Assisted Evaluation**: Langsmith includes AI-assisted evaluation tools to benchmark and evaluate the performance of your models easily.\n",
      "\n",
      "For more detailed information, you can refer to the [Langsmith documentation](https://docs.smith.langchain.com/tutorials/Developers/evaluation) and other resources available on their [official website](https://www.langchain.com/langsmith).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can langsmith help with testing?',\n",
       " 'output': 'Langsmith offers a variety of features to help with testing, debugging, and evaluating Language Learning Models (LLMs) and AI applications. Here are some key aspects:\\n\\n1. **Automated Testing**: Langsmith supports setting up automated testing workflows that can be integrated into CI/CD pipelines. This ensures that your models are consistently evaluated and tested as part of your development process.\\n\\n2. **Dataset Creation and Management**: You can create and manage datasets within Langsmith. This includes labeled datasets for more comprehensive evaluation and unlabeled datasets for general testing.\\n\\n3. **Evaluation Metrics**: Langsmith provides built-in and custom evaluators to measure the correctness and performance of your models. This includes metrics like contextual accuracy and other custom criteria.\\n\\n4. **Prompt Management**: Langsmith allows you to refine, test, and version your prompts in one place. This helps in managing and improving the prompts that power your models.\\n\\n5. **Performance Monitoring**: The platform offers tools to monitor application-level usage stats, collect feedback, filter traces, measure costs, and compare performance. This helps in understanding the behavior and efficiency of your models in real-time.\\n\\n6. **Collaboration**: Langsmith supports cross-team collaboration, enabling teams to work together on crafting prompts, debugging issues, and capturing feedback.\\n\\n7. **Debugging and Logging**: The platform provides detailed logging and tracing capabilities to help debug issues and ensure that logs and traces are submitted in full before the program proceeds.\\n\\n8. **AI-Assisted Evaluation**: Langsmith includes AI-assisted evaluation tools to benchmark and evaluate the performance of your models easily.\\n\\nFor more detailed information, you can refer to the [Langsmith documentation](https://docs.smith.langchain.com/tutorials/Developers/evaluation) and other resources available on their [official website](https://www.langchain.com/langsmith).'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()\n",
    "tools = [search]\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41b88c",
   "metadata": {},
   "source": [
    "LangSmith上で手動でFeedbackを付与することができる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bbb6e-ebce-4bcc-86e5-89260829c215",
   "metadata": {},
   "source": [
    "#### Runの情報を取得\n",
    "LangSmithのAPIを使ってFeedbackを作成するためには、run idを取得する必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ade91553-80da-49ad-8417-45bc3c070287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'Langsmith testing features'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'content': 'How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLet’s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLet’s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)'}, {'url': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'content': 'This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf you’re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.'}, {'url': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'content': \"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\"}, {'url': 'https://docs.smith.langchain.com/tutorials/Developers/evaluation', 'content': \"Set up automated testing to run in CI/CD; For more information on the evaluation workflows LangSmith supports, check out the how-to guides. Lots to cover, let's dive in! Create a dataset The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:\"}, {'url': 'https://www.langchain.com/langsmith', 'content': \"BETA Sign Up\\nContact Sales\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter 🦜🔗 LangChain\\nLangSmith\\nLangServe\\nAgents\\nRetrieval\\nEvaluation\\nBlog\\nDocs\\n🦜🔗 LangChain\\nBuild and deploy LLM apps with confidence\\nAn all-in-one developer platform for every step of the application lifecycle.\\n Prompt playground\\nCross-team collaboration\\nCatalog of ranging models & tasks\\nProven prompting strategies\\nExplore LangChain Hub\\nTurn the magic of LLM applications into enterprise-ready products\\nNative collaboration\\nBring your team together in LangSmith to craft prompts, debug, and capture feedback.\\n Application-level usage stats\\nFeedback collection\\nFilter traces\\nCost measurement\\nPerformance comparison\\nGo To Docs\\nManage Prompts\\nPrompts power your team's chains and agents, and LangSmith allows you to refine, test, and version them in one place. Dataset curation\\nEvaluate chain performance\\nAI-assisted evaluation\\nEasy benchmarking\\nGo To Docs\\nMonitor\\nGiven the stochastic nature of LLMs, it can be hard to answer the simple question: “what’s happening with my application?”\"}]\u001b[0m\u001b[32;1m\u001b[1;3mLangsmith offers a variety of features to help with testing, debugging, evaluating, and monitoring Language Learning Models (LLMs) and AI applications. Here are some key aspects:\n",
      "\n",
      "1. **Automated Testing**: Langsmith supports setting up automated testing workflows that can be integrated into CI/CD pipelines. This ensures that your models are consistently evaluated and tested as part of your development process.\n",
      "\n",
      "2. **Evaluation Metrics**: Langsmith provides built-in and custom evaluators to measure the correctness of responses to prompts. This includes metrics like contextual accuracy and other custom criteria that can be defined based on your specific needs.\n",
      "\n",
      "3. **Dataset Management**: You can create and manage datasets within Langsmith, allowing you to run tests on labeled and unlabeled data. This helps in evaluating the performance of your models across different types of data.\n",
      "\n",
      "4. **Feedback and Monitoring**: Langsmith allows you to collect feedback, monitor application-level usage stats, filter traces, and measure costs. This helps in understanding the performance and efficiency of your models in real-world scenarios.\n",
      "\n",
      "5. **Prompt Management**: Langsmith provides tools to refine, test, and version prompts. This is crucial for ensuring that your prompts are optimized and effective in generating the desired responses from your models.\n",
      "\n",
      "6. **Cross-Team Collaboration**: Langsmith supports native collaboration features, enabling teams to work together in crafting prompts, debugging issues, and capturing feedback. This fosters a collaborative environment for developing and refining LLM applications.\n",
      "\n",
      "7. **Real-World Examples and Tutorials**: Langsmith offers a cookbook of real-world examples and tutorials to help you get started and make the most of its features. This includes hands-on examples and code snippets to guide you through various use cases.\n",
      "\n",
      "For more detailed information, you can refer to the [Langsmith documentation](https://docs.smith.langchain.com/tutorials/Developers/evaluation) and other resources available on their [official website](https://www.langchain.com/langsmith).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import tracing_v2_enabled\n",
    "\n",
    "with tracing_v2_enabled() as cb:\n",
    "    agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea3b34c5-bab4-4c1b-b4f2-75b17268af71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/o/bd14a154-65e7-52b4-bdce-b9a16d5e3513/projects/p/dc1fff1d-e827-4ee4-b948-e71e0914f7dd/r/73c37ddb-d255-443d-a136-35eee99e5a58?poll=true'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RunのURLを取得\n",
    "cb.get_run_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e246e6de-f5cc-4ed1-86f7-f181a9c8b9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('73c37ddb-d255-443d-a136-35eee99e5a58')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run IDを取得\n",
    "run_id = cb.latest_run.id\n",
    "run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a495f-1e89-4e7c-a8de-d6323e7a9280",
   "metadata": {},
   "source": [
    "#### Feedbackの作成\n",
    "run_idを指定してFeedbackを追加することができる。\n",
    "keyは自分で任意の値を指定することが可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf65cbde-36c8-4d11-8aa5-d17b7798aa11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('28f1db89-a530-4d41-beb4-0c143a9c8b95'), created_at=datetime.datetime(2024, 6, 26, 1, 52, 2, 559765, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 26, 1, 52, 2, 559768, tzinfo=datetime.timezone.utc), run_id=UUID('73c37ddb-d255-443d-a136-35eee99e5a58'), key='test-score', score=1, value=None, comment=None, correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "ls_client = Client()\n",
    "\n",
    "score_feedback1 = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, key=\"test-score\", score=1\n",
    ")\n",
    "score_feedback1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a3dc6",
   "metadata": {},
   "source": [
    "同じキーを指定すると別のfeedbackとして保存される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3a1eb0a-a932-4e1d-9c4e-37364aa55ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('a210e562-3f1d-44b4-8711-46d2699a7dfb'), created_at=datetime.datetime(2024, 6, 26, 1, 52, 3, 237451, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 26, 1, 52, 3, 237463, tzinfo=datetime.timezone.utc), run_id=UUID('73c37ddb-d255-443d-a136-35eee99e5a58'), key='test-score', score=100, value=None, comment=None, correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_feedback2 = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, key=\"test-score\", score=100\n",
    ")\n",
    "score_feedback2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e323f9",
   "metadata": {},
   "source": [
    "feedback_idを指定して、上書きすることも可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36caba68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('28f1db89-a530-4d41-beb4-0c143a9c8b95'), created_at=datetime.datetime(2024, 6, 26, 1, 52, 3, 910193, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 26, 1, 52, 3, 910200, tzinfo=datetime.timezone.utc), run_id=UUID('73c37ddb-d255-443d-a136-35eee99e5a58'), key='test-score', score=500, value=None, comment=None, correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_score_feedback1 = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, feedback_id=score_feedback1.id, key=\"test-score\", score=500\n",
    ")\n",
    "update_score_feedback1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d96f00ec-b857-47b4-b202-2c8505d0ca8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('81bf1761-efef-404c-9bc0-20dfaa319e23'), created_at=datetime.datetime(2024, 6, 26, 1, 52, 5, 112288, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 26, 1, 52, 5, 112292, tzinfo=datetime.timezone.utc), run_id=UUID('73c37ddb-d255-443d-a136-35eee99e5a58'), key='test-comment', score=None, value=None, comment='test comment', correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_feedback = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, key=\"test-comment\", comment=\"test comment\"\n",
    ")\n",
    "comment_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f91dab-2c51-4cf4-9fd3-f860a4aa8cc0",
   "metadata": {},
   "source": [
    "#### Feedbackの削除\n",
    "feedback_idを指定して、作成したfeedbackを削除することも可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46f03c34-c7d6-4970-bc1e-861ccc5829fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作成したfeedbackを削除\n",
    "ls_client.delete_feedback(score_feedback1.id)\n",
    "ls_client.delete_feedback(comment_feedback.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36130122-2fb6-4a93-b16c-4eba75acdb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_feedbackでfeedbackをすべて取得し、削除\n",
    "for feedback in ls_client.list_feedback(run_ids=[cb.latest_run.id]):\n",
    "    ls_client.delete_feedback(feedback.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12237ca-d2a0-463b-85f0-55658c4dd09b",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "- DatasetにあらかじめInputとOutputの組み合わせからなるExampleを保存\n",
    "- ExampleのInputを使ってLLMを実行し、出てきたOutputを保存されているOutputを使って評価\n",
    "- 評価の方法は、LangSmithがあらかじめ用意しているものか、カスタムで作成することができる\n",
    "- 評価結果は、key (評価指標の名前), score (評価結果), commentとして残すことが可能\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccc0f086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgc3ViZ3JhcGggRXZhbHVhdGlvblBpcGVsaW5lW0V2YWx1YXRpb24gUGlwZWxpbmVdCiAgICBCW0RhdGFzZXRzXSAtLT4gQ1tFeGFtcGxlc10KICAgIEMgLS0+IERbSW5wdXRzXQogICAgQyAtLT4gRVtFeHBlY3RlZCBPdXRwdXRzXQogICAgRCAtLT4gRltMTE1dCiAgICBGIC0tPiBHW1J1biBPdXRwdXRzXQogICAgRSAtLT4gSFtFdmFsdWF0b3JzXQogICAgRyAtLT4gSAogICAgSCAtLT4gSVtFdmFsdWF0aW9uIFJlc3VsdF0KCiAgICBzdWJncmFwaCBJW0V2YWx1YXRpb24gUmVzdWx0XQogICAgICBkaXJlY3Rpb24gTFIKICAgICAgSTFbS2V5OiBtZXRyaWMgbmFtZV0gLS4tIEkyW1Njb3JlOiBtZXRyaWMgdmFsdWVdIC0uLSBJM1tDb21tZW50OiByZWFzb25pbmddCiAgICAgIEkzIC0uLSBJMQogICAgZW5kCiAgZW5kCgogIGNsYXNzRGVmIG1haW4gZmlsbDojZjlmLHN0cm9rZTojMzMzLHN0cm9rZS13aWR0aDoycHg7CiAgY2xhc3NEZWYgc3ViIGZpbGw6I2JiZixzdHJva2U6IzMzMyxzdHJva2Utd2lkdGg6MnB4OwogIGNsYXNzRGVmIGRldGFpbCBmaWxsOiNmYjMsc3Ryb2tlOiMzMzMsc3Ryb2tlLXdpZHRoOjJweDsKCiAgY2xhc3MgQixDLEQsRSxGLEcsSCxJIG1haW47CiAgY2xhc3MgSTEsSTIsSTMgZGV0YWlsOyAgIAogICA=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph TD\n",
    "  subgraph EvaluationPipeline[Evaluation Pipeline]\n",
    "    B[Datasets] --> C[Examples]\n",
    "    C --> D[Inputs]\n",
    "    C --> E[Expected Outputs]\n",
    "    D --> F[LLM]\n",
    "    F --> G[Run Outputs]\n",
    "    E --> H[Evaluators]\n",
    "    G --> H\n",
    "    H --> I[Evaluation Result]\n",
    "\n",
    "    subgraph I[Evaluation Result]\n",
    "      direction LR\n",
    "      I1[Key: metric name] -.- I2[Score: metric value] -.- I3[Comment: reasoning]\n",
    "      I3 -.- I1\n",
    "    end\n",
    "  end\n",
    "\n",
    "  classDef main fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "  classDef sub fill:#bbf,stroke:#333,stroke-width:2px;\n",
    "  classDef detail fill:#fb3,stroke:#333,stroke-width:2px;\n",
    "\n",
    "  class B,C,D,E,F,G,H,I main;\n",
    "  class I1,I2,I3 detail;   \n",
    "   \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc834a",
   "metadata": {},
   "source": [
    "#### DatasetとExampleを作成\n",
    "Datasetを作成し、Exampleを保存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9553e43f-b1e1-45a0-ac0e-faa11c2e6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from langsmith import Client\n",
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "ls_client = Client()  # LangSmithのクライアント\n",
    "\n",
    "# 作成するデータセット\n",
    "dataset_name = \"SQL Samples\"\n",
    "\n",
    "# データセットがあれば削除\n",
    "if ls_client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = ls_client.delete_dataset(dataset_name=dataset_name)\n",
    "\n",
    "dataset = ls_client.create_dataset(\n",
    "    dataset_name, description=\"ML Workshop用のサンプルクエリ\"\n",
    ")\n",
    "\n",
    "# データセットにexampleを保存\n",
    "ls_client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"MAUを取得\"},\n",
    "        {\"question\": \"新規ユーザ数の推移\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\n",
    "            \"query\": textwrap.dedent(\"\"\"\n",
    "           SELECT\n",
    "               COUNT(DISTINCT user_id) AS monthly_active_users\n",
    "           FROM\n",
    "               `your_dataset.user_activities`\n",
    "           WHERE\n",
    "               activity_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH) AND CURRENT_DATE()\n",
    "        \"\"\"),\n",
    "            \"tables\": [\"user_activities\"],\n",
    "        },\n",
    "        {\n",
    "            \"query\": textwrap.dedent(\"\"\"\n",
    "            SELECT\n",
    "                signup_date,\n",
    "                COUNT(user_id) AS new_users\n",
    "            FROM\n",
    "                `your_dataset.user_activities`\n",
    "            GROUP BY\n",
    "                signup_date\n",
    "            ORDER BY\n",
    "            　　 signup_date\n",
    "        \"\"\"),\n",
    "            \"tables\": [\"user_activities\"],\n",
    "        },\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90a464ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/o/bd14a154-65e7-52b4-bdce-b9a16d5e3513/datasets/ec7e16e3-d0fe-4aa9-9b24-241327e6f1ce'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasetのURLを取得\n",
    "dataset.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c94cc301-2dc7-4c8d-8ed2-cb0bdec486f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasetのexample数を取得\n",
    "dataset.example_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad151f48-5fed-436b-8221-808184fcf0bf",
   "metadata": {},
   "source": [
    "example_countが0となっているので、LangSmith Clientを使ってdatasetを読み直す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58a3b2c3-fe2b-467b-8444-e9772e6e8f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ls_client.read_dataset(dataset_name=dataset_name)\n",
    "dataset.example_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92a4743c-38d8-4ff6-87b5-77e32e8c1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"月ごとのCV数の推移\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\n",
    "            \"query\": textwrap.dedent(\"\"\"\n",
    "           SELECT\n",
    "               FORMAT_TIMESTAMP('%Y-%m', conv_date) AS conversion_month,\n",
    "               COUNT(conv_id) AS conversions\n",
    "           FROM\n",
    "               `your_dataset.your_table`\n",
    "           WHERE\n",
    "               conv_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR) AND CURRENT_DATE()\n",
    "           GROUP BY\n",
    "               conversion_month\n",
    "           ORDER BY\n",
    "               conversion_month\n",
    "        \"\"\"),\n",
    "            \"tables\": [\"user_activities\"],\n",
    "        },\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ca496-6ce7-46d6-9a55-0d795cfb15be",
   "metadata": {},
   "source": [
    "exampleを増やしたことにより、datasetのバージョンも変更されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e92f1240-bab6-44ef-a5dc-21f15a752b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "modified_at: 2024-06-26 01:52:09.950582+00:00\n",
      "example_count: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = ls_client.read_dataset(dataset_name=dataset_name)\n",
    "print(f\"\"\"\n",
    "modified_at: {dataset.modified_at}\n",
    "example_count: {dataset.example_count}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8904cd3-8a18-4978-821f-c6d12e5a6728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "question: 月ごとのCV数の推移\n",
      "query: \n",
      "SELECT\n",
      "    FORMAT_TIMESTAMP('%Y-%m', conv_date) AS conversion_month,\n",
      "    COUNT(conv_id) AS conversions\n",
      "FROM\n",
      "    `your_dataset.your_table`\n",
      "WHERE\n",
      "    conv_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR) AND CURRENT_DATE()\n",
      "GROUP BY\n",
      "    conversion_month\n",
      "ORDER BY\n",
      "    conversion_month\n",
      "\n",
      "    \n",
      "\n",
      "question: MAUを取得\n",
      "query: \n",
      "SELECT\n",
      "    COUNT(DISTINCT user_id) AS monthly_active_users\n",
      "FROM\n",
      "    `your_dataset.user_activities`\n",
      "WHERE\n",
      "    activity_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH) AND CURRENT_DATE()\n",
      "\n",
      "    \n",
      "\n",
      "question: 新規ユーザ数の推移\n",
      "query: \n",
      "SELECT\n",
      "    signup_date,\n",
      "    COUNT(user_id) AS new_users\n",
      "FROM\n",
      "    `your_dataset.user_activities`\n",
      "GROUP BY\n",
      "    signup_date\n",
      "ORDER BY\n",
      "　　 signup_date\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# datasetに保存されているexampleの一覧\n",
    "for example in ls_client.list_examples(dataset_name=dataset_name):\n",
    "    print(f\"\"\"\n",
    "question: {example.inputs[\"question\"]}\n",
    "query: {example.outputs[\"query\"]}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64866e-8183-42bc-b9a5-a0a946579f61",
   "metadata": {},
   "source": [
    "### Custom Evaluationを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00e595a1-18e2-4de5-bfce-01c6a4dd2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'ml-workshop-97d77c3e' at:\n",
      "https://smith.langchain.com/o/bd14a154-65e7-52b4-bdce-b9a16d5e3513/datasets/ec7e16e3-d0fe-4aa9-9b24-241327e6f1ce/compare?selectedSessions=335a9383-c555-4d54-b4e2-658a4bc17c8c\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15140305e0e74aab99a299af03e35c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id: cf15f7b3-6b9d-41fa-a94e-704a2f64e5b5\n",
      "run id: 602a44ee-fd61-4818-b532-0cc4feb9a80c\n",
      "\n",
      "['user_activities']\n",
      "```\n",
      "SELECT \n",
      "  EXTRACT(YEAR FROM activity_date) AS year,\n",
      "  EXTRACT(MONTH FROM activity_date) AS month,\n",
      "  COUNT(DISTINCT user_id) AS MAU\n",
      "FROM \n",
      "  `project_id.dataset_id.user_activities`\n",
      "GROUP BY \n",
      "  year, \n",
      "  month\n",
      "ORDER BY \n",
      "  year DESC, \n",
      "  month DESC\n",
      "```\n",
      "\n",
      "['user_activities']\n",
      "```\n",
      "SELECT \n",
      "  DATE(created_at) AS date, \n",
      "  COUNT(DISTINCT user_id) AS new_user_count\n",
      "FROM \n",
      "  user_activities\n",
      "WHERE \n",
      "  new_user = TRUE\n",
      "GROUP BY \n",
      "  date\n",
      "ORDER BY \n",
      "  date\n",
      "```\n",
      "run id: e9428f05-a949-4fea-af40-e52872d75d5f\n",
      "\n",
      "['user_activities']\n",
      "```\n",
      "SELECT \n",
      "  FORMAT_TIMESTAMP('%Y-%m', TIMESTAMP_SECONDS(time)) as Month,\n",
      "  COUNT(*) as CV \n",
      "FROM \n",
      "  `project.dataset.user_activities` \n",
      "WHERE \n",
      "  activity_type = 'conversion'\n",
      "GROUP BY \n",
      "  Month\n",
      "ORDER BY \n",
      "  Month\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# inputsにexampleが1つずつ渡される\n",
    "def predict(inputs: dict) -> dict:\n",
    "    model = ChatOpenAI(model=\"gpt-4\")\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"あなたはBigQueryのエキスパートです. 出したいデータのクエリを作成してください. 出力はクエリのみで他の情報は不要です.\",\n",
    "            ),\n",
    "            (\"human\", \"{question}, tableはuser_activitiesを使います.\"),\n",
    "        ]\n",
    "    )\n",
    "    output_parser = StrOutputParser()\n",
    "    llm = prompt | model | output_parser\n",
    "    return {\"output\": llm.invoke(inputs)}\n",
    "\n",
    "\n",
    "# Custom Evaluation\n",
    "def must_have_user_activities(run: Run, example: Example) -> dict:\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    print(f\"run id: {run.id}\\n\")\n",
    "    required = example.outputs.get(\"tables\") or []  # outputsのキー (tables) と合わせる\n",
    "    print(required)\n",
    "    print(prediction)\n",
    "    score = all(\n",
    "        phrase in prediction for phrase in required\n",
    "    )  # scoreは自分で定義したものでよい\n",
    "    return {\n",
    "        \"key\": \"must_have_user_activities\",\n",
    "        \"score\": score,\n",
    "        \"comment\": \"comment test\",\n",
    "    }  # key, score, commentを返す\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict,\n",
    "    data=dataset_name,  # The data to predict and grade over\n",
    "    evaluators=[must_have_user_activities],  # The evaluators to score the results\n",
    "    experiment_prefix=\"ml-workshop\",  # A prefix for your experiment names to easily identify them\n",
    "    metadata={\n",
    "        \"version\": \"1.0.0\",\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
