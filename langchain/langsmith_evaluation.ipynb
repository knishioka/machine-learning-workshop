{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e45ac2",
   "metadata": {},
   "source": [
    "# LangSmithã‚’ä½¿ã£ãŸè©•ä¾¡\n",
    "LangSmithã®åŸºæœ¬çš„ãªãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®æ–¹æ³•ã¨ã€feedbackã‚„evaluationã‚’ä½¿ã£ãŸè©•ä¾¡ã‚’è¡Œã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7809f17c-44bc-4f19-8863-4bc854340537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a069ef0b",
   "metadata": {},
   "source": [
    "## LangSmithé–¢é€£ã®ç’°å¢ƒå¤‰æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc4f0cd4-3b87-42fe-aa4f-54b69028d157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env Variables\n",
      "LANGCHAIN_TRACING_V2: true\n",
      "LANGCHAIN_PROJECT: machine-learning-workshop\n",
      "LANGCHAIN_ENDPOINT: https://api.smith.langchain.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f\"\"\"Env Variables\n",
    "LANGCHAIN_TRACING_V2: {os.environ[\"LANGCHAIN_TRACING_V2\"]}\n",
    "LANGCHAIN_PROJECT: {os.environ[\"LANGCHAIN_PROJECT\"]}\n",
    "LANGCHAIN_ENDPOINT: {os.environ[\"LANGCHAIN_ENDPOINT\"]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45564955-2936-4c57-922c-69b525c0c4d0",
   "metadata": {},
   "source": [
    "## LangSmithã®ãƒˆãƒ¬ãƒ¼ã‚¹ã®åŸºæœ¬\n",
    "traceableãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ä»»æ„ã®é–¢æ•°ã®å¼•æ•°ã¨è¿”ã‚Šå€¤ã‚’LangSmithã§ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c29549-322a-4d07-865f-3f4ec80d36b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "SELECT\n",
       "  (SUM(CASE WHEN pageviews > 1 THEN 1 ELSE 0 END) / COUNT(*)) AS bounce_rate\n",
       "FROM\n",
       "  `your_dataset.your_table`\n",
       "WHERE\n",
       "  session_id IS NOT NULL;\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langsmith import traceable\n",
    "import openai\n",
    "\n",
    "openai_client = openai.Client()\n",
    "\n",
    "\n",
    "@traceable\n",
    "def format_prompt(question):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"ã‚ãªãŸã¯BigQueryã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™. å‡ºã—ãŸã„ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ã‚¨ãƒªã‚’ä½œæˆã—ã¦ãã ã•ã„. å‡ºåŠ›ã¯ã‚¯ã‚¨ãƒªã®ã¿ã§ä»–ã®æƒ…å ±ã¯ä¸è¦ã§ã™.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def invoke_llm(messages):\n",
    "    return openai_client.chat.completions.create(\n",
    "        messages=messages, model=\"gpt-4o\", temperature=0\n",
    "    )\n",
    "\n",
    "\n",
    "@traceable\n",
    "def parse_output(response):\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "@traceable\n",
    "def run_pipeline():\n",
    "    messages = format_prompt(\"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®å›éŠç‡\")\n",
    "    response = invoke_llm(messages)\n",
    "    return parse_output(response)\n",
    "\n",
    "\n",
    "display(Markdown(run_pipeline()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147b87e-6380-4c7a-8aa8-e2ee0186a19e",
   "metadata": {},
   "source": [
    "openaiã¨ã®ã‚„ã‚Šå–ã‚Šã®å¯è¦³æ¸¬ã«ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼ `wrap_openai` ã‚’ä½¿ã†ã¨ã€ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ãªã©ã‚’ç°¡å˜ã«å–å¾—å¯èƒ½ã«ãªã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a83e551-9053-4dbd-a30f-578b84698965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "SELECT\n",
       "  user_id,\n",
       "  COUNT(DISTINCT page_id) AS pages_visited,\n",
       "  COUNT(DISTINCT session_id) AS sessions,\n",
       "  COUNT(DISTINCT page_id) / COUNT(DISTINCT session_id) AS page_views_per_session\n",
       "FROM\n",
       "  `your_project.your_dataset.your_table`\n",
       "GROUP BY\n",
       "  user_id\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "wrap_openai_client = wrap_openai(openai.Client())\n",
    "\n",
    "\n",
    "@traceable(name=\"run_pipeline with wrap_openai\")\n",
    "def run_pipeline_with_wrap_llm():\n",
    "    messages = format_prompt(\"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®å›éŠç‡\")\n",
    "    response = wrap_openai_client.chat.completions.create(\n",
    "        messages=messages, model=\"gpt-4o\", temperature=0\n",
    "    )\n",
    "    return parse_output(response)\n",
    "\n",
    "\n",
    "display(Markdown(run_pipeline_with_wrap_llm()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326a818-99b9-4252-943a-e3729c14da4b",
   "metadata": {},
   "source": [
    "LCEL (LangChain Expression Language) ã‚’ä½¿ãˆã°ã€LangSmithã§ã®è¦³æ¸¬ãŒæ¥½ã«ã§ãã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db794be6-9d7d-4003-84a6-532324cd2627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ä¸‹è¨˜ã®ã‚¯ã‚¨ãƒªã¯ã€ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®å„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ãŠã‘ã‚‹ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼æ•°ã‚’è¨ˆç®—ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®å›éŠç‡ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®ä¸€ã¤ã®æ–¹æ³•ã§ã™ã€‚\n",
       "\n",
       "```\n",
       "SELECT\n",
       "  sessionId,\n",
       "  COUNT(pageViewId) as pageViews\n",
       "FROM\n",
       "  `project.dataset.table`\n",
       "GROUP BY\n",
       "  sessionId\n",
       "```\n",
       "\n",
       "ã“ã“ã§ã€`project.dataset.table`ã¯ã‚ãªãŸã®BigQueryãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…·ä½“çš„ãªå ´æ‰€ã‚’æŒ‡ã—ã¾ã™ã€‚ã¾ãŸã€`sessionId` ã¨ `pageViewId` ã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã¨ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼IDã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’æŒ‡ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã¯ã€ã‚ãªãŸã®å…·ä½“çš„ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«åŸºã¥ã„ã¦å¤‰æ›´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ã‚ãªãŸã¯BigQueryã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™. å‡ºã—ãŸã„ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ã‚¨ãƒªã‚’ä½œæˆã—ã¦ãã ã•ã„. å‡ºåŠ›ã¯ã‚¯ã‚¨ãƒªã®ã¿ã§ä»–ã®æƒ…å ±ã¯ä¸è¦ã§ã™.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "display(Markdown(chain.invoke(\"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®å›éŠç‡\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7eeb0",
   "metadata": {},
   "source": [
    "## Feedbackã¨Evaluationã‚’ä½¿ã£ãŸè©•ä¾¡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "769bd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a6ccde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgQVtMYW5nU21pdGhdIC0tPiBCW1VzZXIgYW5kIFByb2R1Y3QgVGVhbSBGZWVkYmFja10KICBBIC0tPiBDW1ByZXBhcmVkIERhdGFzZXQgRXhwZXJpbWVudHNdCgogIHN1YmdyYXBoIEJbRmVlZGJhY2tdCiAgICBCMVtBbm5vdGF0ZSBSdW5zXSAtLT4gQjJbU2F2ZSBGZWVkYmFja10KICBlbmQKCiAgc3ViZ3JhcGggQ1tFdmFsdWF0aW9uXQogICAgQzFbSW5wdXQgYW5kIE91dHB1dF0gLS0+IEMyW1J1biBFeHBlcmltZW50c10KICBlbmQKCiAgY2xhc3NEZWYgbWFpbiBmaWxsOiNmOWYsc3Ryb2tlOiMzMzMsc3Ryb2tlLXdpZHRoOjJweDsKICBjbGFzc0RlZiBzdWIgZmlsbDojYmJmLHN0cm9rZTojMzMzLHN0cm9rZS13aWR0aDoycHg7CiAgY2xhc3NEZWYgZGV0YWlsIGZpbGw6I2ZiMyxzdHJva2U6IzMzMyxzdHJva2Utd2lkdGg6MnB4OwoKICBjbGFzcyBBLEIsQyBtYWluOwogIGNsYXNzIEIxLEIyLEMxLEMyIGRldGFpbDsgICAKICAg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph TD\n",
    "  A[LangSmith] --> B[User and Product Team Feedback]\n",
    "  A --> C[Prepared Dataset Experiments]\n",
    "\n",
    "  subgraph B[Feedback]\n",
    "    B1[Annotate Runs] --> B2[Save Feedback]\n",
    "  end\n",
    "\n",
    "  subgraph C[Evaluation]\n",
    "    C1[Input and Output] --> C2[Run Experiments]\n",
    "  end\n",
    "\n",
    "  classDef main fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "  classDef sub fill:#bbf,stroke:#333,stroke-width:2px;\n",
    "  classDef detail fill:#fb3,stroke:#333,stroke-width:2px;\n",
    "\n",
    "  class A,B,C main;\n",
    "  class B1,B2,C1,C2 detail;   \n",
    "   \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf00005-30e6-4f85-8878-bf98ba4ad8d7",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "- Traceã•ã‚ŒãŸå®Ÿè¡Œã®ä¸­ã«å«ã¾ã‚Œã‚‹Runã«ã€è‡ªåˆ†ã§å®šç¾©ã—ãŸTagã‚„Keyã‚’Annotate\n",
    "  - trace_id1ã¤ã«å¯¾ã—ã¦è¤‡æ•°ã®run_idãŒå«ã¾ã‚Œã‚‹æ§‹é€ \n",
    "  - æœ€åˆã®run_idã¯trace_idã¨åŒä¸€\n",
    "- APIçµŒç”±ã®feedbackã§ã¯Key, æ‰‹å‹•ã®feedbackã§ã¯Tagã§Annotateã™ã‚‹ä»•çµ„ã¿ã¨ãªã£ã¦ã„ã‚‹ãŒã€Tagã‚‚Keyã¨ã—ã¦ä¿å­˜ã•ã‚Œã¦ã„ã‚‹\n",
    "\"- ãŸã ã—ã€APIçµŒç”±ã®feedbackã¯recordãŒè¿½åŠ ãƒ»ä¸Šæ›¸ãã§ãã‚‹ã®ã«å¯¾ã—ã¦ã€æ‰‹å‹•ã®feedbackã¯ä¸Šæ›¸ãã®ã¿ã¨ã„ã†é•ã„ãŒã‚ã‚‹\n",
    "\"- æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã§åŒã˜ã‚­ãƒ¼ã®ã‚‚ã®ã¯é›†è¨ˆã•ã‚Œã¦è¡¨ç¤ºã•ã‚Œã‚‹\n",
    "- LLMã‚¢ãƒ—ãƒªã‚’ä½¿ã£ã¦ã„ã‚‹ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ã€åŸºæœ¬çš„ã«APIçµŒç”±ã®ç™»éŒ²ã¨ãªã‚‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3859db3-efbd-407e-8aef-ed1fe2bc396b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9a3f0de-4149-4343-b359-ec6c4fa8cb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'Langsmith testing features'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'content': 'This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf youâ€™re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nItâ€™s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, letâ€™s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.'}, {'url': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'content': 'How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLetâ€™s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLetâ€™s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, letâ€™s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, letâ€™s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)'}, {'url': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'content': \"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\"}, {'url': 'https://www.langchain.com/langsmith', 'content': \"BETA Sign Up\\nContact Sales\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter ğŸ¦œğŸ”— LangChain\\nLangSmith\\nLangServe\\nAgents\\nRetrieval\\nEvaluation\\nBlog\\nDocs\\nğŸ¦œğŸ”— LangChain\\nBuild and deploy LLM apps with confidence\\nAn all-in-one developer platform for every step of the application lifecycle.\\n Prompt playground\\nCross-team collaboration\\nCatalog of ranging models & tasks\\nProven prompting strategies\\nExplore LangChain Hub\\nTurn the magic of LLM applications into enterprise-ready products\\nNative collaboration\\nBring your team together in LangSmith to craft prompts, debug, and capture feedback.\\n Application-level usage stats\\nFeedback collection\\nFilter traces\\nCost measurement\\nPerformance comparison\\nGo To Docs\\nManage Prompts\\nPrompts power your team's chains and agents, and LangSmith allows you to refine, test, and version them in one place. Dataset curation\\nEvaluate chain performance\\nAI-assisted evaluation\\nEasy benchmarking\\nGo To Docs\\nMonitor\\nGiven the stochastic nature of LLMs, it can be hard to answer the simple question: â€œwhatâ€™s happening with my application?â€\"}, {'url': 'https://docs.smith.langchain.com/tutorials/Developers/evaluation', 'content': \"Set up automated testing to run in CI/CD; For more information on the evaluation workflows LangSmith supports, check out the how-to guides. Lots to cover, let's dive in! Create a dataset The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:\"}]\u001b[0m\u001b[32;1m\u001b[1;3mLangsmith offers a variety of features to help with testing, debugging, and evaluating Language Learning Models (LLMs) and AI applications. Here are some key aspects:\n",
      "\n",
      "1. **Automated Testing**: Langsmith supports setting up automated testing workflows that can be integrated into CI/CD pipelines. This ensures that your models are consistently evaluated and tested as part of your development process.\n",
      "\n",
      "2. **Dataset Creation and Management**: You can create and manage datasets within Langsmith. This includes labeled datasets for more comprehensive evaluation and unlabeled datasets for general testing.\n",
      "\n",
      "3. **Evaluation Metrics**: Langsmith provides built-in and custom evaluators to measure the correctness and performance of your models. This includes metrics like contextual accuracy and other custom criteria.\n",
      "\n",
      "4. **Prompt Management**: Langsmith allows you to refine, test, and version your prompts in one place. This helps in managing and improving the prompts that power your models.\n",
      "\n",
      "5. **Performance Monitoring**: The platform offers tools to monitor application-level usage stats, collect feedback, filter traces, measure costs, and compare performance. This helps in understanding the behavior and efficiency of your models in real-time.\n",
      "\n",
      "6. **Collaboration**: Langsmith supports cross-team collaboration, enabling teams to work together on crafting prompts, debugging issues, and capturing feedback.\n",
      "\n",
      "7. **Debugging and Logging**: The platform provides detailed logging and tracing capabilities to help debug issues and ensure that logs and traces are submitted in full before the program proceeds.\n",
      "\n",
      "8. **AI-Assisted Evaluation**: Langsmith includes AI-assisted evaluation tools to benchmark and evaluate the performance of your models easily.\n",
      "\n",
      "For more detailed information, you can refer to the [Langsmith documentation](https://docs.smith.langchain.com/tutorials/Developers/evaluation) and other resources available on their [official website](https://www.langchain.com/langsmith).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can langsmith help with testing?',\n",
       " 'output': 'Langsmith offers a variety of features to help with testing, debugging, and evaluating Language Learning Models (LLMs) and AI applications. Here are some key aspects:\\n\\n1. **Automated Testing**: Langsmith supports setting up automated testing workflows that can be integrated into CI/CD pipelines. This ensures that your models are consistently evaluated and tested as part of your development process.\\n\\n2. **Dataset Creation and Management**: You can create and manage datasets within Langsmith. This includes labeled datasets for more comprehensive evaluation and unlabeled datasets for general testing.\\n\\n3. **Evaluation Metrics**: Langsmith provides built-in and custom evaluators to measure the correctness and performance of your models. This includes metrics like contextual accuracy and other custom criteria.\\n\\n4. **Prompt Management**: Langsmith allows you to refine, test, and version your prompts in one place. This helps in managing and improving the prompts that power your models.\\n\\n5. **Performance Monitoring**: The platform offers tools to monitor application-level usage stats, collect feedback, filter traces, measure costs, and compare performance. This helps in understanding the behavior and efficiency of your models in real-time.\\n\\n6. **Collaboration**: Langsmith supports cross-team collaboration, enabling teams to work together on crafting prompts, debugging issues, and capturing feedback.\\n\\n7. **Debugging and Logging**: The platform provides detailed logging and tracing capabilities to help debug issues and ensure that logs and traces are submitted in full before the program proceeds.\\n\\n8. **AI-Assisted Evaluation**: Langsmith includes AI-assisted evaluation tools to benchmark and evaluate the performance of your models easily.\\n\\nFor more detailed information, you can refer to the [Langsmith documentation](https://docs.smith.langchain.com/tutorials/Developers/evaluation) and other resources available on their [official website](https://www.langchain.com/langsmith).'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()\n",
    "tools = [search]\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41b88c",
   "metadata": {},
   "source": [
    "LangSmithä¸Šã§æ‰‹å‹•ã§Feedbackã‚’ä»˜ä¸ã™ã‚‹ã“ã¨ãŒã§ãã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bbb6e-ebce-4bcc-86e5-89260829c215",
   "metadata": {},
   "source": [
    "#### Runã®æƒ…å ±ã‚’å–å¾—\n",
    "LangSmithã®APIã‚’ä½¿ã£ã¦Feedbackã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã¯ã€run idã‚’å–å¾—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ade91553-80da-49ad-8417-45bc3c070287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'Langsmith testing features'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'content': 'How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLetâ€™s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLetâ€™s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, letâ€™s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, letâ€™s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)'}, {'url': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'content': 'This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf youâ€™re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nItâ€™s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, letâ€™s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.'}, {'url': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'content': \"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\"}, {'url': 'https://docs.smith.langchain.com/tutorials/Developers/evaluation', 'content': \"Set up automated testing to run in CI/CD; For more information on the evaluation workflows LangSmith supports, check out the how-to guides. Lots to cover, let's dive in! Create a dataset The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:\"}, {'url': 'https://www.langchain.com/langsmith', 'content': \"BETA Sign Up\\nContact Sales\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter ğŸ¦œğŸ”— LangChain\\nLangSmith\\nLangServe\\nAgents\\nRetrieval\\nEvaluation\\nBlog\\nDocs\\nğŸ¦œğŸ”— LangChain\\nBuild and deploy LLM apps with confidence\\nAn all-in-one developer platform for every step of the application lifecycle.\\n Prompt playground\\nCross-team collaboration\\nCatalog of ranging models & tasks\\nProven prompting strategies\\nExplore LangChain Hub\\nTurn the magic of LLM applications into enterprise-ready products\\nNative collaboration\\nBring your team together in LangSmith to craft prompts, debug, and capture feedback.\\n Application-level usage stats\\nFeedback collection\\nFilter traces\\nCost measurement\\nPerformance comparison\\nGo To Docs\\nManage Prompts\\nPrompts power your team's chains and agents, and LangSmith allows you to refine, test, and version them in one place. Dataset curation\\nEvaluate chain performance\\nAI-assisted evaluation\\nEasy benchmarking\\nGo To Docs\\nMonitor\\nGiven the stochastic nature of LLMs, it can be hard to answer the simple question: â€œwhatâ€™s happening with my application?â€\"}]\u001b[0m\u001b[32;1m\u001b[1;3mLangsmith offers a variety of features to help with testing, debugging, evaluating, and monitoring Language Learning Models (LLMs) and AI applications. Here are some key aspects:\n",
      "\n",
      "1. **Automated Testing**: Langsmith supports setting up automated testing workflows that can be integrated into CI/CD pipelines. This ensures that your models are consistently evaluated and tested as part of your development process.\n",
      "\n",
      "2. **Evaluation Metrics**: Langsmith provides built-in and custom evaluators to measure the correctness of responses to prompts. This includes metrics like contextual accuracy and other custom criteria that can be defined based on your specific needs.\n",
      "\n",
      "3. **Dataset Management**: You can create and manage datasets within Langsmith, allowing you to run tests on labeled and unlabeled data. This helps in evaluating the performance of your models across different types of data.\n",
      "\n",
      "4. **Feedback and Monitoring**: Langsmith allows you to collect feedback, monitor application-level usage stats, filter traces, and measure costs. This helps in understanding the performance and efficiency of your models in real-world scenarios.\n",
      "\n",
      "5. **Prompt Management**: Langsmith provides tools to refine, test, and version prompts. This is crucial for ensuring that your prompts are optimized and effective in generating the desired responses from your models.\n",
      "\n",
      "6. **Cross-Team Collaboration**: Langsmith supports native collaboration features, enabling teams to work together in crafting prompts, debugging issues, and capturing feedback. This fosters a collaborative environment for developing and refining LLM applications.\n",
      "\n",
      "7. **Real-World Examples and Tutorials**: Langsmith offers a cookbook of real-world examples and tutorials to help you get started and make the most of its features. This includes hands-on examples and code snippets to guide you through various use cases.\n",
      "\n",
      "For more detailed information, you can refer to the [Langsmith documentation](https://docs.smith.langchain.com/tutorials/Developers/evaluation) and other resources available on their [official website](https://www.langchain.com/langsmith).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import tracing_v2_enabled\n",
    "\n",
    "with tracing_v2_enabled() as cb:\n",
    "    agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea3b34c5-bab4-4c1b-b4f2-75b17268af71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/o/bd14a154-65e7-52b4-bdce-b9a16d5e3513/projects/p/dc1fff1d-e827-4ee4-b948-e71e0914f7dd/r/73c37ddb-d255-443d-a136-35eee99e5a58?poll=true'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runã®URLã‚’å–å¾—\n",
    "cb.get_run_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e246e6de-f5cc-4ed1-86f7-f181a9c8b9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('73c37ddb-d255-443d-a136-35eee99e5a58')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run IDã‚’å–å¾—\n",
    "run_id = cb.latest_run.id\n",
    "run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a495f-1e89-4e7c-a8de-d6323e7a9280",
   "metadata": {},
   "source": [
    "#### Feedbackã®ä½œæˆ\n",
    "run_idã‚’æŒ‡å®šã—ã¦Feedbackã‚’è¿½åŠ ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n",
    "keyã¯è‡ªåˆ†ã§ä»»æ„ã®å€¤ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒå¯èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf65cbde-36c8-4d11-8aa5-d17b7798aa11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('28f1db89-a530-4d41-beb4-0c143a9c8b95'), created_at=datetime.datetime(2024, 6, 26, 1, 52, 2, 559765, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 26, 1, 52, 2, 559768, tzinfo=datetime.timezone.utc), run_id=UUID('73c37ddb-d255-443d-a136-35eee99e5a58'), key='test-score', score=1, value=None, comment=None, correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "ls_client = Client()\n",
    "\n",
    "score_feedback1 = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, key=\"test-score\", score=1\n",
    ")\n",
    "score_feedback1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a3dc6",
   "metadata": {},
   "source": [
    "åŒã˜ã‚­ãƒ¼ã‚’æŒ‡å®šã™ã‚‹ã¨åˆ¥ã®feedbackã¨ã—ã¦ä¿å­˜ã•ã‚Œã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3a1eb0a-a932-4e1d-9c4e-37364aa55ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('a210e562-3f1d-44b4-8711-46d2699a7dfb'), created_at=datetime.datetime(2024, 6, 26, 1, 52, 3, 237451, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 26, 1, 52, 3, 237463, tzinfo=datetime.timezone.utc), run_id=UUID('73c37ddb-d255-443d-a136-35eee99e5a58'), key='test-score', score=100, value=None, comment=None, correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_feedback2 = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, key=\"test-score\", score=100\n",
    ")\n",
    "score_feedback2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e323f9",
   "metadata": {},
   "source": [
    "feedback_idã‚’æŒ‡å®šã—ã¦ã€ä¸Šæ›¸ãã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36caba68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('28f1db89-a530-4d41-beb4-0c143a9c8b95'), created_at=datetime.datetime(2024, 6, 26, 1, 52, 3, 910193, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 26, 1, 52, 3, 910200, tzinfo=datetime.timezone.utc), run_id=UUID('73c37ddb-d255-443d-a136-35eee99e5a58'), key='test-score', score=500, value=None, comment=None, correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_score_feedback1 = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, feedback_id=score_feedback1.id, key=\"test-score\", score=500\n",
    ")\n",
    "update_score_feedback1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d96f00ec-b857-47b4-b202-2c8505d0ca8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('81bf1761-efef-404c-9bc0-20dfaa319e23'), created_at=datetime.datetime(2024, 6, 26, 1, 52, 5, 112288, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 26, 1, 52, 5, 112292, tzinfo=datetime.timezone.utc), run_id=UUID('73c37ddb-d255-443d-a136-35eee99e5a58'), key='test-comment', score=None, value=None, comment='test comment', correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}), session_id=None, comparative_experiment_id=None, feedback_group_id=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_feedback = ls_client.create_feedback(\n",
    "    run_id=cb.latest_run.id, key=\"test-comment\", comment=\"test comment\"\n",
    ")\n",
    "comment_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f91dab-2c51-4cf4-9fd3-f860a4aa8cc0",
   "metadata": {},
   "source": [
    "#### Feedbackã®å‰Šé™¤\n",
    "feedback_idã‚’æŒ‡å®šã—ã¦ã€ä½œæˆã—ãŸfeedbackã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46f03c34-c7d6-4970-bc1e-861ccc5829fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½œæˆã—ãŸfeedbackã‚’å‰Šé™¤\n",
    "ls_client.delete_feedback(score_feedback1.id)\n",
    "ls_client.delete_feedback(comment_feedback.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36130122-2fb6-4a93-b16c-4eba75acdb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_feedbackã§feedbackã‚’ã™ã¹ã¦å–å¾—ã—ã€å‰Šé™¤\n",
    "for feedback in ls_client.list_feedback(run_ids=[cb.latest_run.id]):\n",
    "    ls_client.delete_feedback(feedback.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12237ca-d2a0-463b-85f0-55658c4dd09b",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "- Datasetã«ã‚ã‚‰ã‹ã˜ã‚Inputã¨Outputã®çµ„ã¿åˆã‚ã›ã‹ã‚‰ãªã‚‹Exampleã‚’ä¿å­˜\n",
    "- Exampleã®Inputã‚’ä½¿ã£ã¦LLMã‚’å®Ÿè¡Œã—ã€å‡ºã¦ããŸOutputã‚’ä¿å­˜ã•ã‚Œã¦ã„ã‚‹Outputã‚’ä½¿ã£ã¦è©•ä¾¡\n",
    "- è©•ä¾¡ã®æ–¹æ³•ã¯ã€LangSmithãŒã‚ã‚‰ã‹ã˜ã‚ç”¨æ„ã—ã¦ã„ã‚‹ã‚‚ã®ã‹ã€ã‚«ã‚¹ã‚¿ãƒ ã§ä½œæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹\n",
    "- è©•ä¾¡çµæœã¯ã€key (è©•ä¾¡æŒ‡æ¨™ã®åå‰), score (è©•ä¾¡çµæœ), commentã¨ã—ã¦æ®‹ã™ã“ã¨ãŒå¯èƒ½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccc0f086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgc3ViZ3JhcGggRXZhbHVhdGlvblBpcGVsaW5lW0V2YWx1YXRpb24gUGlwZWxpbmVdCiAgICBCW0RhdGFzZXRzXSAtLT4gQ1tFeGFtcGxlc10KICAgIEMgLS0+IERbSW5wdXRzXQogICAgQyAtLT4gRVtFeHBlY3RlZCBPdXRwdXRzXQogICAgRCAtLT4gRltMTE1dCiAgICBGIC0tPiBHW1J1biBPdXRwdXRzXQogICAgRSAtLT4gSFtFdmFsdWF0b3JzXQogICAgRyAtLT4gSAogICAgSCAtLT4gSVtFdmFsdWF0aW9uIFJlc3VsdF0KCiAgICBzdWJncmFwaCBJW0V2YWx1YXRpb24gUmVzdWx0XQogICAgICBkaXJlY3Rpb24gTFIKICAgICAgSTFbS2V5OiBtZXRyaWMgbmFtZV0gLS4tIEkyW1Njb3JlOiBtZXRyaWMgdmFsdWVdIC0uLSBJM1tDb21tZW50OiByZWFzb25pbmddCiAgICAgIEkzIC0uLSBJMQogICAgZW5kCiAgZW5kCgogIGNsYXNzRGVmIG1haW4gZmlsbDojZjlmLHN0cm9rZTojMzMzLHN0cm9rZS13aWR0aDoycHg7CiAgY2xhc3NEZWYgc3ViIGZpbGw6I2JiZixzdHJva2U6IzMzMyxzdHJva2Utd2lkdGg6MnB4OwogIGNsYXNzRGVmIGRldGFpbCBmaWxsOiNmYjMsc3Ryb2tlOiMzMzMsc3Ryb2tlLXdpZHRoOjJweDsKCiAgY2xhc3MgQixDLEQsRSxGLEcsSCxJIG1haW47CiAgY2xhc3MgSTEsSTIsSTMgZGV0YWlsOyAgIAogICA=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph TD\n",
    "  subgraph EvaluationPipeline[Evaluation Pipeline]\n",
    "    B[Datasets] --> C[Examples]\n",
    "    C --> D[Inputs]\n",
    "    C --> E[Expected Outputs]\n",
    "    D --> F[LLM]\n",
    "    F --> G[Run Outputs]\n",
    "    E --> H[Evaluators]\n",
    "    G --> H\n",
    "    H --> I[Evaluation Result]\n",
    "\n",
    "    subgraph I[Evaluation Result]\n",
    "      direction LR\n",
    "      I1[Key: metric name] -.- I2[Score: metric value] -.- I3[Comment: reasoning]\n",
    "      I3 -.- I1\n",
    "    end\n",
    "  end\n",
    "\n",
    "  classDef main fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "  classDef sub fill:#bbf,stroke:#333,stroke-width:2px;\n",
    "  classDef detail fill:#fb3,stroke:#333,stroke-width:2px;\n",
    "\n",
    "  class B,C,D,E,F,G,H,I main;\n",
    "  class I1,I2,I3 detail;   \n",
    "   \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc834a",
   "metadata": {},
   "source": [
    "#### Datasetã¨Exampleã‚’ä½œæˆ\n",
    "Datasetã‚’ä½œæˆã—ã€Exampleã‚’ä¿å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9553e43f-b1e1-45a0-ac0e-faa11c2e6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from langsmith import Client\n",
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "ls_client = Client()  # LangSmithã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ\n",
    "\n",
    "# ä½œæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "dataset_name = \"SQL Samples\"\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã‚ã‚Œã°å‰Šé™¤\n",
    "if ls_client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = ls_client.delete_dataset(dataset_name=dataset_name)\n",
    "\n",
    "dataset = ls_client.create_dataset(\n",
    "    dataset_name, description=\"ML Workshopç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒª\"\n",
    ")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«exampleã‚’ä¿å­˜\n",
    "ls_client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"MAUã‚’å–å¾—\"},\n",
    "        {\"question\": \"æ–°è¦ãƒ¦ãƒ¼ã‚¶æ•°ã®æ¨ç§»\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\n",
    "            \"query\": textwrap.dedent(\"\"\"\n",
    "           SELECT\n",
    "               COUNT(DISTINCT user_id) AS monthly_active_users\n",
    "           FROM\n",
    "               `your_dataset.user_activities`\n",
    "           WHERE\n",
    "               activity_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH) AND CURRENT_DATE()\n",
    "        \"\"\"),\n",
    "            \"tables\": [\"user_activities\"],\n",
    "        },\n",
    "        {\n",
    "            \"query\": textwrap.dedent(\"\"\"\n",
    "            SELECT\n",
    "                signup_date,\n",
    "                COUNT(user_id) AS new_users\n",
    "            FROM\n",
    "                `your_dataset.user_activities`\n",
    "            GROUP BY\n",
    "                signup_date\n",
    "            ORDER BY\n",
    "            ã€€ã€€ signup_date\n",
    "        \"\"\"),\n",
    "            \"tables\": [\"user_activities\"],\n",
    "        },\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90a464ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/o/bd14a154-65e7-52b4-bdce-b9a16d5e3513/datasets/ec7e16e3-d0fe-4aa9-9b24-241327e6f1ce'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasetã®URLã‚’å–å¾—\n",
    "dataset.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c94cc301-2dc7-4c8d-8ed2-cb0bdec486f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasetã®exampleæ•°ã‚’å–å¾—\n",
    "dataset.example_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad151f48-5fed-436b-8221-808184fcf0bf",
   "metadata": {},
   "source": [
    "example_countãŒ0ã¨ãªã£ã¦ã„ã‚‹ã®ã§ã€LangSmith Clientã‚’ä½¿ã£ã¦datasetã‚’èª­ã¿ç›´ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58a3b2c3-fe2b-467b-8444-e9772e6e8f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ls_client.read_dataset(dataset_name=dataset_name)\n",
    "dataset.example_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92a4743c-38d8-4ff6-87b5-77e32e8c1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"æœˆã”ã¨ã®CVæ•°ã®æ¨ç§»\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\n",
    "            \"query\": textwrap.dedent(\"\"\"\n",
    "           SELECT\n",
    "               FORMAT_TIMESTAMP('%Y-%m', conv_date) AS conversion_month,\n",
    "               COUNT(conv_id) AS conversions\n",
    "           FROM\n",
    "               `your_dataset.your_table`\n",
    "           WHERE\n",
    "               conv_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR) AND CURRENT_DATE()\n",
    "           GROUP BY\n",
    "               conversion_month\n",
    "           ORDER BY\n",
    "               conversion_month\n",
    "        \"\"\"),\n",
    "            \"tables\": [\"user_activities\"],\n",
    "        },\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ca496-6ce7-46d6-9a55-0d795cfb15be",
   "metadata": {},
   "source": [
    "exampleã‚’å¢—ã‚„ã—ãŸã“ã¨ã«ã‚ˆã‚Šã€datasetã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚‚å¤‰æ›´ã•ã‚Œã¦ã„ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e92f1240-bab6-44ef-a5dc-21f15a752b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "modified_at: 2024-06-26 01:52:09.950582+00:00\n",
      "example_count: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = ls_client.read_dataset(dataset_name=dataset_name)\n",
    "print(f\"\"\"\n",
    "modified_at: {dataset.modified_at}\n",
    "example_count: {dataset.example_count}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8904cd3-8a18-4978-821f-c6d12e5a6728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "question: æœˆã”ã¨ã®CVæ•°ã®æ¨ç§»\n",
      "query: \n",
      "SELECT\n",
      "    FORMAT_TIMESTAMP('%Y-%m', conv_date) AS conversion_month,\n",
      "    COUNT(conv_id) AS conversions\n",
      "FROM\n",
      "    `your_dataset.your_table`\n",
      "WHERE\n",
      "    conv_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR) AND CURRENT_DATE()\n",
      "GROUP BY\n",
      "    conversion_month\n",
      "ORDER BY\n",
      "    conversion_month\n",
      "\n",
      "    \n",
      "\n",
      "question: MAUã‚’å–å¾—\n",
      "query: \n",
      "SELECT\n",
      "    COUNT(DISTINCT user_id) AS monthly_active_users\n",
      "FROM\n",
      "    `your_dataset.user_activities`\n",
      "WHERE\n",
      "    activity_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH) AND CURRENT_DATE()\n",
      "\n",
      "    \n",
      "\n",
      "question: æ–°è¦ãƒ¦ãƒ¼ã‚¶æ•°ã®æ¨ç§»\n",
      "query: \n",
      "SELECT\n",
      "    signup_date,\n",
      "    COUNT(user_id) AS new_users\n",
      "FROM\n",
      "    `your_dataset.user_activities`\n",
      "GROUP BY\n",
      "    signup_date\n",
      "ORDER BY\n",
      "ã€€ã€€ signup_date\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# datasetã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹exampleã®ä¸€è¦§\n",
    "for example in ls_client.list_examples(dataset_name=dataset_name):\n",
    "    print(f\"\"\"\n",
    "question: {example.inputs[\"question\"]}\n",
    "query: {example.outputs[\"query\"]}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64866e-8183-42bc-b9a5-a0a946579f61",
   "metadata": {},
   "source": [
    "### Custom Evaluationã‚’å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00e595a1-18e2-4de5-bfce-01c6a4dd2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'ml-workshop-97d77c3e' at:\n",
      "https://smith.langchain.com/o/bd14a154-65e7-52b4-bdce-b9a16d5e3513/datasets/ec7e16e3-d0fe-4aa9-9b24-241327e6f1ce/compare?selectedSessions=335a9383-c555-4d54-b4e2-658a4bc17c8c\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15140305e0e74aab99a299af03e35c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id: cf15f7b3-6b9d-41fa-a94e-704a2f64e5b5\n",
      "run id: 602a44ee-fd61-4818-b532-0cc4feb9a80c\n",
      "\n",
      "['user_activities']\n",
      "```\n",
      "SELECT \n",
      "  EXTRACT(YEAR FROM activity_date) AS year,\n",
      "  EXTRACT(MONTH FROM activity_date) AS month,\n",
      "  COUNT(DISTINCT user_id) AS MAU\n",
      "FROM \n",
      "  `project_id.dataset_id.user_activities`\n",
      "GROUP BY \n",
      "  year, \n",
      "  month\n",
      "ORDER BY \n",
      "  year DESC, \n",
      "  month DESC\n",
      "```\n",
      "\n",
      "['user_activities']\n",
      "```\n",
      "SELECT \n",
      "  DATE(created_at) AS date, \n",
      "  COUNT(DISTINCT user_id) AS new_user_count\n",
      "FROM \n",
      "  user_activities\n",
      "WHERE \n",
      "  new_user = TRUE\n",
      "GROUP BY \n",
      "  date\n",
      "ORDER BY \n",
      "  date\n",
      "```\n",
      "run id: e9428f05-a949-4fea-af40-e52872d75d5f\n",
      "\n",
      "['user_activities']\n",
      "```\n",
      "SELECT \n",
      "  FORMAT_TIMESTAMP('%Y-%m', TIMESTAMP_SECONDS(time)) as Month,\n",
      "  COUNT(*) as CV \n",
      "FROM \n",
      "  `project.dataset.user_activities` \n",
      "WHERE \n",
      "  activity_type = 'conversion'\n",
      "GROUP BY \n",
      "  Month\n",
      "ORDER BY \n",
      "  Month\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# inputsã«exampleãŒ1ã¤ãšã¤æ¸¡ã•ã‚Œã‚‹\n",
    "def predict(inputs: dict) -> dict:\n",
    "    model = ChatOpenAI(model=\"gpt-4\")\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"ã‚ãªãŸã¯BigQueryã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™. å‡ºã—ãŸã„ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ã‚¨ãƒªã‚’ä½œæˆã—ã¦ãã ã•ã„. å‡ºåŠ›ã¯ã‚¯ã‚¨ãƒªã®ã¿ã§ä»–ã®æƒ…å ±ã¯ä¸è¦ã§ã™.\",\n",
    "            ),\n",
    "            (\"human\", \"{question}, tableã¯user_activitiesã‚’ä½¿ã„ã¾ã™.\"),\n",
    "        ]\n",
    "    )\n",
    "    output_parser = StrOutputParser()\n",
    "    llm = prompt | model | output_parser\n",
    "    return {\"output\": llm.invoke(inputs)}\n",
    "\n",
    "\n",
    "# Custom Evaluation\n",
    "def must_have_user_activities(run: Run, example: Example) -> dict:\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    print(f\"run id: {run.id}\\n\")\n",
    "    required = example.outputs.get(\"tables\") or []  # outputsã®ã‚­ãƒ¼ (tables) ã¨åˆã‚ã›ã‚‹\n",
    "    print(required)\n",
    "    print(prediction)\n",
    "    score = all(\n",
    "        phrase in prediction for phrase in required\n",
    "    )  # scoreã¯è‡ªåˆ†ã§å®šç¾©ã—ãŸã‚‚ã®ã§ã‚ˆã„\n",
    "    return {\n",
    "        \"key\": \"must_have_user_activities\",\n",
    "        \"score\": score,\n",
    "        \"comment\": \"comment test\",\n",
    "    }  # key, score, commentã‚’è¿”ã™\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict,\n",
    "    data=dataset_name,  # The data to predict and grade over\n",
    "    evaluators=[must_have_user_activities],  # The evaluators to score the results\n",
    "    experiment_prefix=\"ml-workshop\",  # A prefix for your experiment names to easily identify them\n",
    "    metadata={\n",
    "        \"version\": \"1.0.0\",\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
