{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e062617f-0c6a-4461-ae7d-a479b949adff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54dcdcb3-a22d-49cb-92fe-537bef126f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8700b83b-d486-48c3-a451-fe36d61ad4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    add_start_index=True   # 分割された位置を保存できる\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52b3e29-75c7-4996-98e8-8d2460a52fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "youtube_docs = YoutubeLoader.from_youtube_url(\n",
    "   youtube_url=\"https://www.youtube.com/watch?v=X550Zbz_ROE\",\n",
    "   language=\"en\"\n",
    ").load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfad1095-edb5-43eb-a2fc-bf8e9881cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"./tmp/chroma_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98fbebcd-e8b5-452e-951f-bf810733b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60c698ec-d815-49c5-bcb5-a649324ae034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "collection_name = \"chroma_test\"\n",
    "db = Chroma.from_documents(youtube_docs, embeddings, collection_name=collection_name, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "669bcf12-a02a-482e-ae5d-6860e3ee4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73433066-69a0-40dd-b308-81d91f1092be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model=\"gpt-3.5-turbo-0613\")\n",
    "chain = RetrievalQA.from_llm(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62fe4927-4fba-4b44-bd90-a51a0545899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"LangChain Agentとはなんですか？\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"LangChain Agentとはなんですか？\",\n",
      "  \"context\": \"Context:\\nthat do things like summarize the conversation as it goes along we've got other ones where we limit the window so it only remembers the last few encounters and then we've got sort of things where you know it merges some of those so where it has the last few but then it does a summary for everything else the summary though is actually done by calling out to a language model itself and asking it hey summarize this conversation so that's something that you will see as well then we've got some sort of more external ways of doing this so these can be things like making some kind of Knowledge Graph memory and putting things into sort of entity memory for doing these kinds of things and then lastly you can just customize it right so you can come up if you've got a very unique case that you want to use the memory something yourself you can actually write your own system for doing memory with this in Lang chain as well let's jump into the code and look at how these things work Okay so we've got the normal sort of setup of installing open AI Lang chain Etc put your opening I key in there and the first one we're going to look at is basically the conversation buffer memory so in Lane chamber we need to import the type of memory it is and then we will basically instantiate that and then we'll pass it into the chain so the chain that we're using here is just a simple conversation chain which allows us to basically converse with the model by doing composition predict and then passing in what we want to say and you'll see that as we go through this that I've got it set to verbose equals true so that we can see what the prompt is going in and we can then see the response that we're getting back so here we're basically setting up the simplest kind of memory the conversation buffer and it's just going to basically track what the user says what the agent says and then just stack that into the prompt as we go through so you can see we start off with me saying hi there I'm Sam and we\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nContext:\\nthat do things like summarize the conversation as it goes along we've got other ones where we limit the window so it only remembers the last few encounters and then we've got sort of things where you know it merges some of those so where it has the last few but then it does a summary for everything else the summary though is actually done by calling out to a language model itself and asking it hey summarize this conversation so that's something that you will see as well then we've got some sort of more external ways of doing this so these can be things like making some kind of Knowledge Graph memory and putting things into sort of entity memory for doing these kinds of things and then lastly you can just customize it right so you can come up if you've got a very unique case that you want to use the memory something yourself you can actually write your own system for doing memory with this in Lang chain as well let's jump into the code and look at how these things work Okay so we've got the normal sort of setup of installing open AI Lang chain Etc put your opening I key in there and the first one we're going to look at is basically the conversation buffer memory so in Lane chamber we need to import the type of memory it is and then we will basically instantiate that and then we'll pass it into the chain so the chain that we're using here is just a simple conversation chain which allows us to basically converse with the model by doing composition predict and then passing in what we want to say and you'll see that as we go through this that I've got it set to verbose equals true so that we can see what the prompt is going in and we can then see the response that we're getting back so here we're basically setting up the simplest kind of memory the conversation buffer and it's just going to basically track what the user says what the agent says and then just stack that into the prompt as we go through so you can see we start off with me saying hi there I'm Sam and we\\nHuman: LangChain Agentとはなんですか？\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] [6.30s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うエージェントのことです。このエージェントは、会話の要約や過去のエンカウンターの記憶など、さまざまなメモリ機能を持っています。また、カスタマイズも可能で、独自のメモリシステムを使用することもできます。\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うエージェントのことです。このエージェントは、会話の要約や過去のエンカウンターの記憶など、さまざまなメモリ機能を持っています。また、カスタマイズも可能で、独自のメモリシステムを使用することもできます。\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 468,\n",
      "      \"completion_tokens\": 117,\n",
      "      \"total_tokens\": 585\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0613\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [6.30s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うエージェントのことです。このエージェントは、会話の要約や過去のエンカウンターの記憶など、さまざまなメモリ機能を持っています。また、カスタマイズも可能で、独自のメモリシステムを使用することもできます。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [6.30s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うエージェントのことです。このエージェントは、会話の要約や過去のエンカウンターの記憶など、さまざまなメモリ機能を持っています。また、カスタマイズも可能で、独自のメモリシステムを使用することもできます。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [6.56s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \"LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うエージェントのことです。このエージェントは、会話の要約や過去のエンカウンターの記憶など、さまざまなメモリ機能を持っています。また、カスタマイズも可能で、独自のメモリシステムを使用することもできます。\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"LangChain Agentとはなんですか？\"\n",
    "res = chain(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc2e220b-5389-4368-abc5-a46433c54882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うエージェントのことです。このエージェントは、会話の要約や過去のエンカウンターの記憶など、さまざまなメモリ機能を持っています。また、カスタマイズも可能で、独自のメモリシステムを使用することもできます。\n"
     ]
    }
   ],
   "source": [
    "print(res[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42bf437f-19c8-46df-ac35-545de84765a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"具体的な例を教えてください\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"具体的な例を教えてください\",\n",
      "  \"context\": \"Context:\\njust print that out and we can look at it and we can see that yeah response is doing great when sem asks for customer support the AI responds positively and asks what kind of Customs support Sam needs so you'll notice here that it's kind of doing a co-reference resolution in that rather than a human would probably say it responds to what he asked or she here it's sticking to Ai and Sam in there so this is a summary one it's also very useful the next one is is kind of an alternate version of the first one that we had so this is a conversation buffer window memory so we're going to be doing the same sort of thing of just feeding the conversation into the prompt but the difference is now we're going to set the last number of interactions that we are sitting in here now I've set this very low just so we can see it here uh but you could actually set this to be much higher than this you could get like the last five or ten interactions depending on how many tokens you want to use and this also comes down to also how much money you want to spend for this kind of thing so it's instantiating it setting k equals 2 here we've got a conversation hi there I'm Sam what brings you here today I'm looking for some customer support so it's asking me what kind of customer support you need my TV is not working so now we've got our two things going so now it's gonna ask me again sorry here for that it's asking for more information I give it the information but you notice now when I tell it I turn you turn it on make some weird noises and then goes black and the prompt now has the last interactions right the last two full interactions that we've had plus this new one but it's lost that first one where I said hi I'm Sam so it's just feeding the last two interactions into the the large language model so if you've got something where like let's say we set it to k equals five most conversations probably aren't Gonna Change hugely it will sometimes refer back to things early on in the\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nContext:\\njust print that out and we can look at it and we can see that yeah response is doing great when sem asks for customer support the AI responds positively and asks what kind of Customs support Sam needs so you'll notice here that it's kind of doing a co-reference resolution in that rather than a human would probably say it responds to what he asked or she here it's sticking to Ai and Sam in there so this is a summary one it's also very useful the next one is is kind of an alternate version of the first one that we had so this is a conversation buffer window memory so we're going to be doing the same sort of thing of just feeding the conversation into the prompt but the difference is now we're going to set the last number of interactions that we are sitting in here now I've set this very low just so we can see it here uh but you could actually set this to be much higher than this you could get like the last five or ten interactions depending on how many tokens you want to use and this also comes down to also how much money you want to spend for this kind of thing so it's instantiating it setting k equals 2 here we've got a conversation hi there I'm Sam what brings you here today I'm looking for some customer support so it's asking me what kind of customer support you need my TV is not working so now we've got our two things going so now it's gonna ask me again sorry here for that it's asking for more information I give it the information but you notice now when I tell it I turn you turn it on make some weird noises and then goes black and the prompt now has the last interactions right the last two full interactions that we've had plus this new one but it's lost that first one where I said hi I'm Sam so it's just feeding the last two interactions into the the large language model so if you've got something where like let's say we set it to k equals five most conversations probably aren't Gonna Change hugely it will sometimes refer back to things early on in the\\nHuman: 具体的な例を教えてください\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] [6.67s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"具体的な例として、以下のような会話が考えられます。\\n\\nユーザー: こんにちは、私はサムです。何かサポートが必要です。\\nAI: 了解しました、どのようなサポートが必要でしょうか？\\nユーザー: テレビが動かなくなってしまいました。\\nAI: それは困りますね。具体的にどのような症状が見られますか？\\nユーザー: 電源を入れると変な音がして画面が真っ暗になります。\\n\\nこの会話では、AIがユーザーの要件を理解し、適切な質問を返しています。AIは直前の2つの対話を参照して、より適切な応答を生成しています。\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"具体的な例として、以下のような会話が考えられます。\\n\\nユーザー: こんにちは、私はサムです。何かサポートが必要です。\\nAI: 了解しました、どのようなサポートが必要でしょうか？\\nユーザー: テレビが動かなくなってしまいました。\\nAI: それは困りますね。具体的にどのような症状が見られますか？\\nユーザー: 電源を入れると変な音がして画面が真っ暗になります。\\n\\nこの会話では、AIがユーザーの要件を理解し、適切な質問を返しています。AIは直前の2つの対話を参照して、より適切な応答を生成しています。\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 482,\n",
      "      \"completion_tokens\": 228,\n",
      "      \"total_tokens\": 710\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0613\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [6.68s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"具体的な例として、以下のような会話が考えられます。\\n\\nユーザー: こんにちは、私はサムです。何かサポートが必要です。\\nAI: 了解しました、どのようなサポートが必要でしょうか？\\nユーザー: テレビが動かなくなってしまいました。\\nAI: それは困りますね。具体的にどのような症状が見られますか？\\nユーザー: 電源を入れると変な音がして画面が真っ暗になります。\\n\\nこの会話では、AIがユーザーの要件を理解し、適切な質問を返しています。AIは直前の2つの対話を参照して、より適切な応答を生成しています。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [6.68s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"具体的な例として、以下のような会話が考えられます。\\n\\nユーザー: こんにちは、私はサムです。何かサポートが必要です。\\nAI: 了解しました、どのようなサポートが必要でしょうか？\\nユーザー: テレビが動かなくなってしまいました。\\nAI: それは困りますね。具体的にどのような症状が見られますか？\\nユーザー: 電源を入れると変な音がして画面が真っ暗になります。\\n\\nこの会話では、AIがユーザーの要件を理解し、適切な質問を返しています。AIは直前の2つの対話を参照して、より適切な応答を生成しています。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [7.66s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \"具体的な例として、以下のような会話が考えられます。\\n\\nユーザー: こんにちは、私はサムです。何かサポートが必要です。\\nAI: 了解しました、どのようなサポートが必要でしょうか？\\nユーザー: テレビが動かなくなってしまいました。\\nAI: それは困りますね。具体的にどのような症状が見られますか？\\nユーザー: 電源を入れると変な音がして画面が真っ暗になります。\\n\\nこの会話では、AIがユーザーの要件を理解し、適切な質問を返しています。AIは直前の2つの対話を参照して、より適切な応答を生成しています。\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "follow_up_question =  \"具体的な例を教えてください\"\n",
    "res = chain(follow_up_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "659262f1-7df8-47a3-93be-fb690a6ef0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'具体的な例として、以下のような会話が考えられます。\\n\\nユーザー: こんにちは、私はサムです。何かサポートが必要です。\\nAI: 了解しました、どのようなサポートが必要でしょうか？\\nユーザー: テレビが動かなくなってしまいました。\\nAI: それは困りますね。具体的にどのような症状が見られますか？\\nユーザー: 電源を入れると変な音がして画面が真っ暗になります。\\n\\nこの会話では、AIがユーザーの要件を理解し、適切な質問を返しています。AIは直前の2つの対話を参照して、より適切な応答を生成しています。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e65ec52-7168-46ac-8f64-0b282e6fbd54",
   "metadata": {},
   "source": [
    "## Conversational Retriveal Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1286606-cdd3-493b-af85-7cb70a0677b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "210a1fbe-0570-428f-9b22-bd23a2450422",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc904366-d48c-4d4f-a5cd-44485de612ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"LangChain Agentとはなんですか？\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"LangChain Agentとはなんですか？\",\n",
      "  \"context\": \"that do things like summarize the conversation as it goes along we've got other ones where we limit the window so it only remembers the last few encounters and then we've got sort of things where you know it merges some of those so where it has the last few but then it does a summary for everything else the summary though is actually done by calling out to a language model itself and asking it hey summarize this conversation so that's something that you will see as well then we've got some sort of more external ways of doing this so these can be things like making some kind of Knowledge Graph memory and putting things into sort of entity memory for doing these kinds of things and then lastly you can just customize it right so you can come up if you've got a very unique case that you want to use the memory something yourself you can actually write your own system for doing memory with this in Lang chain as well let's jump into the code and look at how these things work Okay so we've got the normal sort of setup of installing open AI Lang chain Etc put your opening I key in there and the first one we're going to look at is basically the conversation buffer memory so in Lane chamber we need to import the type of memory it is and then we will basically instantiate that and then we'll pass it into the chain so the chain that we're using here is just a simple conversation chain which allows us to basically converse with the model by doing composition predict and then passing in what we want to say and you'll see that as we go through this that I've got it set to verbose equals true so that we can see what the prompt is going in and we can then see the response that we're getting back so here we're basically setting up the simplest kind of memory the conversation buffer and it's just going to basically track what the user says what the agent says and then just stack that into the prompt as we go through so you can see we start off with me saying hi there I'm Sam and we\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nthat do things like summarize the conversation as it goes along we've got other ones where we limit the window so it only remembers the last few encounters and then we've got sort of things where you know it merges some of those so where it has the last few but then it does a summary for everything else the summary though is actually done by calling out to a language model itself and asking it hey summarize this conversation so that's something that you will see as well then we've got some sort of more external ways of doing this so these can be things like making some kind of Knowledge Graph memory and putting things into sort of entity memory for doing these kinds of things and then lastly you can just customize it right so you can come up if you've got a very unique case that you want to use the memory something yourself you can actually write your own system for doing memory with this in Lang chain as well let's jump into the code and look at how these things work Okay so we've got the normal sort of setup of installing open AI Lang chain Etc put your opening I key in there and the first one we're going to look at is basically the conversation buffer memory so in Lane chamber we need to import the type of memory it is and then we will basically instantiate that and then we'll pass it into the chain so the chain that we're using here is just a simple conversation chain which allows us to basically converse with the model by doing composition predict and then passing in what we want to say and you'll see that as we go through this that I've got it set to verbose equals true so that we can see what the prompt is going in and we can then see the response that we're getting back so here we're basically setting up the simplest kind of memory the conversation buffer and it's just going to basically track what the user says what the agent says and then just stack that into the prompt as we go through so you can see we start off with me saying hi there I'm Sam and we\\nHuman: LangChain Agentとはなんですか？\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] [4.79s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"LangChain Agentは、OpenAIのLangChainフレームワークを使用して作成された会話モデルです。このエージェントは、対話の内容を記憶し、それを利用して追加の応答を生成することができます。さまざまな種類のメモリを使用して、会話の要約や追加の情報を提供することができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"LangChain Agentは、OpenAIのLangChainフレームワークを使用して作成された会話モデルです。このエージェントは、対話の内容を記憶し、それを利用して追加の応答を生成することができます。さまざまな種類のメモリを使用して、会話の要約や追加の情報を提供することができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 466,\n",
      "      \"completion_tokens\": 153,\n",
      "      \"total_tokens\": 619\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0613\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [4.79s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"LangChain Agentは、OpenAIのLangChainフレームワークを使用して作成された会話モデルです。このエージェントは、対話の内容を記憶し、それを利用して追加の応答を生成することができます。さまざまな種類のメモリを使用して、会話の要約や追加の情報を提供することができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain] [4.79s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"LangChain Agentは、OpenAIのLangChainフレームワークを使用して作成された会話モデルです。このエージェントは、対話の内容を記憶し、それを利用して追加の応答を生成することができます。さまざまな種類のメモリを使用して、会話の要約や追加の情報を提供することができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] [5.05s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"LangChain Agentは、OpenAIのLangChainフレームワークを使用して作成された会話モデルです。このエージェントは、対話の内容を記憶し、それを利用して追加の応答を生成することができます。さまざまな種類のメモリを使用して、会話の要約や追加の情報を提供することができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "res = cr_chain({\"question\": question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c769c4c8-27fa-410b-a859-2135ea0549c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain Agentは、OpenAIのLangChainフレームワークを使用して作成された会話モデルです。このエージェントは、対話の内容を記憶し、それを利用して追加の応答を生成することができます。さまざまな種類のメモリを使用して、会話の要約や追加の情報を提供することができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e96d9385-20d2-486b-a46c-3e0d38f22118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"具体的な例を教えてください\",\n",
      "  \"chat_history\": [\n",
      "    [\n",
      "      \"LangChain Agentとはなんですか？\",\n",
      "      \"LangChain Agentは、OpenAIのLangChainフレームワークを使用して作成された会話モデルです。このエージェントは、対話の内容を記憶し、それを利用して追加の応答を生成することができます。さまざまな種類のメモリを使用して、会話の要約や追加の情報を提供することができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\"\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"具体的な例を教えてください\",\n",
      "  \"chat_history\": \"\\nHuman: LangChain Agentとはなんですか？\\nAssistant: LangChain Agentは、OpenAIのLangChainフレームワークを使用して作成された会話モデルです。このエージェントは、対話の内容を記憶し、それを利用して追加の応答を生成することができます。さまざまな種類のメモリを使用して、会話の要約や追加の情報を提供することができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: LangChain Agentとはなんですか？\\nAssistant: LangChain Agentは、OpenAIのLangChainフレームワークを使用して作成された会話モデルです。このエージェントは、対話の内容を記憶し、それを利用して追加の応答を生成することができます。さまざまな種類のメモリを使用して、会話の要約や追加の情報を提供することができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\\nFollow Up Input: 具体的な例を教えてください\\nStandalone question:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain > 3:llm:ChatOpenAI] [988ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"具体的な例はありますか？\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"具体的な例はありますか？\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 225,\n",
      "      \"completion_tokens\": 10,\n",
      "      \"total_tokens\": 235\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0613\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain] [989ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"具体的な例はありますか？\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"具体的な例はありますか？\",\n",
      "  \"context\": \"early on in the conversation but a lot of things a lot of times you can sort of fool people with just a very short memory with a memory of just three to five steps back in the conversation you can see if we look at the conversation memory buffer though it's still got the full conversation in there so if we wanted to use that to to look at it or store it some the next one is kind of a combination of the first ones in the now we're gonna have a summary like the second one we looked at but we're also going to have a buffer in there as well right so here we're basically sending it to be have a buffer of the number of tokens where we're going to have basically the tokens is going to be limited to 40 tokens so we could set it you know a few different ways for that but we've instantiated this this memory like that I and then we're going to go through it again so just quickly hi there I'm Sam nothing really different going on here we've got the I need help with my broken TV and you can see it's just taking in steps of the conversation at this point and asked me why I basically tell it's what's wrong and suddenly now in this interaction we've now gone beyond where we were so now it's doing a summary of the early steps so the human Sam introduces themselves to the AI the AI responds and asks what brings Sam to them today and then it go then it gives us the action actual conversation for the last K number of steps or the last number of you know tokens in in this case so you can see here that okay I've entered that it doesn't sound good you know if it's a hardware I say it seems to be a hardware issue and sure enough now the summary is updated as well right so we've lost one uh one step in the conversation but we've updated the summary to include that right the AI expresses sympathy and asks what the problem is in there so this is kind of like giving you a little bit of The Best of Both Worlds in there at any point too we can actually sort of print out the sort of moving\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain > 7:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nearly on in the conversation but a lot of things a lot of times you can sort of fool people with just a very short memory with a memory of just three to five steps back in the conversation you can see if we look at the conversation memory buffer though it's still got the full conversation in there so if we wanted to use that to to look at it or store it some the next one is kind of a combination of the first ones in the now we're gonna have a summary like the second one we looked at but we're also going to have a buffer in there as well right so here we're basically sending it to be have a buffer of the number of tokens where we're going to have basically the tokens is going to be limited to 40 tokens so we could set it you know a few different ways for that but we've instantiated this this memory like that I and then we're going to go through it again so just quickly hi there I'm Sam nothing really different going on here we've got the I need help with my broken TV and you can see it's just taking in steps of the conversation at this point and asked me why I basically tell it's what's wrong and suddenly now in this interaction we've now gone beyond where we were so now it's doing a summary of the early steps so the human Sam introduces themselves to the AI the AI responds and asks what brings Sam to them today and then it go then it gives us the action actual conversation for the last K number of steps or the last number of you know tokens in in this case so you can see here that okay I've entered that it doesn't sound good you know if it's a hardware I say it seems to be a hardware issue and sure enough now the summary is updated as well right so we've lost one uh one step in the conversation but we've updated the summary to include that right the AI expresses sympathy and asks what the problem is in there so this is kind of like giving you a little bit of The Best of Both Worlds in there at any point too we can actually sort of print out the sort of moving\\nHuman: 具体的な例はありますか？\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain > 7:llm:ChatOpenAI] [9.63s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"はい、具体的な例をいくつかご紹介します。\\n\\n例1：\\nAI: こんにちは、私はサムです。どうしたらお手伝いできますか？\\nユーザー: こんにちは、私の壊れたテレビの修理に助けが必要です。\\nAI: それは困りますね。具体的に何が問題なのでしょうか？\\n（ここでAIは会話のバッファを保持し、会話のサマリーを更新します）\\n\\n例2：\\nAI: こんにちは、私はサムです。どうしたらお手伝いできますか？\\nユーザー: こんにちは、私の壊れたテレビの修理に助けが必要です。\\nAI: それは困りますね。具体的に何が問題なのでしょうか？\\nユーザー: おそらくハードウェアの問題です。\\n（ここでAIは会話のバッファを保持し、会話のサマリーを更新します）\\n\\nこれらの例では、AIがユーザーとの会話を進めながら、会話のバッファを保持し、最新の会話のサマリーを提供しています。\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"はい、具体的な例をいくつかご紹介します。\\n\\n例1：\\nAI: こんにちは、私はサムです。どうしたらお手伝いできますか？\\nユーザー: こんにちは、私の壊れたテレビの修理に助けが必要です。\\nAI: それは困りますね。具体的に何が問題なのでしょうか？\\n（ここでAIは会話のバッファを保持し、会話のサマリーを更新します）\\n\\n例2：\\nAI: こんにちは、私はサムです。どうしたらお手伝いできますか？\\nユーザー: こんにちは、私の壊れたテレビの修理に助けが必要です。\\nAI: それは困りますね。具体的に何が問題なのでしょうか？\\nユーザー: おそらくハードウェアの問題です。\\n（ここでAIは会話のバッファを保持し、会話のサマリーを更新します）\\n\\nこれらの例では、AIがユーザーとの会話を進めながら、会話のバッファを保持し、最新の会話のサマリーを提供しています。\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 487,\n",
      "      \"completion_tokens\": 329,\n",
      "      \"total_tokens\": 816\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0613\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain] [9.63s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"はい、具体的な例をいくつかご紹介します。\\n\\n例1：\\nAI: こんにちは、私はサムです。どうしたらお手伝いできますか？\\nユーザー: こんにちは、私の壊れたテレビの修理に助けが必要です。\\nAI: それは困りますね。具体的に何が問題なのでしょうか？\\n（ここでAIは会話のバッファを保持し、会話のサマリーを更新します）\\n\\n例2：\\nAI: こんにちは、私はサムです。どうしたらお手伝いできますか？\\nユーザー: こんにちは、私の壊れたテレビの修理に助けが必要です。\\nAI: それは困りますね。具体的に何が問題なのでしょうか？\\nユーザー: おそらくハードウェアの問題です。\\n（ここでAIは会話のバッファを保持し、会話のサマリーを更新します）\\n\\nこれらの例では、AIがユーザーとの会話を進めながら、会話のバッファを保持し、最新の会話のサマリーを提供しています。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain] [9.63s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"はい、具体的な例をいくつかご紹介します。\\n\\n例1：\\nAI: こんにちは、私はサムです。どうしたらお手伝いできますか？\\nユーザー: こんにちは、私の壊れたテレビの修理に助けが必要です。\\nAI: それは困りますね。具体的に何が問題なのでしょうか？\\n（ここでAIは会話のバッファを保持し、会話のサマリーを更新します）\\n\\n例2：\\nAI: こんにちは、私はサムです。どうしたらお手伝いできますか？\\nユーザー: こんにちは、私の壊れたテレビの修理に助けが必要です。\\nAI: それは困りますね。具体的に何が問題なのでしょうか？\\nユーザー: おそらくハードウェアの問題です。\\n（ここでAIは会話のバッファを保持し、会話のサマリーを更新します）\\n\\nこれらの例では、AIがユーザーとの会話を進めながら、会話のバッファを保持し、最新の会話のサマリーを提供しています。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] [11.14s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"はい、具体的な例をいくつかご紹介します。\\n\\n例1：\\nAI: こんにちは、私はサムです。どうしたらお手伝いできますか？\\nユーザー: こんにちは、私の壊れたテレビの修理に助けが必要です。\\nAI: それは困りますね。具体的に何が問題なのでしょうか？\\n（ここでAIは会話のバッファを保持し、会話のサマリーを更新します）\\n\\n例2：\\nAI: こんにちは、私はサムです。どうしたらお手伝いできますか？\\nユーザー: こんにちは、私の壊れたテレビの修理に助けが必要です。\\nAI: それは困りますね。具体的に何が問題なのでしょうか？\\nユーザー: おそらくハードウェアの問題です。\\n（ここでAIは会話のバッファを保持し、会話のサマリーを更新します）\\n\\nこれらの例では、AIがユーザーとの会話を進めながら、会話のバッファを保持し、最新の会話のサマリーを提供しています。\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chat_history.append((question, res[\"answer\"]))\n",
    "res = cr_chain({\"question\": follow_up_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a4a5aaa-df7f-4348-9180-f3023bd33a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'はい、具体的な例をいくつかご紹介します。\\n\\n例1：\\nAI: こんにちは、私はサムです。どうしたらお手伝いできますか？\\nユーザー: こんにちは、私の壊れたテレビの修理に助けが必要です。\\nAI: それは困りますね。具体的に何が問題なのでしょうか？\\n（ここでAIは会話のバッファを保持し、会話のサマリーを更新します）\\n\\n例2：\\nAI: こんにちは、私はサムです。どうしたらお手伝いできますか？\\nユーザー: こんにちは、私の壊れたテレビの修理に助けが必要です。\\nAI: それは困りますね。具体的に何が問題なのでしょうか？\\nユーザー: おそらくハードウェアの問題です。\\n（ここでAIは会話のバッファを保持し、会話のサマリーを更新します）\\n\\nこれらの例では、AIがユーザーとの会話を進めながら、会話のバッファを保持し、最新の会話のサマリーを提供しています。'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf4b6e15-bdb3-4833-be9c-18aee0e6a13c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "次の会話とフォローアップの質問がある場合、会話を考慮してフォローアップの質問を独立した質問に言い換えなさい。\n",
    "\n",
    "会話:\n",
    "{chat_history}\n",
    "フォローアップの質問: {question}\n",
    "独立した質問:\"\"\"\n",
    "\n",
    "cr_chain.question_generator.prompt.template = prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a0dd8c9-8e0e-4057-8173-ea1ca2911bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"具体的な例を教えてください\",\n",
      "  \"chat_history\": [\n",
      "    [\n",
      "      \"LangChain Agentとはなんですか？\",\n",
      "      \"LangChain Agentは、OpenAIのLangChainフレームワークを使用して作成された会話モデルです。このエージェントは、対話の内容を記憶し、それを利用して追加の応答を生成することができます。さまざまな種類のメモリを使用して、会話の要約や追加の情報を提供することができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\"\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"具体的な例を教えてください\",\n",
      "  \"chat_history\": \"\\nHuman: LangChain Agentとはなんですか？\\nAssistant: LangChain Agentは、OpenAIのLangChainフレームワークを使用して作成された会話モデルです。このエージェントは、対話の内容を記憶し、それを利用して追加の応答を生成することができます。さまざまな種類のメモリを使用して、会話の要約や追加の情報を提供することができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n次の会話とフォローアップの質問がある場合、会話を考慮してフォローアップの質問を独立した質問に言い換えなさい。\\n\\n会話:\\n\\nHuman: LangChain Agentとはなんですか？\\nAssistant: LangChain Agentは、OpenAIのLangChainフレームワークを使用して作成された会話モデルです。このエージェントは、対話の内容を記憶し、それを利用して追加の応答を生成することができます。さまざまな種類のメモリを使用して、会話の要約や追加の情報を提供することができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\\nフォローアップの質問: 具体的な例を教えてください\\n独立した質問:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain > 3:llm:ChatOpenAI] [686ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"LangChain Agentは何に使用されますか？\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"LangChain Agentは何に使用されますか？\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 271,\n",
      "      \"completion_tokens\": 11,\n",
      "      \"total_tokens\": 282\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0613\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain] [688ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"LangChain Agentは何に使用されますか？\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"LangChain Agentは何に使用されますか？\",\n",
      "  \"context\": \"that do things like summarize the conversation as it goes along we've got other ones where we limit the window so it only remembers the last few encounters and then we've got sort of things where you know it merges some of those so where it has the last few but then it does a summary for everything else the summary though is actually done by calling out to a language model itself and asking it hey summarize this conversation so that's something that you will see as well then we've got some sort of more external ways of doing this so these can be things like making some kind of Knowledge Graph memory and putting things into sort of entity memory for doing these kinds of things and then lastly you can just customize it right so you can come up if you've got a very unique case that you want to use the memory something yourself you can actually write your own system for doing memory with this in Lang chain as well let's jump into the code and look at how these things work Okay so we've got the normal sort of setup of installing open AI Lang chain Etc put your opening I key in there and the first one we're going to look at is basically the conversation buffer memory so in Lane chamber we need to import the type of memory it is and then we will basically instantiate that and then we'll pass it into the chain so the chain that we're using here is just a simple conversation chain which allows us to basically converse with the model by doing composition predict and then passing in what we want to say and you'll see that as we go through this that I've got it set to verbose equals true so that we can see what the prompt is going in and we can then see the response that we're getting back so here we're basically setting up the simplest kind of memory the conversation buffer and it's just going to basically track what the user says what the agent says and then just stack that into the prompt as we go through so you can see we start off with me saying hi there I'm Sam and we\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain > 7:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nthat do things like summarize the conversation as it goes along we've got other ones where we limit the window so it only remembers the last few encounters and then we've got sort of things where you know it merges some of those so where it has the last few but then it does a summary for everything else the summary though is actually done by calling out to a language model itself and asking it hey summarize this conversation so that's something that you will see as well then we've got some sort of more external ways of doing this so these can be things like making some kind of Knowledge Graph memory and putting things into sort of entity memory for doing these kinds of things and then lastly you can just customize it right so you can come up if you've got a very unique case that you want to use the memory something yourself you can actually write your own system for doing memory with this in Lang chain as well let's jump into the code and look at how these things work Okay so we've got the normal sort of setup of installing open AI Lang chain Etc put your opening I key in there and the first one we're going to look at is basically the conversation buffer memory so in Lane chamber we need to import the type of memory it is and then we will basically instantiate that and then we'll pass it into the chain so the chain that we're using here is just a simple conversation chain which allows us to basically converse with the model by doing composition predict and then passing in what we want to say and you'll see that as we go through this that I've got it set to verbose equals true so that we can see what the prompt is going in and we can then see the response that we're getting back so here we're basically setting up the simplest kind of memory the conversation buffer and it's just going to basically track what the user says what the agent says and then just stack that into the prompt as we go through so you can see we start off with me saying hi there I'm Sam and we\\nHuman: LangChain Agentは何に使用されますか？\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain > 7:llm:ChatOpenAI] [5.23s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"LangChain Agentは、会話モデルと連携して、対話型のAIチャットボットやアシスタントを作成するために使用されます。このエージェントは、ユーザーとの対話を理解し、適切な応答を生成することができます。また、記憶や要約などの機能を組み合わせて、より高度な対話体験を提供することも可能です。具体的には、会話の記憶や要約、知識グラフの構築など、様々な応用があります。\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"LangChain Agentは、会話モデルと連携して、対話型のAIチャットボットやアシスタントを作成するために使用されます。このエージェントは、ユーザーとの対話を理解し、適切な応答を生成することができます。また、記憶や要約などの機能を組み合わせて、より高度な対話体験を提供することも可能です。具体的には、会話の記憶や要約、知識グラフの構築など、様々な応用があります。\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 467,\n",
      "      \"completion_tokens\": 181,\n",
      "      \"total_tokens\": 648\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0613\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain] [5.23s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"LangChain Agentは、会話モデルと連携して、対話型のAIチャットボットやアシスタントを作成するために使用されます。このエージェントは、ユーザーとの対話を理解し、適切な応答を生成することができます。また、記憶や要約などの機能を組み合わせて、より高度な対話体験を提供することも可能です。具体的には、会話の記憶や要約、知識グラフの構築など、様々な応用があります。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain] [5.23s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"LangChain Agentは、会話モデルと連携して、対話型のAIチャットボットやアシスタントを作成するために使用されます。このエージェントは、ユーザーとの対話を理解し、適切な応答を生成することができます。また、記憶や要約などの機能を組み合わせて、より高度な対話体験を提供することも可能です。具体的には、会話の記憶や要約、知識グラフの構築など、様々な応用があります。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] [6.20s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"LangChain Agentは、会話モデルと連携して、対話型のAIチャットボットやアシスタントを作成するために使用されます。このエージェントは、ユーザーとの対話を理解し、適切な応答を生成することができます。また、記憶や要約などの機能を組み合わせて、より高度な対話体験を提供することも可能です。具体的には、会話の記憶や要約、知識グラフの構築など、様々な応用があります。\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "res = cr_chain({\"question\": follow_up_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "635ba95d-59fe-48aa-96ec-288e74f7e2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain Agentは、会話モデルと連携して、対話型のAIチャットボットやアシスタントを作成するために使用されます。このエージェントは、ユーザーとの対話を理解し、適切な応答を生成することができます。また、記憶や要約などの機能を組み合わせて、より高度な対話体験を提供することも可能です。具体的には、会話の記憶や要約、知識グラフの構築など、様々な応用があります。'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9cb938-62fd-403c-9417-4257f4ee7c65",
   "metadata": {},
   "source": [
    "### Memoryを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "668287b0-468c-47e8-b7ee-fe7278954eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c09af3c6-843f-402e-b942-b6baf7467469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"chat_history\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa69915d-8cd9-41f5-a1c5-9742e5b0c39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_memory = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    condense_question_prompt=prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "278abc2b-8005-4f4f-83ea-61d3ec5a3f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"LangChain Agentとはなんですか？\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"LangChain Agentとはなんですか？\",\n",
      "  \"context\": \"that do things like summarize the conversation as it goes along we've got other ones where we limit the window so it only remembers the last few encounters and then we've got sort of things where you know it merges some of those so where it has the last few but then it does a summary for everything else the summary though is actually done by calling out to a language model itself and asking it hey summarize this conversation so that's something that you will see as well then we've got some sort of more external ways of doing this so these can be things like making some kind of Knowledge Graph memory and putting things into sort of entity memory for doing these kinds of things and then lastly you can just customize it right so you can come up if you've got a very unique case that you want to use the memory something yourself you can actually write your own system for doing memory with this in Lang chain as well let's jump into the code and look at how these things work Okay so we've got the normal sort of setup of installing open AI Lang chain Etc put your opening I key in there and the first one we're going to look at is basically the conversation buffer memory so in Lane chamber we need to import the type of memory it is and then we will basically instantiate that and then we'll pass it into the chain so the chain that we're using here is just a simple conversation chain which allows us to basically converse with the model by doing composition predict and then passing in what we want to say and you'll see that as we go through this that I've got it set to verbose equals true so that we can see what the prompt is going in and we can then see the response that we're getting back so here we're basically setting up the simplest kind of memory the conversation buffer and it's just going to basically track what the user says what the agent says and then just stack that into the prompt as we go through so you can see we start off with me saying hi there I'm Sam and we\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nthat do things like summarize the conversation as it goes along we've got other ones where we limit the window so it only remembers the last few encounters and then we've got sort of things where you know it merges some of those so where it has the last few but then it does a summary for everything else the summary though is actually done by calling out to a language model itself and asking it hey summarize this conversation so that's something that you will see as well then we've got some sort of more external ways of doing this so these can be things like making some kind of Knowledge Graph memory and putting things into sort of entity memory for doing these kinds of things and then lastly you can just customize it right so you can come up if you've got a very unique case that you want to use the memory something yourself you can actually write your own system for doing memory with this in Lang chain as well let's jump into the code and look at how these things work Okay so we've got the normal sort of setup of installing open AI Lang chain Etc put your opening I key in there and the first one we're going to look at is basically the conversation buffer memory so in Lane chamber we need to import the type of memory it is and then we will basically instantiate that and then we'll pass it into the chain so the chain that we're using here is just a simple conversation chain which allows us to basically converse with the model by doing composition predict and then passing in what we want to say and you'll see that as we go through this that I've got it set to verbose equals true so that we can see what the prompt is going in and we can then see the response that we're getting back so here we're basically setting up the simplest kind of memory the conversation buffer and it's just going to basically track what the user says what the agent says and then just stack that into the prompt as we go through so you can see we start off with me saying hi there I'm Sam and we\\nHuman: LangChain Agentとはなんですか？\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] [5.42s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うプログラムです。LangChainは、言語モデルを使用して自然な対話を生成するためのフレームワークです。LangChain Agentは、対話のコンテキストを管理し、過去の対話を記憶し、それに基づいて回答を生成することができます。さまざまな種類のメモリを使用して、対話の要約やメモリ管理を行うことができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うプログラムです。LangChainは、言語モデルを使用して自然な対話を生成するためのフレームワークです。LangChain Agentは、対話のコンテキストを管理し、過去の対話を記憶し、それに基づいて回答を生成することができます。さまざまな種類のメモリを使用して、対話の要約やメモリ管理を行うことができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 466,\n",
      "      \"completion_tokens\": 194,\n",
      "      \"total_tokens\": 660\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0613\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [5.42s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うプログラムです。LangChainは、言語モデルを使用して自然な対話を生成するためのフレームワークです。LangChain Agentは、対話のコンテキストを管理し、過去の対話を記憶し、それに基づいて回答を生成することができます。さまざまな種類のメモリを使用して、対話の要約やメモリ管理を行うことができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain] [5.42s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うプログラムです。LangChainは、言語モデルを使用して自然な対話を生成するためのフレームワークです。LangChain Agentは、対話のコンテキストを管理し、過去の対話を記憶し、それに基づいて回答を生成することができます。さまざまな種類のメモリを使用して、対話の要約やメモリ管理を行うことができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] [5.81s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うプログラムです。LangChainは、言語モデルを使用して自然な対話を生成するためのフレームワークです。LangChain Agentは、対話のコンテキストを管理し、過去の対話を記憶し、それに基づいて回答を生成することができます。さまざまな種類のメモリを使用して、対話の要約やメモリ管理を行うことができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'LangChain Agentとはなんですか？',\n",
       " 'chat_history': [HumanMessage(content='LangChain Agentとはなんですか？'),\n",
       "  AIMessage(content='LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うプログラムです。LangChainは、言語モデルを使用して自然な対話を生成するためのフレームワークです。LangChain Agentは、対話のコンテキストを管理し、過去の対話を記憶し、それに基づいて回答を生成することができます。さまざまな種類のメモリを使用して、対話の要約やメモリ管理を行うことができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。')],\n",
       " 'answer': 'LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うプログラムです。LangChainは、言語モデルを使用して自然な対話を生成するためのフレームワークです。LangChain Agentは、対話のコンテキストを管理し、過去の対話を記憶し、それに基づいて回答を生成することができます。さまざまな種類のメモリを使用して、対話の要約やメモリ管理を行うことができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_memory(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4475d52-c4ef-424f-bbad-b73612306906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"具体的な例を教えてください\",\n",
      "  \"chat_history\": \"\\nHuman: LangChain Agentとはなんですか？\\nAssistant: LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うプログラムです。LangChainは、言語モデルを使用して自然な対話を生成するためのフレームワークです。LangChain Agentは、対話のコンテキストを管理し、過去の対話を記憶し、それに基づいて回答を生成することができます。さまざまな種類のメモリを使用して、対話の要約やメモリ管理を行うことができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n次の会話とフォローアップの質問がある場合、会話を考慮してフォローアップの質問を独立した質問に言い換えなさい。\\n\\n会話:\\n\\nHuman: LangChain Agentとはなんですか？\\nAssistant: LangChain Agentは、OpenAIのLangChainモデルを使用して対話を行うプログラムです。LangChainは、言語モデルを使用して自然な対話を生成するためのフレームワークです。LangChain Agentは、対話のコンテキストを管理し、過去の対話を記憶し、それに基づいて回答を生成することができます。さまざまな種類のメモリを使用して、対話の要約やメモリ管理を行うことができます。また、カスタマイズも可能で、独自のメモリシステムを作成することもできます。\\nフォローアップの質問: 具体的な例を教えてください\\n独立した質問:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain > 3:llm:ChatOpenAI] [1.10s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"LangChain Agentは何に基づいて回答を生成することができますか？\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"LangChain Agentは何に基づいて回答を生成することができますか？\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 312,\n",
      "      \"completion_tokens\": 25,\n",
      "      \"total_tokens\": 337\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0613\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain] [1.10s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"LangChain Agentは何に基づいて回答を生成することができますか？\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"LangChain Agentは何に基づいて回答を生成することができますか？\",\n",
      "  \"context\": \"that do things like summarize the conversation as it goes along we've got other ones where we limit the window so it only remembers the last few encounters and then we've got sort of things where you know it merges some of those so where it has the last few but then it does a summary for everything else the summary though is actually done by calling out to a language model itself and asking it hey summarize this conversation so that's something that you will see as well then we've got some sort of more external ways of doing this so these can be things like making some kind of Knowledge Graph memory and putting things into sort of entity memory for doing these kinds of things and then lastly you can just customize it right so you can come up if you've got a very unique case that you want to use the memory something yourself you can actually write your own system for doing memory with this in Lang chain as well let's jump into the code and look at how these things work Okay so we've got the normal sort of setup of installing open AI Lang chain Etc put your opening I key in there and the first one we're going to look at is basically the conversation buffer memory so in Lane chamber we need to import the type of memory it is and then we will basically instantiate that and then we'll pass it into the chain so the chain that we're using here is just a simple conversation chain which allows us to basically converse with the model by doing composition predict and then passing in what we want to say and you'll see that as we go through this that I've got it set to verbose equals true so that we can see what the prompt is going in and we can then see the response that we're getting back so here we're basically setting up the simplest kind of memory the conversation buffer and it's just going to basically track what the user says what the agent says and then just stack that into the prompt as we go through so you can see we start off with me saying hi there I'm Sam and we\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain > 7:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nthat do things like summarize the conversation as it goes along we've got other ones where we limit the window so it only remembers the last few encounters and then we've got sort of things where you know it merges some of those so where it has the last few but then it does a summary for everything else the summary though is actually done by calling out to a language model itself and asking it hey summarize this conversation so that's something that you will see as well then we've got some sort of more external ways of doing this so these can be things like making some kind of Knowledge Graph memory and putting things into sort of entity memory for doing these kinds of things and then lastly you can just customize it right so you can come up if you've got a very unique case that you want to use the memory something yourself you can actually write your own system for doing memory with this in Lang chain as well let's jump into the code and look at how these things work Okay so we've got the normal sort of setup of installing open AI Lang chain Etc put your opening I key in there and the first one we're going to look at is basically the conversation buffer memory so in Lane chamber we need to import the type of memory it is and then we will basically instantiate that and then we'll pass it into the chain so the chain that we're using here is just a simple conversation chain which allows us to basically converse with the model by doing composition predict and then passing in what we want to say and you'll see that as we go through this that I've got it set to verbose equals true so that we can see what the prompt is going in and we can then see the response that we're getting back so here we're basically setting up the simplest kind of memory the conversation buffer and it's just going to basically track what the user says what the agent says and then just stack that into the prompt as we go through so you can see we start off with me saying hi there I'm Sam and we\\nHuman: LangChain Agentは何に基づいて回答を生成することができますか？\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain > 7:llm:ChatOpenAI] [3.88s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"LangChain Agentは、様々な方法で回答を生成することができます。これには、会話の要約や過去のエンカウンターの記憶、言語モデルへの問い合わせなどが含まれます。また、カスタマイズすることも可能です。LangChainでは、異なる種類のメモリを使用したり、独自のメモリシステムを作成したりすることができます。\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"LangChain Agentは、様々な方法で回答を生成することができます。これには、会話の要約や過去のエンカウンターの記憶、言語モデルへの問い合わせなどが含まれます。また、カスタマイズすることも可能です。LangChainでは、異なる種類のメモリを使用したり、独自のメモリシステムを作成したりすることができます。\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 481,\n",
      "      \"completion_tokens\": 140,\n",
      "      \"total_tokens\": 621\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0613\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain] [3.88s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"LangChain Agentは、様々な方法で回答を生成することができます。これには、会話の要約や過去のエンカウンターの記憶、言語モデルへの問い合わせなどが含まれます。また、カスタマイズすることも可能です。LangChainでは、異なる種類のメモリを使用したり、独自のメモリシステムを作成したりすることができます。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain] [3.88s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"LangChain Agentは、様々な方法で回答を生成することができます。これには、会話の要約や過去のエンカウンターの記憶、言語モデルへの問い合わせなどが含まれます。また、カスタマイズすることも可能です。LangChainでは、異なる種類のメモリを使用したり、独自のメモリシステムを作成したりすることができます。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] [5.25s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"LangChain Agentは、様々な方法で回答を生成することができます。これには、会話の要約や過去のエンカウンターの記憶、言語モデルへの問い合わせなどが含まれます。また、カスタマイズすることも可能です。LangChainでは、異なる種類のメモリを使用したり、独自のメモリシステムを作成したりすることができます。\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "res = chain_with_memory(follow_up_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b590b2e7-5bca-4792-a787-c8f15fb219e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain Agentは、様々な方法で回答を生成することができます。これには、会話の要約や過去のエンカウンターの記憶、言語モデルへの問い合わせなどが含まれます。また、カスタマイズすることも可能です。LangChainでは、異なる種類のメモリを使用したり、独自のメモリシステムを作成したりすることができます。'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca25d6d-64c5-41a5-bb10-1df7863453ba",
   "metadata": {},
   "source": [
    "### デフォルトで使われているプロンプト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc4994a8-e757-41e1-96df-b068237b672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88224678-5886-42ce-bb05-ed52c7faa8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "次の会話とフォローアップの質問がある場合、会話を考慮してフォローアップの質問を独立した質問に言い換えなさい。\n",
      "\n",
      "会話:\n",
      "{chat_history}\n",
      "フォローアップの質問: {question}\n",
      "独立した質問:\n"
     ]
    }
   ],
   "source": [
    "print(CONDENSE_QUESTION_PROMPT.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34ef261b-4b2b-43f0-b19e-45a0d83707d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "{context}\n",
      "\n",
      "Question: {question}\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "print(QA_PROMPT.template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
