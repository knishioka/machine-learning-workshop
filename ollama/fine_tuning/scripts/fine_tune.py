#!/usr/bin/env python3
"""
Ollama ãƒ¢ãƒ‡ãƒ«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆç”¨ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å‹•ä½œã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â€»ã“ã‚Œã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§ã‚ã‚Šã€çœŸã®fine-tuningï¼ˆãƒ¢ãƒ‡ãƒ«é‡ã¿ã®æ›´æ–°ï¼‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“
"""

import json
import subprocess
import sys
import os
from typing import List, Dict
import argparse


def load_training_data(file_path: str) -> List[Dict]:
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line.strip()))
    return data


def prepare_training_prompts(data: List[Dict]) -> str:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ãƒ‡ãƒ¼ã‚¿ã‚’Ollamaå½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›"""
    prompts = []
    for item in data:
        instruction = item.get('instruction', '')
        input_text = item.get('input', '')
        output = item.get('output', '')
        
        if input_text:
            prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
        else:
            prompt = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"
        
        prompts.append(prompt)
    
    return "\n\n---\n\n".join(prompts)


def create_modelfile(base_model: str, training_prompts: str, model_name: str) -> str:
    """ãƒ¢ãƒ‡ãƒ«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®Modelfileã‚’ä½œæˆ"""
    modelfile_content = f"""FROM {base_model}

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
SYSTEM ã‚ãªãŸã¯è¦ªåˆ‡ã§ä¸å¯§ãªã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãŠå®¢æ§˜ã®è³ªå•ã«å¯¾ã—ã¦ã€æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_predict 512
"""
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ã‚’è¿½åŠ ï¼ˆFew-shotå­¦ç¿’ï¼‰
    for prompt in training_prompts.split("\n\n---\n\n"):
        user_msg = prompt.split('### Response:')[0].strip().replace('\n', ' ')
        assistant_msg = prompt.split('### Response:')[1].strip().replace('\n', ' ')
        modelfile_content += f"\nMESSAGE user {user_msg}"
        modelfile_content += f"\nMESSAGE assistant {assistant_msg}"
    
    modelfile_path = f"Modelfile.{model_name}"
    with open(modelfile_path, 'w', encoding='utf-8') as f:
        f.write(modelfile_content)
    
    return modelfile_path


def fine_tune_model(modelfile_path: str, model_name: str):
    """Ollamaã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º"""
    print(f"ãƒ¢ãƒ‡ãƒ«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹: {model_name}")
    
    try:
        # Ollamaã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        cmd = f"ollama create {model_name} -f {modelfile_path}"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        if result.returncode == 0:
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†: {model_name}")
            print("å‡ºåŠ›:", result.stdout)
        else:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•—")
            print("ã‚¨ãƒ©ãƒ¼:", result.stderr)
            sys.exit(1)
            
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)


def test_model(model_name: str):
    """ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ"""
    test_prompts = [
        "é…é€ã«ã¯ã©ã®ãã‚‰ã„æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã‹ï¼Ÿ",
        "è¿”å“ã¯ã§ãã¾ã™ã‹ï¼Ÿ",
        "æ”¯æ‰•ã„æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„"
    ]
    
    print(f"\nğŸ§ª ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ: {model_name}")
    print("-" * 50)
    
    for prompt in test_prompts:
        print(f"\nè³ªå•: {prompt}")
        cmd = f'ollama run {model_name} "{prompt}"'
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        print(f"å›ç­”: {result.stdout.strip()}")
        print("-" * 50)


def main():
    parser = argparse.ArgumentParser(description='Ollama ãƒ¢ãƒ‡ãƒ«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--data', type=str, default='training_data.jsonl',
                        help='ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ãƒ‡ãƒ¼ã‚¿ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--base-model', type=str, default='llama3.2:1b',
                        help='ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹Ollamaãƒ¢ãƒ‡ãƒ«')
    parser.add_argument('--model-name', type=str, default='customer-support',
                        help='ä½œæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®åå‰')
    parser.add_argument('--test', action='store_true',
                        help='ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³å¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ')
    
    args = parser.parse_args()
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    print(f"ğŸ“š ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {args.data}")
    training_data = load_training_data(args.data)
    print(f"âœ… {len(training_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æº–å‚™
    print("\nğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ã‚’æº–å‚™ä¸­...")
    training_prompts = prepare_training_prompts(training_data)
    
    # Modelfileã‚’ä½œæˆ
    print("\nğŸ“„ Modelfileã‚’ä½œæˆä¸­...")
    modelfile_path = create_modelfile(args.base_model, training_prompts, args.model_name)
    print(f"âœ… Modelfileã‚’ä½œæˆã—ã¾ã—ãŸ: {modelfile_path}")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
    print(f"\nğŸš€ ãƒ¢ãƒ‡ãƒ«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¾ã™...")
    fine_tune_model(modelfile_path, args.model_name)
    
    # ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    if args.test:
        test_model(args.model_name)
    
    print(f"\nâœ¨ å®Œäº†ï¼ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ« '{args.model_name}' ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚")
    print(f"ä½¿ç”¨æ–¹æ³•: ollama run {args.model_name}")


if __name__ == "__main__":
    main()