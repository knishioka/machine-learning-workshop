#!/usr/bin/env python3
"""
Mac Local Fine-tuning Script
Apple Silicon Mac (M1/M2/M3)ã§æœ€å°é™ã®fine-tuningã‚’å®Ÿè¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
MLXãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦åŠ¹ç‡çš„ã«å‹•ä½œ
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path
from typing import List, Dict

# ã‚«ãƒ©ãƒ¼å‡ºåŠ›ç”¨
class Colors:
    BLUE = '\033[94m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    END = '\033[0m'

def print_status(message: str, status: str = "info"):
    """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è‰²ä»˜ãã§è¡¨ç¤º"""
    if status == "info":
        print(f"{Colors.BLUE}â„¹ï¸  {message}{Colors.END}")
    elif status == "success":
        print(f"{Colors.GREEN}âœ… {message}{Colors.END}")
    elif status == "warning":
        print(f"{Colors.YELLOW}âš ï¸  {message}{Colors.END}")
    elif status == "error":
        print(f"{Colors.RED}âŒ {message}{Colors.END}")

def check_mac_platform():
    """Macãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®ãƒã‚§ãƒƒã‚¯"""
    import platform
    
    if platform.system() != "Darwin":
        print_status("ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯macOSå°‚ç”¨ã§ã™", "error")
        sys.exit(1)
    
    # Apple Siliconã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
    processor = platform.processor()
    if "arm" in processor.lower():
        print_status("Apple Silicon Macæ¤œå‡º âœ¨", "success")
        return "apple_silicon"
    else:
        print_status("Intel Macæ¤œå‡ºï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒåˆ¶é™ã•ã‚Œã¾ã™ï¼‰", "warning")
        return "intel"

def check_dependencies():
    """å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯"""
    dependencies = {
        "mlx": "pip install mlx mlx-lm",
        "transformers": "pip install transformers",
        "torch": "pip install torch torchvision torchaudio",
        "peft": "pip install peft"
    }
    
    missing = []
    for package, install_cmd in dependencies.items():
        try:
            __import__(package)
        except ImportError:
            missing.append((package, install_cmd))
    
    if missing:
        print_status("å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒä¸è¶³ã—ã¦ã„ã¾ã™:", "error")
        for pkg, cmd in missing:
            print(f"  - {pkg}: {cmd}")
        print("\nå¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)
    
    print_status("ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯å®Œäº†", "success")

def create_sample_data() -> List[Dict]:
    """æœ€å°é™ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    data = [
        {
            "text": "### Human: Pythonã§ãƒªã‚¹ãƒˆã‚’é€†é †ã«ã™ã‚‹æ–¹æ³•ã¯ï¼Ÿ\n### Assistant: ãƒªã‚¹ãƒˆã‚’é€†é †ã«ã™ã‚‹æ–¹æ³•ã¯3ã¤ã‚ã‚Šã¾ã™ï¼š\n1. `reversed()`: `list(reversed(my_list))`\n2. ã‚¹ãƒ©ã‚¤ã‚¹: `my_list[::-1]`\n3. `reverse()`: `my_list.reverse()` (å…ƒã®ãƒªã‚¹ãƒˆã‚’å¤‰æ›´)"
        },
        {
            "text": "### Human: gitã§ã‚³ãƒŸãƒƒãƒˆã‚’å–ã‚Šæ¶ˆã™ã«ã¯ï¼Ÿ\n### Assistant: gitã§ã‚³ãƒŸãƒƒãƒˆã‚’å–ã‚Šæ¶ˆã™æ–¹æ³•ï¼š\n- `git reset --soft HEAD~1`: ã‚³ãƒŸãƒƒãƒˆã®ã¿å–ã‚Šæ¶ˆã—\n- `git reset --hard HEAD~1`: å¤‰æ›´ã”ã¨å–ã‚Šæ¶ˆã—\n- `git commit --amend`: ç›´å‰ã®ã‚³ãƒŸãƒƒãƒˆã‚’ä¿®æ­£"
        },
        {
            "text": "### Human: Dockerã‚³ãƒ³ãƒ†ãƒŠãŒèµ·å‹•ã—ãªã„\n### Assistant: ç¢ºèªäº‹é …ï¼š\n1. `docker logs <container_id>`: ãƒ­ã‚°ç¢ºèª\n2. `docker ps -a`: ã‚³ãƒ³ãƒ†ãƒŠçŠ¶æ…‹ç¢ºèª\n3. Dockerfileã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆç¢ºèª"
        },
        {
            "text": "### Human: TypeError: 'NoneType' object is not iterable\n### Assistant: Noneã«å¯¾ã—ã¦ç¹°ã‚Šè¿”ã—å‡¦ç†ã‚’è¡ŒãŠã†ã¨ã—ãŸæ™‚ã®ã‚¨ãƒ©ãƒ¼ã§ã™ã€‚\nå¯¾å‡¦æ³•ï¼š\n1. å¤‰æ•°ãŒNoneã§ãªã„ã‹ç¢ºèª: `if data is not None:`\n2. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š: `data = data or []`"
        },
        {
            "text": "### Human: APIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒé…ã„\n### Assistant: APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ”¹å–„æ–¹æ³•ï¼š\n1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å®Ÿè£…ï¼ˆRedisç­‰ï¼‰\n2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã®æœ€é©åŒ–\n3. ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®å°å…¥\n4. éåŒæœŸå‡¦ç†ã®æ´»ç”¨"
        }
    ]
    return data

def fine_tune_with_mlx(data_path: str, output_path: str, iterations: int = 100, model_name: str = None):
    """MLXã‚’ä½¿ç”¨ã—ãŸfine-tuning"""
    print_status("MLXã§ã®fine-tuningé–‹å§‹", "info")
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã®é¸æŠï¼ˆãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦ï¼‰
    if model_name is None:
        # åˆ©ç”¨å¯èƒ½ãªMLXãƒ¢ãƒ‡ãƒ«
        available_models = {
            "small": "mlx-community/Qwen2.5-1.5B-Instruct-4bit",  # 1.5B - æœ€å°
            "medium": "mlx-community/Qwen2.5-3B-Instruct-4bit",   # 3B - ãƒãƒ©ãƒ³ã‚¹
            "large": "mlx-community/Mistral-7B-Instruct-v0.2-4bit" # 7B - é«˜å“è³ª
        }
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å°ã•ã„ãƒ¢ãƒ‡ãƒ«
        model_name = available_models["small"]
        print_status(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_name}", "info")
    
    # MLXã‚³ãƒãƒ³ãƒ‰ã®æ§‹ç¯‰ï¼ˆæ­£ã—ã„ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    cmd = f"""
python -m mlx_lm.lora \\
    --model {model_name} \\
    --train \\
    --data {data_path} \\
    --adapter-path {output_path} \\
    --iters {iterations} \\
    --batch-size 1 \\
    --learning-rate 1e-4
"""
    
    print(f"\nå®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰:\n{cmd}")
    print_status("æ³¨: MLXã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:", "warning")
    print("pip install mlx mlx-lm")
    
    # å®Ÿéš›ã®å®Ÿè¡Œã¯ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã§
    import subprocess
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    
    if result.returncode == 0:
        print_status("Fine-tuningå®Œäº†ï¼", "success")
    else:
        print_status(f"ã‚¨ãƒ©ãƒ¼: {result.stderr}", "error")
        print_status("ä»£æ›¿æ¡ˆ: PyTorchã‚’ä½¿ç”¨ã—ãŸfine-tuningã‚’è©¦ã—ã¦ãã ã•ã„ (--method pytorch)", "info")
        return False
    
    return True

def fine_tune_with_pytorch(data: List[Dict], model_name: str, output_dir: str):
    """PyTorch + PEFTã‚’ä½¿ç”¨ã—ãŸfine-tuningï¼ˆä»£æ›¿æ‰‹æ³•ï¼‰"""
    print_status("PyTorch + PEFTã§ã®fine-tuningé–‹å§‹", "info")
    
    try:
        import torch
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            TrainingArguments,
            Trainer,
            DataCollatorForLanguageModeling
        )
        from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
        from datasets import Dataset
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        device = "mps" if torch.backends.mps.is_available() else "cpu"
        print_status(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}", "info")
        
        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
        print_status("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...", "info")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            load_in_8bit=True if device == "cpu" else False,
            device_map="auto",
            trust_remote_code=True
        )
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.pad_token = tokenizer.eos_token
        
        # LoRAè¨­å®š
        peft_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.1,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        # ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
        model = prepare_model_for_kbit_training(model)
        model = get_peft_model(model, peft_config)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
        def tokenize_function(examples):
            return tokenizer(examples["text"], padding=True, truncation=True, max_length=512)
        
        dataset = Dataset.from_list(data)
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=4,
            warmup_steps=10,
            logging_steps=10,
            save_steps=50,
            eval_steps=50,
            save_total_limit=2,
            load_best_model_at_end=True,
            fp16=False,  # MPSã§ã¯FP16éå¯¾å¿œ
            push_to_hub=False,
        )
        
        # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
        print_status("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ï¼ˆã“ã‚Œã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰...", "warning")
        start_time = time.time()
        trainer.train()
        end_time = time.time()
        
        print_status(f"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼ æ‰€è¦æ™‚é–“: {end_time - start_time:.2f}ç§’", "success")
        
        # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        return True
        
    except Exception as e:
        print_status(f"ã‚¨ãƒ©ãƒ¼: {str(e)}", "error")
        return False

def convert_to_gguf(model_path: str, output_path: str):
    """ãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›ï¼ˆOllamaç”¨ï¼‰"""
    print_status("GGUFå½¢å¼ã¸ã®å¤‰æ›", "info")
    
    # å¤‰æ›ã‚³ãƒãƒ³ãƒ‰ï¼ˆllama.cppã®convert.pyã‚’ä½¿ç”¨ï¼‰
    cmd = f"""
python convert.py {model_path} \\
    --outfile {output_path} \\
    --outtype q4_0
"""
    
    print(f"\nå¤‰æ›ã«ã¯llama.cppã®convert.pyãŒå¿…è¦ã§ã™ã€‚")
    print(f"æ‰‹å‹•ã§å®Ÿè¡Œã—ã¦ãã ã•ã„:\n{cmd}")
    
    return True

def create_modelfile(gguf_path: str, output_path: str):
    """Ollamaç”¨ã®Modelfileã‚’ä½œæˆ"""
    content = f"""FROM {gguf_path}

SYSTEM "ã‚ãªãŸã¯æŠ€è¡“çš„ãªè³ªå•ã«ç­”ãˆã‚‹è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€ã‚¨ãƒ©ãƒ¼è§£æ±ºã€é–‹ç™ºãƒ„ãƒ¼ãƒ«ã«ã¤ã„ã¦çš„ç¢ºãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚"

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.1
"""
    
    with open(output_path, 'w') as f:
        f.write(content)
    
    print_status(f"Modelfileä½œæˆ: {output_path}", "success")

def main():
    parser = argparse.ArgumentParser(description='Mac Local Fine-tuning Script')
    parser.add_argument('--method', choices=['mlx', 'pytorch'], default='mlx',
                        help='Fine-tuningæ‰‹æ³•ã®é¸æŠ')
    parser.add_argument('--model', type=str, default='TinyLlama/TinyLlama-1.1B-Chat-v1.0',
                        help='ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆPyTorchä½¿ç”¨æ™‚ï¼‰')
    parser.add_argument('--mlx-model', type=str, choices=['small', 'medium', 'large'], default='small',
                        help='MLXãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: small(1.5B), medium(3B), large(7B)')
    parser.add_argument('--iterations', type=int, default=100,
                        help='ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°')
    parser.add_argument('--output-dir', type=str, default='./finetuned_model',
                        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    print(f"\n{Colors.BLUE}=== Mac Local Fine-tuning Script ==={Colors.END}\n")
    
    # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒã‚§ãƒƒã‚¯
    platform_type = check_mac_platform()
    
    # ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
    check_dependencies()
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    print_status("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...", "info")
    data = create_sample_data()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    os.makedirs(args.output_dir, exist_ok=True)
    data_path = os.path.join(args.output_dir, "training_data.jsonl")
    with open(data_path, 'w') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print_status(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {data_path} ({len(data)}ã‚µãƒ³ãƒ—ãƒ«)", "success")
    
    # Fine-tuningå®Ÿè¡Œ
    if args.method == 'mlx' and platform_type == 'apple_silicon':
        print_status("MLXã‚’ä½¿ç”¨ã—ãŸfine-tuningã‚’é¸æŠ", "info")
        # MLXãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ãŸãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—
        mlx_models = {
            "small": "mlx-community/Qwen2.5-1.5B-Instruct-4bit",
            "medium": "mlx-community/Qwen2.5-3B-Instruct-4bit",
            "large": "mlx-community/Mistral-7B-Instruct-v0.2-4bit"
        }
        mlx_model = mlx_models.get(args.mlx_model, mlx_models["small"])
        success = fine_tune_with_mlx(data_path, args.output_dir, args.iterations, mlx_model)
    else:
        if args.method == 'mlx' and platform_type == 'intel':
            print_status("Intel Macã§ã¯MLXã¯æœ€é©ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚PyTorchã‚’ä½¿ç”¨ã—ã¾ã™ã€‚", "warning")
        print_status("PyTorch + PEFTã‚’ä½¿ç”¨ã—ãŸfine-tuningã‚’é¸æŠ", "info")
        success = fine_tune_with_pytorch(data, args.model, args.output_dir)
    
    if success:
        print_status("\nFine-tuningæˆåŠŸï¼", "success")
        
        # GGUFå¤‰æ›ã®æ¡ˆå†…
        print("\n" + "="*50)
        print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. ãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›")
        print("2. Modelfileã‚’ä½œæˆ")
        print("3. Ollamaã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ")
        print("\nä¾‹:")
        print(f"ollama create my-mac-model -f {args.output_dir}/Modelfile")
        
        # Modelfileä½œæˆ
        modelfile_path = os.path.join(args.output_dir, "Modelfile")
        create_modelfile("./model.gguf", modelfile_path)
    else:
        print_status("Fine-tuningã«å¤±æ•—ã—ã¾ã—ãŸ", "error")
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è­¦å‘Š
    print(f"\n{Colors.YELLOW}ğŸ’¡ ãƒ’ãƒ³ãƒˆ:{Colors.END}")
    print("- Fine-tuningä¸­ã¯ä»–ã®é‡ã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‰ã˜ã¦ãã ã•ã„")
    print("- Activity Monitorã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–ã—ã¦ãã ã•ã„")
    print("- é›»æºã«æ¥ç¶šã—ãŸçŠ¶æ…‹ã§å®Ÿè¡Œã—ã¦ãã ã•ã„")
    print("- ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã«ã¯ãƒ¡ãƒ¢ãƒª16GBä»¥ä¸Šã‚’æ¨å¥¨")

if __name__ == "__main__":
    main()