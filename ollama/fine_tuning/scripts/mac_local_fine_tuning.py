#!/usr/bin/env python3
"""
Mac Local Fine-tuning Script
Apple Silicon Mac (M1/M2/M3)ã§æœ€å°é™ã®fine-tuningã‚’å®Ÿè¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
MLXãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦åŠ¹ç‡çš„ã«å‹•ä½œ
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path
from typing import List, Dict

# ã‚«ãƒ©ãƒ¼å‡ºåŠ›ç”¨
class Colors:
    BLUE = '\033[94m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    END = '\033[0m'

def print_status(message: str, status: str = "info"):
    """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è‰²ä»˜ãã§è¡¨ç¤º"""
    if status == "info":
        print(f"{Colors.BLUE}â„¹ï¸  {message}{Colors.END}")
    elif status == "success":
        print(f"{Colors.GREEN}âœ… {message}{Colors.END}")
    elif status == "warning":
        print(f"{Colors.YELLOW}âš ï¸  {message}{Colors.END}")
    elif status == "error":
        print(f"{Colors.RED}âŒ {message}{Colors.END}")

def check_mac_platform():
    """Macãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®ãƒã‚§ãƒƒã‚¯"""
    import platform
    
    if platform.system() != "Darwin":
        print_status("ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯macOSå°‚ç”¨ã§ã™", "error")
        sys.exit(1)
    
    # Apple Siliconã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
    processor = platform.processor()
    if "arm" in processor.lower():
        print_status("Apple Silicon Macæ¤œå‡º âœ¨", "success")
        return "apple_silicon"
    else:
        print_status("Intel Macæ¤œå‡ºï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒåˆ¶é™ã•ã‚Œã¾ã™ï¼‰", "warning")
        return "intel"

def check_dependencies(method="pytorch"):
    """å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯"""
    # å…±é€šã®ä¾å­˜é–¢ä¿‚
    common_dependencies = {
        "transformers": "pip install transformers",
        "torch": "pip install torch torchvision torchaudio",
        "peft": "pip install peft",
        "datasets": "pip install datasets",
        "accelerate": "pip install accelerate"
    }
    
    # MLXç‰¹æœ‰ã®ä¾å­˜é–¢ä¿‚
    mlx_dependencies = {
        "mlx": "pip install mlx mlx-lm"
    }
    
    # ãƒ¡ã‚½ãƒƒãƒ‰ã«å¿œã˜ã¦å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’é¸æŠ
    if method == "mlx":
        dependencies = {**common_dependencies, **mlx_dependencies}
    else:
        dependencies = common_dependencies
    
    missing = []
    for package, install_cmd in dependencies.items():
        try:
            __import__(package)
        except ImportError:
            missing.append((package, install_cmd))
    
    if missing:
        print_status("å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒä¸è¶³ã—ã¦ã„ã¾ã™:", "error")
        for pkg, cmd in missing:
            print(f"  - {pkg}: {cmd}")
        print("\nå¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)
    
    print_status("ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯å®Œäº†", "success")

def create_sample_data() -> List[Dict]:
    """ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    data = [
        {
            "text": "### Human: æ³¨æ–‡ã—ãŸå•†å“ãŒã¾ã å±Šãã¾ã›ã‚“\n### Assistant: ãŠå¾…ãŸã›ã—ã¦ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚æ³¨æ–‡ç•ªå·ã‚’æ•™ãˆã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ\né…é€çŠ¶æ³ã‚’ç¢ºèªã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚é€šå¸¸ã€ã”æ³¨æ–‡ã‹ã‚‰3-5å–¶æ¥­æ—¥ã§ãŠå±Šã‘ã—ã¦ãŠã‚Šã¾ã™ã€‚\né…é€çŠ¶æ³ã¯ã€Œãƒã‚¤ãƒšãƒ¼ã‚¸ã€â†’ã€Œæ³¨æ–‡å±¥æ­´ã€ã‹ã‚‰ã‚‚ã”ç¢ºèªã„ãŸã ã‘ã¾ã™ã€‚"
        },
        {
            "text": "### Human: å•†å“ã‚’è¿”å“ã—ãŸã„ã§ã™\n### Assistant: è¿”å“ã‚’ã”å¸Œæœ›ã¨ã®ã“ã¨ã€æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚\nè¿”å“ã¯å•†å“åˆ°ç€å¾Œ14æ—¥ä»¥å†…ã§ã‚ã‚Œã°å¯èƒ½ã§ã™ã€‚\n1. ãƒã‚¤ãƒšãƒ¼ã‚¸ã‹ã‚‰è¿”å“ç”³è«‹\n2. è¿”å“ãƒ©ãƒ™ãƒ«ã‚’å°åˆ·\n3. å•†å“ã‚’æ¢±åŒ…ã—ã¦ç™ºé€\næœªä½¿ç”¨ãƒ»æœªé–‹å°ã®å•†å“ã«é™ã‚Šã¾ã™ã€‚è¿”é‡‘ã¯è¿”å“ç¢ºèªå¾Œ5-7å–¶æ¥­æ—¥ã§å‡¦ç†ã•ã‚Œã¾ã™ã€‚"
        },
        {
            "text": "### Human: æ”¯æ‰•ã„æ–¹æ³•ã‚’å¤‰æ›´ã§ãã¾ã™ã‹ï¼Ÿ\n### Assistant: ã”æ³¨æ–‡å¾Œã®æ”¯æ‰•ã„æ–¹æ³•å¤‰æ›´ã«ã¤ã„ã¦èª¬æ˜ã„ãŸã—ã¾ã™ã€‚\n- ç™ºé€å‰ï¼šãƒã‚¤ãƒšãƒ¼ã‚¸ã‹ã‚‰å¤‰æ›´å¯èƒ½\n- ç™ºé€å¾Œï¼šå¤‰æ›´ä¸å¯\nåˆ©ç”¨å¯èƒ½ãªæ”¯æ‰•ã„æ–¹æ³•ï¼š\nãƒ»ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰\nãƒ»ä»£é‡‘å¼•æ›\nãƒ»ã‚³ãƒ³ãƒ“ãƒ‹æ±ºæ¸ˆ\nãƒ»éŠ€è¡ŒæŒ¯è¾¼"
        },
        {
            "text": "### Human: å•†å“ã®ã‚µã‚¤ã‚ºãŒåˆã‚ãªã„\n### Assistant: ã‚µã‚¤ã‚ºãŒåˆã‚ãªã‹ã£ãŸã¨ã®ã“ã¨ã€ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚\nã‚µã‚¤ã‚ºäº¤æ›ã‚’æ‰¿ã£ã¦ãŠã‚Šã¾ã™ï¼š\n1. å•†å“åˆ°ç€å¾Œ7æ—¥ä»¥å†…ã«ã”é€£çµ¡ãã ã•ã„\n2. ãƒã‚¤ãƒšãƒ¼ã‚¸ã‹ã‚‰äº¤æ›ç”³è«‹\n3. æ–°ã—ã„ã‚µã‚¤ã‚ºã‚’ãŠé¸ã³ãã ã•ã„\n4. è¿”é€æ–™ã¯ç„¡æ–™ã§ã™\nã‚¿ã‚°ã‚’å¤–ã•ã‚Œã¦ã„ãªã„å ´åˆã«é™ã‚Šäº¤æ›å¯èƒ½ã§ã™ã€‚"
        },
        {
            "text": "### Human: ãƒã‚¤ãƒ³ãƒˆã®æœ‰åŠ¹æœŸé™ã¯ã„ã¤ã¾ã§ã§ã™ã‹ï¼Ÿ\n### Assistant: ãƒã‚¤ãƒ³ãƒˆã®æœ‰åŠ¹æœŸé™ã«ã¤ã„ã¦ã”æ¡ˆå†…ã„ãŸã—ã¾ã™ã€‚\n- é€šå¸¸ãƒã‚¤ãƒ³ãƒˆï¼šæœ€çµ‚åˆ©ç”¨æ—¥ã‹ã‚‰1å¹´é–“\n- ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒã‚¤ãƒ³ãƒˆï¼šä»˜ä¸æ—¥ã‹ã‚‰6ãƒ¶æœˆ\n- èª•ç”Ÿæ—¥ãƒã‚¤ãƒ³ãƒˆï¼šä»˜ä¸æ—¥ã‹ã‚‰3ãƒ¶æœˆ\nãƒã‚¤ãƒšãƒ¼ã‚¸ã®ã€Œãƒã‚¤ãƒ³ãƒˆå±¥æ­´ã€ã§è©³ç´°ã‚’ã”ç¢ºèªã„ãŸã ã‘ã¾ã™ã€‚\næœŸé™åˆ‡ã‚Œå‰ã«ãƒ¡ãƒ¼ãƒ«ã§ãŠçŸ¥ã‚‰ã›ã„ãŸã—ã¾ã™ã€‚"
        }
    ]
    return data

def fine_tune_with_mlx(data_path: str, output_path: str, iterations: int = 100, model_name: str = None):
    """MLXã‚’ä½¿ç”¨ã—ãŸfine-tuning"""
    print_status("MLXã§ã®fine-tuningé–‹å§‹", "info")
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã®é¸æŠï¼ˆãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦ï¼‰
    if model_name is None:
        # åˆ©ç”¨å¯èƒ½ãªMLXãƒ¢ãƒ‡ãƒ«
        available_models = {
            "small": "mlx-community/Qwen2.5-1.5B-Instruct-4bit",  # 1.5B - æœ€å°
            "medium": "mlx-community/Qwen2.5-3B-Instruct-4bit",   # 3B - ãƒãƒ©ãƒ³ã‚¹
            "large": "mlx-community/Mistral-7B-Instruct-v0.2-4bit" # 7B - é«˜å“è³ª
        }
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å°ã•ã„ãƒ¢ãƒ‡ãƒ«
        model_name = available_models["small"]
        print_status(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_name}", "info")
    
    # MLXã‚³ãƒãƒ³ãƒ‰ã®æ§‹ç¯‰ï¼ˆæ­£ã—ã„ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    cmd = f"""
python -m mlx_lm.lora \\
    --model {model_name} \\
    --train \\
    --data {data_path} \\
    --adapter-path {output_path} \\
    --iters {iterations} \\
    --batch-size 1 \\
    --learning-rate 1e-4
"""
    
    print(f"\nå®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰:\n{cmd}")
    print_status("æ³¨: MLXã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:", "warning")
    print("pip install mlx mlx-lm")
    
    # å®Ÿéš›ã®å®Ÿè¡Œã¯ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã§
    import subprocess
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    
    if result.returncode == 0:
        print_status("Fine-tuningå®Œäº†ï¼", "success")
    else:
        print_status(f"ã‚¨ãƒ©ãƒ¼: {result.stderr}", "error")
        print_status("ä»£æ›¿æ¡ˆ: PyTorchã‚’ä½¿ç”¨ã—ãŸfine-tuningã‚’è©¦ã—ã¦ãã ã•ã„ (--method pytorch)", "info")
        return False
    
    return True

def fine_tune_with_pytorch(data: List[Dict], model_name: str, output_dir: str):
    """PyTorch + PEFTã‚’ä½¿ç”¨ã—ãŸfine-tuningï¼ˆä»£æ›¿æ‰‹æ³•ï¼‰"""
    print_status("PyTorch + PEFTã§ã®fine-tuningé–‹å§‹", "info")
    
    try:
        import torch
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            TrainingArguments,
            Trainer,
            DataCollatorForLanguageModeling
        )
        from peft import LoraConfig, get_peft_model
        from datasets import Dataset
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        device = "mps" if torch.backends.mps.is_available() else "cpu"
        print_status(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}", "info")
        
        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
        print_status("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...", "info")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16 if device == "mps" else torch.float32
        )
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.pad_token = tokenizer.eos_token
        
        # LoRAè¨­å®š
        peft_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.1,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        # ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
        model = get_peft_model(model, peft_config)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
        def tokenize_function(examples):
            return tokenizer(examples["text"], padding=True, truncation=True, max_length=512)
        
        dataset = Dataset.from_list(data)
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=4,
            warmup_steps=10,
            logging_steps=10,
            save_steps=50,
            save_strategy="steps",
            save_total_limit=2,
            load_best_model_at_end=False,  # è©•ä¾¡ãªã—ã®å ´åˆã¯False
            fp16=False,  # MPSã§ã¯FP16éå¯¾å¿œ
            push_to_hub=False,
            report_to="none",  # wandbãªã©ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç„¡åŠ¹åŒ–
        )
        
        # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
        print_status("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ï¼ˆã“ã‚Œã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰...", "warning")
        start_time = time.time()
        trainer.train()
        end_time = time.time()
        
        print_status(f"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼ æ‰€è¦æ™‚é–“: {end_time - start_time:.2f}ç§’", "success")
        
        # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        return True
        
    except Exception as e:
        print_status(f"ã‚¨ãƒ©ãƒ¼: {str(e)}", "error")
        return False

def convert_to_gguf(model_path: str, output_path: str):
    """ãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›ï¼ˆOllamaç”¨ï¼‰"""
    print_status("GGUFå½¢å¼ã¸ã®å¤‰æ›", "info")
    
    # å¤‰æ›ã‚³ãƒãƒ³ãƒ‰ï¼ˆllama.cppã®convert.pyã‚’ä½¿ç”¨ï¼‰
    cmd = f"""
python convert.py {model_path} \\
    --outfile {output_path} \\
    --outtype q4_0
"""
    
    print(f"\nå¤‰æ›ã«ã¯llama.cppã®convert.pyãŒå¿…è¦ã§ã™ã€‚")
    print(f"æ‰‹å‹•ã§å®Ÿè¡Œã—ã¦ãã ã•ã„:\n{cmd}")
    
    return True

def create_modelfile(gguf_path: str, output_path: str):
    """Ollamaç”¨ã®Modelfileã‚’ä½œæˆ"""
    content = f"""FROM {gguf_path}

SYSTEM "ã‚ãªãŸã¯ECã‚µã‚¤ãƒˆã®ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆæ‹…å½“ã§ã™ã€‚ãŠå®¢æ§˜ã®å•ã„åˆã‚ã›ã«ä¸å¯§ã«å¯¾å¿œã—ã€æ³¨æ–‡ã€é…é€ã€è¿”å“ã€å•†å“ã«ã¤ã„ã¦ã®è³ªå•ã«è¦ªåˆ‡ã«ç­”ãˆã¦ãã ã•ã„ã€‚"

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.1
"""
    
    with open(output_path, 'w') as f:
        f.write(content)
    
    print_status(f"Modelfileä½œæˆ: {output_path}", "success")

def main():
    parser = argparse.ArgumentParser(description='Mac Local Fine-tuning Script')
    parser.add_argument('--method', choices=['mlx', 'pytorch'], default='mlx',
                        help='Fine-tuningæ‰‹æ³•ã®é¸æŠ')
    parser.add_argument('--model', type=str, default='TinyLlama/TinyLlama-1.1B-Chat-v1.0',
                        help='ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆPyTorchä½¿ç”¨æ™‚ï¼‰')
    parser.add_argument('--mlx-model', type=str, choices=['small', 'medium', 'large'], default='small',
                        help='MLXãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: small(1.5B), medium(3B), large(7B)')
    parser.add_argument('--iterations', type=int, default=100,
                        help='ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°')
    parser.add_argument('--output-dir', type=str, default='./finetuned_model',
                        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    print(f"\n{Colors.BLUE}=== Mac Local Fine-tuning Script ==={Colors.END}\n")
    
    # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒã‚§ãƒƒã‚¯
    platform_type = check_mac_platform()
    
    # ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¡ã‚½ãƒƒãƒ‰ã«å¿œã˜ã¦ï¼‰
    check_dependencies(args.method)
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    print_status("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...", "info")
    data = create_sample_data()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    os.makedirs(args.output_dir, exist_ok=True)
    data_path = os.path.join(args.output_dir, "training_data.jsonl")
    with open(data_path, 'w') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print_status(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {data_path} ({len(data)}ã‚µãƒ³ãƒ—ãƒ«)", "success")
    
    # Fine-tuningå®Ÿè¡Œ
    if args.method == 'mlx' and platform_type == 'apple_silicon':
        print_status("MLXã‚’ä½¿ç”¨ã—ãŸfine-tuningã‚’é¸æŠ", "info")
        # MLXãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ãŸãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—
        mlx_models = {
            "small": "mlx-community/Qwen2.5-1.5B-Instruct-4bit",
            "medium": "mlx-community/Qwen2.5-3B-Instruct-4bit",
            "large": "mlx-community/Mistral-7B-Instruct-v0.2-4bit"
        }
        mlx_model = mlx_models.get(args.mlx_model, mlx_models["small"])
        success = fine_tune_with_mlx(data_path, args.output_dir, args.iterations, mlx_model)
    else:
        if args.method == 'mlx' and platform_type == 'intel':
            print_status("Intel Macã§ã¯MLXã¯æœ€é©ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚PyTorchã‚’ä½¿ç”¨ã—ã¾ã™ã€‚", "warning")
        print_status("PyTorch + PEFTã‚’ä½¿ç”¨ã—ãŸfine-tuningã‚’é¸æŠ", "info")
        success = fine_tune_with_pytorch(data, args.model, args.output_dir)
    
    if success:
        print_status("\nFine-tuningæˆåŠŸï¼", "success")
        
        # GGUFå¤‰æ›ã®æ¡ˆå†…
        print("\n" + "="*50)
        print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. ãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›")
        print("2. Modelfileã‚’ä½œæˆ")
        print("3. Ollamaã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ")
        print("\nä¾‹:")
        print(f"ollama create my-mac-model -f {args.output_dir}/Modelfile")
        
        # Modelfileä½œæˆ
        modelfile_path = os.path.join(args.output_dir, "Modelfile")
        create_modelfile("./model.gguf", modelfile_path)
    else:
        print_status("Fine-tuningã«å¤±æ•—ã—ã¾ã—ãŸ", "error")
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è­¦å‘Š
    print(f"\n{Colors.YELLOW}ğŸ’¡ ãƒ’ãƒ³ãƒˆ:{Colors.END}")
    print("- Fine-tuningä¸­ã¯ä»–ã®é‡ã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‰ã˜ã¦ãã ã•ã„")
    print("- Activity Monitorã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–ã—ã¦ãã ã•ã„")
    print("- é›»æºã«æ¥ç¶šã—ãŸçŠ¶æ…‹ã§å®Ÿè¡Œã—ã¦ãã ã•ã„")
    print("- ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã«ã¯ãƒ¡ãƒ¢ãƒª16GBä»¥ä¸Šã‚’æ¨å¥¨")

if __name__ == "__main__":
    main()