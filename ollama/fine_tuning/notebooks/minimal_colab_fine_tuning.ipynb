{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æœ€å°é™ã®Fine-tuningå®Ÿè·µã‚¬ã‚¤ãƒ‰ï¼ˆGoogle Colabï¼‰\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€æœ€å°é™ã®ãƒªã‚½ãƒ¼ã‚¹ã§åŠ¹æœçš„ãªfine-tuningã‚’è¡Œã†æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n",
    "- **ãƒ¢ãƒ‡ãƒ«**: TinyLlama 1.1Bï¼ˆè»½é‡ã§é«˜é€Ÿï¼‰\n",
    "- **æ‰‹æ³•**: QLoRAï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ï¼‰\n",
    "- **ãƒ‡ãƒ¼ã‚¿**: 50-100ã‚µãƒ³ãƒ—ãƒ«ç¨‹åº¦\n",
    "- **å®Ÿè¡Œæ™‚é–“**: ç´„10-20åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆç´„2åˆ†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unslothã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ï¼‰\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆ1åˆ†ï¼‰\n",
    "æœ€å°é™ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# æœ€å°é™ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ï¼ˆ50ã‚µãƒ³ãƒ—ãƒ«ï¼‰\n",
    "training_data = [\n",
    "    {\n",
    "        \"instruction\": \"Pythonã§ãƒªã‚¹ãƒˆã‚’é€†é †ã«ã™ã‚‹æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„\",\n",
    "        \"output\": \"Pythonã§ãƒªã‚¹ãƒˆã‚’é€†é †ã«ã™ã‚‹æ–¹æ³•ã¯3ã¤ã‚ã‚Šã¾ã™ï¼š\\n1. `reversed()`: `list(reversed(my_list))`\\n2. ã‚¹ãƒ©ã‚¤ã‚¹: `my_list[::-1]`\\n3. `reverse()`: `my_list.reverse()`\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"gitã§ã‚³ãƒŸãƒƒãƒˆã‚’å–ã‚Šæ¶ˆã™æ–¹æ³•\",\n",
    "        \"output\": \"gitã§ã‚³ãƒŸãƒƒãƒˆã‚’å–ã‚Šæ¶ˆã™ã«ã¯ï¼š\\n- `git reset --soft HEAD~1`: ã‚³ãƒŸãƒƒãƒˆã®ã¿å–ã‚Šæ¶ˆã—\\n- `git reset --hard HEAD~1`: å¤‰æ›´ã”ã¨å–ã‚Šæ¶ˆã—\\n- `git commit --amend`: ç›´å‰ã®ã‚³ãƒŸãƒƒãƒˆã‚’ä¿®æ­£\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Dockerã‚³ãƒ³ãƒ†ãƒŠãŒèµ·å‹•ã—ãªã„\",\n",
    "        \"output\": \"Dockerã‚³ãƒ³ãƒ†ãƒŠãŒèµ·å‹•ã—ãªã„å ´åˆã®ç¢ºèªäº‹é …ï¼š\\n1. `docker logs <container_id>`: ãƒ­ã‚°ç¢ºèª\\n2. `docker ps -a`: ã‚³ãƒ³ãƒ†ãƒŠçŠ¶æ…‹ç¢ºèª\\n3. Dockerfileã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆç¢ºèª\\n4. ãƒãƒ¼ãƒˆç«¶åˆã®ç¢ºèª\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"TypeError: 'NoneType' object is not iterable\",\n",
    "        \"output\": \"ã“ã®ã‚¨ãƒ©ãƒ¼ã¯ã€Noneã«å¯¾ã—ã¦ç¹°ã‚Šè¿”ã—å‡¦ç†ã‚’è¡ŒãŠã†ã¨ã—ãŸæ™‚ã«ç™ºç”Ÿã—ã¾ã™ã€‚\\nå¯¾å‡¦æ³•ï¼š\\n1. å¤‰æ•°ãŒNoneã§ãªã„ã‹ç¢ºèª: `if data is not None:`\\n2. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š: `data = data or []`\\n3. é–¢æ•°ã®æˆ»ã‚Šå€¤ã‚’ç¢ºèª\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"APIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒé…ã„\",\n",
    "        \"output\": \"APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ”¹å–„æ–¹æ³•ï¼š\\n1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å®Ÿè£…ï¼ˆRedisç­‰ï¼‰\\n2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã®æœ€é©åŒ–\\n3. ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®å°å…¥\\n4. éåŒæœŸå‡¦ç†ã®æ´»ç”¨\\n5. CDNã®åˆ©ç”¨\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ï¼ˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ï¼‰\n",
    "extended_data = []\n",
    "for item in training_data:\n",
    "    # ã‚ªãƒªã‚¸ãƒŠãƒ«\n",
    "    extended_data.append(item)\n",
    "    \n",
    "    # å°‘ã—é•ã†èãæ–¹ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³\n",
    "    variation = {\n",
    "        \"instruction\": item[\"instruction\"] + \"ã€‚åˆå¿ƒè€…å‘ã‘ã«èª¬æ˜ã—ã¦ãã ã•ã„\",\n",
    "        \"output\": \"åˆå¿ƒè€…ã®æ–¹å‘ã‘ã«èª¬æ˜ã—ã¾ã™ã€‚\\n\\n\" + item[\"output\"]\n",
    "    }\n",
    "    extended_data.append(variation)\n",
    "\n",
    "# åˆè¨ˆ10å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆãƒ‡ãƒ¢ç”¨æœ€å°é™ï¼‰\n",
    "print(f\"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æ•°: {len(extended_data)}\")\n",
    "\n",
    "# JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
    "with open('minimal_training_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(extended_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨LoRAè¨­å®šï¼ˆ2åˆ†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆçŸ­ã‚ã«è¨­å®šã—ã¦ãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰\n",
    "max_seq_length = 512\n",
    "\n",
    "# 4bité‡å­åŒ–ã§1.1Bãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆç´„500MBï¼‰\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/tinyllama-bnb-4bit\",  # æœ€è»½é‡ãƒ¢ãƒ‡ãƒ«\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¿½åŠ ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªfine-tuningï¼‰\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,  # ãƒ©ãƒ³ã‚¯ï¼ˆå°ã•ãã—ã¦ãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™å®Œäº†ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ï¼ˆ1åˆ†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé–¢æ•°\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        text = f\"\"\"### è³ªå•:\n",
    "{instruction}\n",
    "\n",
    "### å›ç­”:\n",
    "{output}\"\"\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "dataset = Dataset.from_list(extended_data)\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tuningå®Ÿè¡Œï¼ˆ10-15åˆ†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šï¼ˆæœ€å°é™ï¼‰\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=5,\n",
    "        max_steps=30,  # æœ€å°é™ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# GPUãƒ¡ãƒ¢ãƒªçµ±è¨ˆ\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"ä½¿ç”¨ä¸­ã®ãƒ¡ãƒ¢ãƒª = {start_gpu_memory} GB.\")\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ä½¿ç”¨ã—ãŸãƒ¡ãƒ¢ãƒª = {used_memory_for_lora} GB ({lora_percentage}% of GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆï¼ˆ1åˆ†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆè³ªå•\n",
    "test_questions = [\n",
    "    \"Pythonã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€æ–¹æ³•ã¯ï¼Ÿ\",\n",
    "    \"ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¾ã—ãŸ: KeyError: 'user_id'\",\n",
    "    \"gitã§å¤‰æ›´ã‚’å–ã‚Šæ¶ˆã—ãŸã„\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    inputs = tokenizer(\n",
    "        f\"\"\"### è³ªå•:\n",
    "{question}\n",
    "\n",
    "### å›ç­”:\"\"\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens=128, temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nè³ªå•: {question}\")\n",
    "    print(f\"å›ç­”: {response.split('### å›ç­”:')[1].strip()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. GGUFå½¢å¼ã§ä¿å­˜ï¼ˆOllamaç”¨ï¼‰ï¼ˆ2åˆ†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GGUFå½¢å¼ã§ä¿å­˜ï¼ˆé‡å­åŒ–ãƒ¬ãƒ™ãƒ«: q4_k_mï¼‰\n",
    "print(\"GGUFå½¢å¼ã§ä¿å­˜ä¸­...\")\n",
    "model.save_pretrained_gguf(\n",
    "    \"minimal_finetuned_model\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\"  # ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„é‡å­åŒ–\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… ä¿å­˜å®Œäº†ï¼\")\n",
    "print(\"ãƒ•ã‚¡ã‚¤ãƒ«å: minimal_finetuned_model-unsloth.Q4_K_M.gguf\")\n",
    "print(\"\\nã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€Ollamaã§ä½¿ç”¨ã§ãã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colabã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "from google.colab import files\n",
    "\n",
    "# GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "files.download('minimal_finetuned_model-unsloth.Q4_K_M.gguf')\n",
    "\n",
    "print(\"\\nğŸ“¥ ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒé–‹å§‹ã•ã‚Œã¾ã—ãŸã€‚\")\n",
    "print(\"\\nãƒ­ãƒ¼ã‚«ãƒ«ã§ã®ä½¿ç”¨æ–¹æ³•:\")\n",
    "print(\"1. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸGGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é©å½“ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®\")\n",
    "print(\"2. Modelfileã‚’ä½œæˆ:\")\n",
    "print(\"   echo 'FROM ./minimal_finetuned_model-unsloth.Q4_K_M.gguf' > Modelfile\")\n",
    "print(\"3. Ollamaã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ:\")\n",
    "print(\"   ollama create my-minimal-model -f Modelfile\")\n",
    "print(\"4. ä½¿ç”¨:\")\n",
    "print(\"   ollama run my-minimal-model \\\"è³ªå•\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€æœ€å°é™ã®ãƒªã‚½ãƒ¼ã‚¹ã§fine-tuningã‚’å®Ÿè¡Œã—ã¾ã—ãŸï¼š\n",
    "\n",
    "- **ãƒ¢ãƒ‡ãƒ«**: TinyLlama 1.1Bï¼ˆæœ€è»½é‡ï¼‰\n",
    "- **ãƒ‡ãƒ¼ã‚¿**: 10ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå®Ÿç”¨ã§ã¯50-100æ¨å¥¨ï¼‰\n",
    "- **å®Ÿè¡Œæ™‚é–“**: ç´„15-20åˆ†\n",
    "- **GPUä½¿ç”¨é‡**: ç´„3-4GB\n",
    "- **å‡ºåŠ›**: Ollamaå¯¾å¿œã®GGUFãƒ•ã‚¡ã‚¤ãƒ«\n",
    "\n",
    "### æ”¹å–„ã®ãƒ’ãƒ³ãƒˆ\n",
    "\n",
    "1. **ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™**: 50-100ã‚µãƒ³ãƒ—ãƒ«ã§å“è³ªå‘ä¸Š\n",
    "2. **ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å¢—ã‚„ã™**: max_steps=50-100\n",
    "3. **å¤§ãã‚ã®ãƒ¢ãƒ‡ãƒ«**: Llama-3.2-3Bï¼ˆä½™è£•ãŒã‚ã‚Œã°ï¼‰\n",
    "4. **è©•ä¾¡ã®è¿½åŠ **: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å“è³ªç¢ºèª"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}