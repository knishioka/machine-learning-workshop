#!/usr/bin/env python3
"""
Ollama Fine-tuning Script
ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆç”¨ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®fine-tuningå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import subprocess
import sys
import os
from typing import List, Dict
import argparse


def load_training_data(file_path: str) -> List[Dict]:
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line.strip()))
    return data


def prepare_training_prompts(data: List[Dict]) -> str:
    """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’Ollamaå½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›"""
    prompts = []
    for item in data:
        instruction = item.get('instruction', '')
        input_text = item.get('input', '')
        output = item.get('output', '')
        
        if input_text:
            prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
        else:
            prompt = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"
        
        prompts.append(prompt)
    
    return "\n\n---\n\n".join(prompts)


def create_modelfile(base_model: str, training_prompts: str, model_name: str) -> str:
    """Fine-tuningç”¨ã®Modelfileã‚’ä½œæˆ"""
    modelfile_content = f"""FROM {base_model}

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
SYSTEM ã‚ãªãŸã¯è¦ªåˆ‡ã§ä¸å¯§ãªã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãŠå®¢æ§˜ã®è³ªå•ã«å¯¾ã—ã¦ã€æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_predict 512
"""
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    for prompt in training_prompts.split("\n\n---\n\n"):
        user_msg = prompt.split('### Response:')[0].strip().replace('\n', ' ')
        assistant_msg = prompt.split('### Response:')[1].strip().replace('\n', ' ')
        modelfile_content += f"\nMESSAGE user {user_msg}"
        modelfile_content += f"\nMESSAGE assistant {assistant_msg}"
    
    modelfile_path = f"Modelfile.{model_name}"
    with open(modelfile_path, 'w', encoding='utf-8') as f:
        f.write(modelfile_content)
    
    return modelfile_path


def fine_tune_model(modelfile_path: str, model_name: str):
    """Ollamaã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’fine-tuning"""
    print(f"Fine-tuningé–‹å§‹: {model_name}")
    
    try:
        # Ollamaã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        cmd = f"ollama create {model_name} -f {modelfile_path}"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        if result.returncode == 0:
            print(f"âœ… Fine-tuningå®Œäº†: {model_name}")
            print("å‡ºåŠ›:", result.stdout)
        else:
            print(f"âŒ Fine-tuningå¤±æ•—")
            print("ã‚¨ãƒ©ãƒ¼:", result.stderr)
            sys.exit(1)
            
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)


def test_model(model_name: str):
    """Fine-tuningã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ"""
    test_prompts = [
        "é…é€ã«ã¯ã©ã®ãã‚‰ã„æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã‹ï¼Ÿ",
        "è¿”å“ã¯ã§ãã¾ã™ã‹ï¼Ÿ",
        "æ”¯æ‰•ã„æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„"
    ]
    
    print(f"\nğŸ§ª ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ: {model_name}")
    print("-" * 50)
    
    for prompt in test_prompts:
        print(f"\nè³ªå•: {prompt}")
        cmd = f'ollama run {model_name} "{prompt}"'
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        print(f"å›ç­”: {result.stdout.strip()}")
        print("-" * 50)


def main():
    parser = argparse.ArgumentParser(description='Ollama Fine-tuning Script')
    parser.add_argument('--data', type=str, default='training_data.jsonl',
                        help='è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--base-model', type=str, default='llama3.2:1b',
                        help='ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹Ollamaãƒ¢ãƒ‡ãƒ«')
    parser.add_argument('--model-name', type=str, default='customer-support',
                        help='ä½œæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®åå‰')
    parser.add_argument('--test', action='store_true',
                        help='Fine-tuningå¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ')
    
    args = parser.parse_args()
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    print(f"ğŸ“š è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {args.data}")
    training_data = load_training_data(args.data)
    print(f"âœ… {len(training_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æº–å‚™
    print("\nğŸ“ è¨“ç·´ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æº–å‚™ä¸­...")
    training_prompts = prepare_training_prompts(training_data)
    
    # Modelfileã‚’ä½œæˆ
    print("\nğŸ“„ Modelfileã‚’ä½œæˆä¸­...")
    modelfile_path = create_modelfile(args.base_model, training_prompts, args.model_name)
    print(f"âœ… Modelfileã‚’ä½œæˆã—ã¾ã—ãŸ: {modelfile_path}")
    
    # Fine-tuningã‚’å®Ÿè¡Œ
    print(f"\nğŸš€ Fine-tuningã‚’é–‹å§‹ã—ã¾ã™...")
    fine_tune_model(modelfile_path, args.model_name)
    
    # ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    if args.test:
        test_model(args.model_name)
    
    print(f"\nâœ¨ å®Œäº†ï¼ãƒ¢ãƒ‡ãƒ« '{args.model_name}' ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚")
    print(f"ä½¿ç”¨æ–¹æ³•: ollama run {args.model_name}")


if __name__ == "__main__":
    main()